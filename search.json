[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible Polyglot Data Science",
    "section": "",
    "text": "Welcome!",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#a-modern-unified-and-language-agnostic-workflow-for-data-science-using-nix.",
    "href": "index.html#a-modern-unified-and-language-agnostic-workflow-for-data-science-using-nix.",
    "title": "Reproducible Polyglot Data Science",
    "section": "A modern, unified, and language-agnostic workflow for data science using Nix.",
    "text": "A modern, unified, and language-agnostic workflow for data science using Nix.\nThis book is a complete reimagining of my previous work, “Building Reproducible Analytical Pipelines with R.” If you’re looking for that book, you can find it here. But if you’re ready for the next step, you’re in the right place.\n\n  \n\nData scientists, statisticians, analysts, researchers, and many other professionals write a lot of code.\nNot only do they write a lot of code, but they must also read and review a lot of code as well. They either work in teams and need to review each other’s code, or need to be able to reproduce results from past projects, be it for peer review or auditing purposes. And yet, they never, or very rarely, get taught the tools and techniques that would make the process of writing, collaborating, reviewing and reproducing projects possible.\nWhich is truly unfortunate because software engineers face the same challenges and solved them decades ago.\nThe aim of this book is to teach you how to use some of the best practices from software engineering and DevOps to make your projects robust, reliable and reproducible. It doesn’t matter if you work alone, in a small or in a big team. It doesn’t matter if your work gets (peer-)reviewed or audited: the techniques presented in this book will make your projects more reliable and save you a lot of frustration!\nAs someone whose primary job is analysing data, you might think that you are not a developer. It seems as if developers are these genius types that write extremely high-quality code and create these super useful packages. The truth is that you are a developer as well. It’s just that your focus is on writing code for your purposes to get your analyses going instead of writing code for others. Or at least, that’s what you think. Because in others, your team-mates are included. Reviewers and auditors are included. Any people that will read your code are included, and there will be people that will read your code. At the very least future you will read your code. By learning how to set up projects and write code in a way that future you will understand and not want to murder you, you will actually work towards improving the quality of your work, naturally.\nThe book can be read for free on https://b-rodrigues.github.io/reproducible-data-science/ and you’ll be able buy a DRM-free Epub or PDF on Leanpub1 once there’s more content.\nThis book is the culmination of my previous works. I started by writing a book focused on R, and then began working on a Python edition. During that process, I had a realization: tackling reproducibility one language at a time was solving the symptoms, not the root cause. The real solution needed to be universal, powerful, and capable of handling any language or tool we might need.\nThat universal solution is Nix.\nThis book moves beyond language-specific tooling. It presents a holistic workflow where R, Python, and Julia are not competitors, but collaborators in a single, cohesive, and perfectly reproducible environment. We will cover:\n\nThe Nix Philosophy: Why Nix is the ultimate tool for solving the “it works on my machine” problem, once and for all.\nDeclarative Environments with {rix}: How to use a simple R interface to define exact, bit-for-bit reproducible software environments that include specific versions of R, Python, Julia, their packages, and any system-level dependencies.\nPolyglot Pipelines with {rixpress} (R) or ryxpress (Python): How to orchestrate complex analytical pipelines that seamlessly pass data between different languages, all managed by the Nix build system.\nUnit Testing and Functional Programming: Core principles for writing robust, testable, and maintainable code, no matter the language.\nDistribution and Automation: How to package your entire reproducible pipeline into a Docker container for easy sharing and automate your workflow with GitHub Actions.\n\nWhile this is not a book for beginners (you should be familiar with at least one data-centric programming language before reading this), I will not assume that you have any knowledge of the tools discussed. But be warned, this book will require you to take the time to read it, and then type on your computer. Type a lot.\nI hope that you will enjoy reading this book and applying the ideas in your day-to-day, ideas which hopefully should improve the reliability, traceability and reproducibility of your code.\nIf you find this book useful, don’t hesitate to let me know! You can submit issues, suggest improvements, and ask questions on the book’s Github repository.\nYou can also buy a physical copy of the book on Amazon.\nIf you want to get to know me better, read my bio2.\nYou’ll also be able to buy a physical copy of the book on Amazon once it’s done. In the meantime, you could buy the R edition.\nIf you find this book useful, don’t hesitate to let me know or leave a rating on Amazon!",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Reproducible Polyglot Data Science",
    "section": "",
    "text": "https://leanpub.com/↩︎\nhttps://www.brodrigues.co/about/me/↩︎",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Three years ago, I wrote a book with a straightforward premise: by borrowing a few key ideas from software engineering, people who analyse data could save themselves a great deal of frustration. The response to that book was more positive than I could have ever hoped for, and it confirmed a suspicion I had: we, as a community, are hungry for better ways to work.\nThat book, however, focused exclusively on the R ecosystem. A recurring question I received was, “This is great, but what about Python?” It was a fair question. The world of data science is not a monologue; it’s a conversation between languages. So, I began what felt like the logical next step: writing a Python edition.\nI mapped out the chapters and identified the equivalent tools, pipenv for dependency management, ploomber for pipelines, and started writing. But as I went deeper, a nagging feeling grew. I was solving the same problems all over again, just with a different set of tools. This feeling was compounded by the rapid churn within the Python ecosystem itself. How many package managers have been created to solve virtual environment management? As of writing, uv is all the rage, and while it may be here to stay, history suggests a new contender is always just around the corner.\nThis pointed to a larger issue. I am convinced that the future of data science is polyglot. An R user and a Python user, both following my original advice, would end up with reproducible projects, but their workflows would be fundamentally incompatible. They couldn’t easily share an environment or build a single pipeline that leverages the strengths of both languages. While companies like Posit have made excellent progress in making it easier to call Python from R, setting up a truly integrated development environment remains a challenge. And what if you wish to bring Julia, the other language of data analysis, into the fold? It is not as popular as Python or R, but it has its own distinct appeal and advantages.\nI realised I was treating the symptoms, not the disease. The root problem wasn’t “How do I make R reproducible?” or “How do I make Python reproducible?”. The real challenge was the lack of a universal foundation that could handle any language, any tool, and any system dependency with absolute, bit-for-bit precision.\nThat’s when I stopped writing the Python book.\nThe solution wasn’t to create another language-specific guide but to find a tool that operated at a more fundamental level. That tool, I am now convinced, is Nix.\nNix is not just another package manager; it is a powerful, declarative system for building and managing software environments. It allows us to define the entire computational environment—from the operating system libraries up to the specific versions of our R and Python packages—in a single, simple text file. When you use Nix, the phrase “it works on my machine” becomes obsolete. It is replaced by the guarantee: “it builds identically, everywhere, every time”—with a few caveats that we will explore, of course.\nThis book is the result of that realisation. It is a complete reimagining of the original. We are moving away from language-specific patchworks and toward a unified, polyglot workflow. We will use Nix as our bedrock, with the {rix} and {rixpress} R packages (or ryxpress for Python) serving as our friendly interface to its power. You will learn to build pipelines where R, Python, and Julia aren’t just neighbours; they are collaborators, working together in a single, perfectly reproducible environment.\nA note for Python-first users: do not be deterred by the fact that {rix} and {rixpress} are R packages: there is a Python version called ryxpress that will allow you to run your pipelines from an interactive Python session. You will be able to use them to define your environments (even integrating tools like uv) and orchestrate your pipelines, while doing all of your analytical work exclusively in Python. In this workflow, R simply becomes a convenient configuration language.\nThe core message from three years ago remains unchanged. You, as someone who writes code to analyse data, are a developer. Your work is important, and it deserves to be reliable. This book aims to give you the tools and the mindset to achieve that. The journey is more ambitious this time, but the payoff is far greater.\nI hope you’ll join me.\nYou can read this book for free online at https://b-rodrigues.github.io/reproducible-data-science/.\nYou can submit issues, suggest improvements, and ask questions on the book’s Github repository.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Who is this book for?\nJust like the previous, R-focused edition of this book, this one will not teach you about machine learning, statistics, or visualisation.\nThe goal is to teach you a set of tools, practices, and project management techniques that should make your projects easier to reproduce, replicate, and retrace, with a focus on polyglot (multilingual) projects. I believe that with LLMs, polyglot projects will become increasingly common.\nBefore LLMs, if you were a Python user, you would avoid R like the plague, and vice versa. This is understandable: even though both are high-level scripting languages, and an R user could likely read and understand Python code (and vice versa), it is still a pain in the loins to set up another language just to run a few lines of code. Now, with LLMs, you can have a model generate the code, and depending on what you’re doing, it’s likely to be correct. However, setting up another language is still quite annoying. This is where Nix comes in. Nix makes adding another language to a project extremely simple.\nThis book is for anyone who uses raw data to build any type of output. This could be a simple quarterly report, in which data is used for tables and graphs, a scientific article for a peer-reviewed journal, or even an interactive web application. The specific output doesn’t matter, because the process is, at its core, always very similar:\nThis book assumes some familiarity with programming, particularly with the R and Python languages. I will not discuss Julia in great detail, as I am not familiar enough with it to do it justice. That being said, I will show you how to add Julia to a project and use it effectively if you need to.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#who-is-this-book-for",
    "href": "intro.html#who-is-this-book-for",
    "title": "1  Introduction",
    "section": "",
    "text": "Get the data;\nClean the data;\nWrite code to analyse the data;\nPut the results into the final product.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-the-aim-of-this-book",
    "href": "intro.html#what-is-the-aim-of-this-book",
    "title": "1  Introduction",
    "section": "1.2 What is the aim of this book?",
    "text": "1.2 What is the aim of this book?\nThe aim of this book is to make the process of analysing data as reliable, retraceable, and reproducible as possible, and to do this by design. This means that once you’re done with the analysis, you’re done. You don’t want to spend time, which you often don’t have anyway, to rewrite or refactor an analysis to make it reproducible after the fact. We both know this is not going to happen. Once an analysis is finished, it’s on to the next one. If you need to rerun an older analysis (for example, because the data has been updated), you’ll simply figure it out at that point, right? That’s a problem for Future You. Hopefully, Future You will remember every quirk of your code, know which script to run at which point, which comments are outdated, and what features of the data need to be checked… You had better hope Future You is a more diligent worker than you are!\nGoing forward, I’m going to refer to a project that is reproducible as a “Reproducible Analytical Pipeline”, or RAP for short. There are only two ways to create a RAP: either you are lucky enough to have someone on your team whose job is to turn your messy code into a RAP, or you do it yourself. The second option is by far the most common. The issue, as stated above, is that most of us simply don’t do it. We are always in a rush to get to the results and don’t think about making the process reproducible, because we assume it takes extra time that is better spent on the analysis itself. This is a misconception, for two reasons.\nThe first is that employing the techniques we will discuss in this book won’t actually take much time. As you will see, they are not things you “add on top of” the analysis, but are part of the analysis itself, and they will also help with managing the project. Some of these techniques, especially testing, will even save you time and headaches.\nThe second reason is that an analysis is never a one-shot. Only the simplest tasks, like pulling a single number from a database, might be. Even then, chances are that once you provide that number, you’ll be asked for a variation of it (for example, disaggregated by one or several variables). Or perhaps you’ll be asked for an update in six months. You will quickly learn to keep that SQL query in a script somewhere to ensure consistency. But what about more complex analyses? Is keeping the script enough? It is a good start, of course, but very often, there is no single script, or a script for each step of the analysis is missing.\nI’ve seen this play out many times in many different organisations. It’s that time of the year again, and a report needs to be written. Ten people are involved, and just gathering the data is already complicated. Some get their data from Word documents attached to emails, some from a website, some from a PDF report from another department. I remember a story a senior manager at my previous job used to tell: once, a client put out a call for a project that involved setting up a PDF scraper. They periodically needed data from another department that only came in PDFs. The manager asked what was, at least from our perspective, an obvious question: “Why can’t they send you the underlying data in a machine-readable format?” They had never thought to ask. So, my manager went to that department and talked to the people putting the PDF together. Their answer? “Well, we could send them the data in any format they want, but they’ve asked for the tables in a PDF.”\nSo the first, and probably most important, lesson here is: when starting to build a RAP, make sure you talk with all the people involved.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "1  Introduction",
    "section": "1.3 Prerequisites",
    "text": "1.3 Prerequisites\nYou should be comfortable with the command line. This book will not assume any particular Integrated Development Environment (IDE), so most of what I’ll show you will be done via the command line. That said, I will spend some time helping you set up a data science-focused IDE, Positron, to work seamlessly with this workflow. The command line may be over 50 years old, but it is not going anywhere. In fact, thanks to the rise of LLMs, it seems to be enjoying a resurgence. Since these models generate text, it is far simpler to ask one for a shell command to solve a problem than to have it produce detailed instructions on where to click in a graphical user interface (GUI). Knowing your way around the command line is also essential for working with modern data science infrastructure: continuous integration platforms, Docker, remote servers… they all live in the terminal. So, if you’re not at all familiar with the command line, you might need to brush up on the basics. Don’t worry, though; this isn’t a book about the intricacies of the Unix command line. The commands I’ll show you will be straightforward and directly applicable to the task at hand.\nThis means however that if you are using Windows, first of all, why? and second of all, you will have to set up Windows Subsystem for Linux. This is because there is no native Nix implementation for Windows, and so we need to run the Linux version through WSL. Don’t worry though, it’s not that hard, and you can then use an IDE from Windows to work with the environments managed by Nix, in a very seamless way (but seriously, consider, if you can, switching to Linux. How can you tolerate ads (ADS!) in the start menu).\nIdeally, you should be comfortable with either R or Python. This book will assume that you have been using at least one of these languages for some projects and want to improve how you manage complex projects. You should know about packages and how to install them, have written some functions, understand loops, and have a basic knowledge of data structures like lists. While this is not a book on visualisation, we will be making some graphs as well.\nOur aim is to write code that can be executed non-interactively. This is because a necessary condition for a workflow to be reproducible and to qualify as a RAP is for it to be executed by a machine, automatically, without any human intervention. This is the second lesson of building RAPs: there should be no human intervention needed to get the outputs once the RAP has started. If you achieve this, then your workflow is likely reproducible, or can at least be made so much more easily than if it requires special manipulation by a human somewhere in the loop.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-actually-is-reproducibility",
    "href": "intro.html#what-actually-is-reproducibility",
    "title": "1  Introduction",
    "section": "1.4 What actually is reproducibility?",
    "text": "1.4 What actually is reproducibility?\nA reproducible project means that it can be rerun by anyone at zero (or very minimal) cost. But there are different levels of reproducibility, which I will discuss in the next section. Let’s first outline the requirements that a project must meet to be considered a RAP.\n\n1.4.1 Using open-source tools is a hard requirement\nOpen source is a hard requirement for reproducibility. No ifs, ands, or buts. I’m not just talking about the code you wrote for your research paper or report; I’m talking about the entire ecosystem you used to write your code and build the workflow.\nIs your code open? Good. Or is it at least available to others in your organisation, in a way that they could re-execute it if needed? Also good.\nBut is it code written in a proprietary program, like STATA, SAS, or MATLAB? Then your project is not reproducible. It doesn’t matter if the code is well-documented, well-written, and available on a version control system. The project is simply not reproducible. Why?\nBecause on a long enough time horizon, there is no way to re-execute your code with the exact same version of the proprietary language and operating system that were used when the project was developed. As I’m writing these lines, MATLAB, for example, is at version R2025a. Buying an older version may not be simple. I’m sure if you contact their sales department, they might be able to sell you an older version. Maybe you can even re-download older versions you’ve already purchased from their website. But maybe it’s not that straightforward. Or maybe they won’t offer this option in the future. In any case, if you search for “purchase old version of Matlab,” you will see that many researchers and engineers have this need. ::: {.content-hidden when-format=“pdf”}\n\n\n\nWanting to run older versions of analytics software is a recurrent need.\n\n\n:::\nAnd if you’re running old code written for version, say, R2008a, there’s no guarantee that it will produce the exact same results on version R2025a. That’s without even mentioning the toolboxes (if you’re not familiar with them, they’re MATLAB’s equivalent of packages or libraries). These evolve as well, and there’s no guarantee that you can purchase older versions. It’s also likely that newer toolboxes cannot even run on older versions of MATLAB.\nLet me be clear: what I’m describing here for MATLAB could also be said for any other proprietary programs still commonly (and unfortunately) used in research and statistics, like STATA, SAS, or SPSS. Even if some of these vendors provide ways to run older versions of their software, the fact that you have to rely on them for this is a barrier to reproducibility. There is no guarantee they will provide this option forever. Who can guarantee that these companies will even be around forever? More likely, they might shift from a program you install on your machine to a subscription-based model.\nFor just 199€ a month, you can execute your SAS (or whatever) scripts on the cloud! Worried about data confidentiality? No problem, data is encrypted and stored safely on our secure servers! Run your analysis from anywhere and don’t worry about your cat knocking coffee over your laptop! And if you purchase the pro licence, for an additional 100€ a month, you can even execute your code in parallel!\nThink this is science fiction? There is a growing and concerning trend for vendors to move to a Software-as-a-Service model with monthly subscriptions. It happened to Adobe’s design software, and the primary reason it hasn’t yet happened for data analytics tools is data privacy concerns. Once those are deemed “solved,” I would not be surprised to see a similar shift.\n\n\n1.4.2 Hidden dependencies can hinder reproducibility\nThen there’s another problem. Let’s suppose you’ve written a thoroughly tested and documented workflow and made it available on GitHub (and let’s even assume the data is freely available and the paper is open access). Or, if you’re in the private sector, you’ve done all of the above, but the workflow is only available internally.\nLet’s further assume that you’ve used R, Python, or another open-source language. Can this analysis be said to be reproducible? Well, if it ran on a proprietary operating system, then the conclusion is: your project is not fully reproducible.\nThis is because the operating system the code runs on can also influence the outputs. There are particularities in operating systems that may cause certain things to work differently. Admittedly, this is rarely a problem in practice, but it does happen1, especially if you’re working with high-precision floating-point arithmetic, as you might in the financial sector, for instance.\nThankfully, as you will see in this book, there is no need to change your operating system to deal with this issue.\n\n\n1.4.3 The requirements of a RAP\nSo where does that leave us? For a project to be truly reproducible, it has to respect the following points:\n\nSource code must be available and thoroughly tested and documented (which is why we will be using Git and GitHub).\nAll dependencies must be easy to find and install (we will deal with this using dependency management tools).\nIt must be written in an open-source programming language (no-code tools like Excel are non-reproducible by default because they can’t be used non-interactively, which is why we will be using languages like R, Python, and Julia).\nThe project needs to run on an open-source operating system (we can deal with this without having to install and learn a new OS, thanks to tools like Docker).\nThe data and the final report must be accessible—if not publicly, then at least within your company. This means the concept of “scripts and/or data available upon request” belongs in the bin.\n\n\n\n\nA real sentence from a real paper published in THE LANCET Regional Health. How about make the data available and I won’t scratch your car, how’s that for a reasonable request?\n\n\n\n\n1.4.4 Are there different types of reproducibility?\nLet’s take one step back. We live in the real world, where constraints outside of our control can make it impossible to build a true RAP. Sometimes we need to settle for something that might not be perfect, but is the next best thing.\nIn what follows, let’s assume the code is tested and documented, so we will only discuss the pipeline’s execution.\nThe least reproducible pipeline would be something that works, but only on your machine. This could be due to hardcoded paths that only exist on your laptop. Anyone wanting to rerun the pipeline would need to change them. This should be documented in a README, which we’ve assumed is the case. But perhaps the pipeline only runs on your laptop because the computational environment is hard to reproduce. Maybe you use software, even open-source software, that is not easy to install (anyone who has tried to install R packages on Linux that depend on {rJava} knows what I’m talking about).\nA better, though still imperfect, pipeline would be one that could be run more easily on any similar machine. This could be achieved by avoiding hardcoded absolute paths and by providing instructions to set up the environment. For example, in Python, this could be as simple as providing a requirements.txt file that lists the project’s dependencies, which could be installed using pip:\npip install -r requirements.txt\nDoing this helps others (or Future You) install the required packages. However, this is not enough, as other software on your system, outside of what pip manages, can still impact the results.\nYou should also ensure that people run the same analysis on the same versions of R or Python that were used to create it. Just installing the right packages is not enough. The same code can produce different results on different versions of a language, or not run at all. If you’ve been using Python for some time, you certainly remember the switch from Python 2 to Python 3. Who knows, the switch to Python 4 might be just as painful!\nThe take-away message is that relying on the language itself being stable over time is not a sufficient condition for reproducibility. We have to set up our code in a way that is explicitly reproducible by dealing with the versions of the language itself.\nSo what does this all mean? It means that reproducibility exists on a continuum. Depending on the constraints you face, your project can be “not very reproducible” or “totally reproducible”. Let’s consider the following list of factors that can influence how reproducible your project truly is:\n\nVersion of the programming language used.\nVersions of the packages/libraries of said programming language.\nThe operating system and its version.\nVersions of the underlying system libraries (which often go hand-in-hand with the OS version, but not always).\nAnd even the hardware architecture that you run the software stack on.\n\nBy “reproducibility is on a continuum,” I mean that you can set up your project to take none, one, two, three, four, or all of the preceding items into consideration.\nThis is not a novel, or new idea. Peng (2011) already discussed this concept but named it the reproducibility spectrum:\n\n\n\nThe reproducibility spectrum from Peng’s 2011 paper.\n\n\nLet me finish this introduction by discussing the last item on the list: hardware architecture. In 2020, Apple changed the hardware architecture of their computers. Their new machines no longer use Intel CPUs, but instead Apple’s own proprietary architecture (Apple Silicon) based on the ARM specification. Concretely, this means that binary packages built for Intel-based Apple computers cannot run on their new machines, at least not without a compatibility layer. If you have a recent Apple Silicon Mac and need to install old packages to rerun a project (and we will learn how to do this), they need to be compiled to work on Apple Silicon first. While a compatibility layer called Rosetta 2 exists, my point is that you never know what might come in the future. The ability to compile from source is important because it requires the fewest dependencies outside of your control. Relying on pre-compiled binaries is not future-proof, which is another reason why open-source tools are a hard requirement for reproducibility.\nFor you Windows users, don’t think that the preceding paragraph does not concern you. It is very likely that Microsoft will push for OEM manufacturers to build more ARM-based computers in the future. There is already an ARM version of Windows, and I believe Microsoft will continue to support it. This is because ARM is much more energy-efficient than other architectures, and any manufacturer can build its own ARM CPUs by purchasing a license—a very interesting proposition from a business perspective.\nIt is also possible that we will move towards more cloud-based computing, though I think this is less likely than the hardware shift. In that case, it is quite likely that the actual code will be running on Linux servers that are ARM-based, due to energy and licensing costs. Here again, if you want to run your historical code, you’ll have to compile old packages and programming language versions from source.\nOk, this might all seem incredibly complicated. How on earth are we supposed to manage all these risks and balance the immediate need for results with the future need to rerun an old project? And what if rerunning it is never needed?\nAs you shall see, this is not as difficult as it sounds, thanks to Nix and the tireless efforts of the nixpkgs maintainers who work to build truly reproducible packages.\nLet’s dive in!\n\n\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–27.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "https://github.com/numpy/numpy/issues/9187↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-nix.html",
    "href": "02-nix.html",
    "title": "2  The Nix Package Manager",
    "section": "",
    "text": "2.1 Introduction\nNix is a package manager that can be installed on your computer, regardless of the operating system. If you are familiar with the Ubuntu Linux distribution, you have likely used apt-get to install software. On macOS, you may have used homebrew for similar purposes. Nix functions in a comparable way but has many advantages over classic package managers, as it focuses on reproducible builds and downloads packages from nixpkgs, currently the largest software repository1.\nIn this chapter, we will explore the critical need for environment reproducibility in modern workflows. We will see why ad-hoc tools often fail, and how Nix’s declarative approach and “component closures” provide a robust solution. We will also cover the core concepts of Nix—derivations, the store, and hermetic builds—that make this possible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "02-nix.html#why-reproducibility-why-nix",
    "href": "02-nix.html#why-reproducibility-why-nix",
    "title": "2  The Nix Package Manager",
    "section": "2.2 Why Reproducibility? Why Nix?",
    "text": "2.2 Why Reproducibility? Why Nix?\n\n2.2.1 Motivation: Reproducibility in Scientific and Data Workflows\nTo ensure that a project is reproducible you need to deal with at least four things:\n\nEnsure the required version of your programming language (R, Python, etc.) is installed.\nEnsure the required versions of all packages are installed.\nEnsure all necessary system dependencies are installed (for example, a working Java installation for the {rJava} R package on Linux).\nEnsure you can install all of this on the hardware you have on hand.\n\nBut in practice, one or most of these bullet points are missing from projects. The goal of this course is to learn how to fulfil all the requirements to build reproducible projects.\nThe current consensus for tackling the first three points is often a mixture of tools: Docker for system dependencies, {renv} or uv for package management, and tools like the R installation manager (rig) for language versions. As for the last point, hardware architecture, the only way out is to be able to compile the software for the target platform. This involves a lot of moving parts and requires significant knowledge to get right.\n\n\n2.2.2 Problems with Ad-Hoc Tools\nTools like Python’s venv or R’s renv only deal with some pieces of the reproducibility puzzle. Often, they assume an underlying OS, do not capture system-level dependencies (like libxml2, pandoc, or curl), and require users to “rebuild” their environments from partial metadata. Docker helps but introduces overhead, security challenges, and complexity, and just adding it to your project doesn’t make it reproducible if you don’t explicitly take some precautionary steps.\nTraditional approaches fail to capture the entire dependency graph of a project in a deterministic way. This leads to “it works on my machine” syndromes, onboarding delays, and subtle bugs.\n\n\n2.2.3 Nix: A Declarative Solution\nWith Nix, we can handle all of these challenges with a single tool.\nThe first advantage of Nix is that its repository, nixpkgs, is humongous. As of this writing, it contains over 120,000 pieces of software, including the entirety of CRAN and Bioconductor. This means you can use Nix to handle everything: R, Python, Julia, their respective packages, and any other software available through nixpkgs, making it particularly useful for polyglot pipelines.\nThe second and most crucial advantage is that Nix allows you to install software in (relatively) isolated environments. When you start a new project, you can use Nix to install a project-specific version of R and all its packages. These dependencies are used only for that project. If you switch to another project, you switch to a different, independent environment. But this also means that all the dependencies of R and R packages, plus all of their dependencies and so on get installed as well. Your project’s development environment will not depend on anything outside of it (well, there are some caveats which we will explore as we move on).\nThis is similar to {renv}, but the difference is profound: you get not only a project-specific library of R packages but also a project-specific R version and all the necessary system dependencies. For example, if you need {xlsx}, Nix automatically figures out that Java is required and installs and configures it for you, without any intervention.\nWhat’s more, you can pin your project to a specific revision of the nixpkgs repository. This ensures that every package Nix installs will always be at the exact same version, regardless of when or where the project is built. The environment is defined in a simple plain-text file, and anyone using that file will get a byte-for-byte identical environment, even on a different operating system.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "02-nix.html#important-concepts",
    "href": "02-nix.html#important-concepts",
    "title": "2  The Nix Package Manager",
    "section": "2.3 Important Concepts",
    "text": "2.3 Important Concepts\nBefore we start using Nix, it is important to spend some time learning about some Nix-centric concepts, starting with the derivation.\nIn Nix terminology, a derivation is a specification for running an executable on precisely defined input files to repeatably produce output files at uniquely determined file system paths. (source)\nIn simpler terms, a derivation is a recipe with precisely defined inputs, steps, and a fixed output. This means that given identical inputs and build steps, the exact same output will always be produced. To achieve this level of reproducibility, several important measures must be taken:\n\nAll inputs to a derivation must be explicitly declared (and “inputs” here is meant in a very broad sense; for example, configuration flags are also inputs!).\nInputs include not just data files but also software dependencies, configuration flags, and environment variables: essentially, anything necessary for the build process.\nThe build process takes place in a hermetic sandbox to ensure the exact same output is always produced.\n\nThe next sections explain these three points in more detail.\n\n2.3.1 Derivations\nHere is an example of a simple Nix expression:\nlet\n  pkgs = import (fetchTarball \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-04-11.tar.gz\") {};\nin\npkgs.stdenv.mkDerivation {\n  name = \"filtered_mtcars\";\n  buildInputs = [ pkgs.gawk ];\n  dontUnpack = true;\n  src = ./mtcars.csv;\n  installPhase = ''\n    mkdir -p $out\n    awk -F',' 'NR==1 || $9==\"1\" { print }' $src &gt; $out/filtered.csv\n  '';\n}\nWithout going into too much detail, this code uses awk, a common Unix data processing tool, to filter the mtcars.csv file. As you can see, a significant amount of boilerplate is required for this simple operation. However, this approach is completely reproducible: the dependencies are declared and pinned to a specific version of the nixpkgs repository. The only thing that could make this small pipeline fail is if the mtcars.csv file is not provided to it.\nNix builds the filtered.csv output file in two steps: it first generates a derivation from this expression, and only then does it build the output. For clarity, I will refer to code like the example above as a derivation rather than an expression, to avoid confusion with the concept of an expression in R.\nThe goal of the tools we will use in this book, {rix} and {rixpress} (or ryxpress if you prefer using Python), is to help you create pipelines from such derivations without needing to learn the Nix language itself, while still benefiting from its powerful reproducibility features.\n\n\n2.3.2 Dependencies of derivations\nNix requires that the dependencies of any derivation be explicitly listed and managed by Nix itself. If you are building an output that requires Quarto, then Quarto must be explicitly listed as an input, even if you already have it installed on your system. The same applies to Quarto’s dependencies, and their dependencies, all the way down. To run a linear regression with R, you essentially need Nix to build the entire universe of software that R depends on first.\nIn Nix terms, this complete set of packages is what its author, Eelco Dolstra, refers to as a component closure:\n\nThe idea is to always deploy component closures: if we deploy a component, then we must also deploy its dependencies, their dependencies, and so on. That is, we must always deploy a set of components that is closed under the ‘depends on’ relation.\n\n(Nix: A Safe and Policy-Free System for Software Deployment, Dolstra et al., 2004).\n\n\n\nFigure 4 of Dolstra et al. (2004)\n\n\nIn the figure, subversion depends on openssl, which itself depends on glibc. Similarly, if you write a derivation to filter mtcars, it requires an input file, R, {dplyr}, and all of their respective dependencies. All of these must be managed by Nix. If any dependency exists “outside” this closure, the pipeline will only work on your machine, defeating the purpose of reproducibility.\n\n\n2.3.3 The Nix store and hermetic builds\nWhen building derivations, their outputs are saved into the Nix store. Typically located at /nix/store/, this folder contains all the software and build artefacts produced by Nix.\nFor example, the output of a derivation might be stored at a path like /nix/store/81k4s9q652jlka0c36khpscnmr8wk7jb-filtered. The long cryptographic hash uniquely identifies the build output and is computed based on the content of the derivation and all its inputs. This ensures that the build is fully reproducible.\nAs a result, building the same derivation on two different machines will yield the same cryptographic hash. You can substitute the built artefact with the derivation that generates it one-to-one, just as in mathematics, where writing \\(f(2)\\) is the same as writing \\(4\\) for the function \\(f(x) := x^2\\).\nTo guarantee that derivations always produce identical outputs, builds must occur in an isolated environment known as a hermetic sandbox. This process ensures that the build is unaffected by external factors, such as the state of the host system. This isolation extends to environment variables and even network access. If you need to download data from an API, for example, it will not work from within the build sandbox. This may seem restrictive, but it makes perfect sense for reproducibility: an API’s output can change over time.\nFor a truly reproducible result, you should obtain the data once, version it, and use that archived data as an input to your analysis.\n\n\n2.3.4 Other key Nix concepts\nWe’ve covered derivations, dependencies, closures, the Nix store, and hermetic builds. That’s the core of what makes Nix tick. But there are a few more concepts worth knowing about before we move on:\n\nPurity: Nix tries very hard to keep builds “pure”: the output only depends on what you explicitly list as inputs. If a build script tries to reach out to the internet or read some random file on your machine, Nix will block it. That can feel restrictive at first, but it’s what guarantees reproducibility.\nBinary caches: You don’t always need to build everything yourself. Think back to the math analogy: if you already know that \\(f(2) = 4\\), there’s no need to compute it again; just reuse the result! Nix does the same with binary caches: because every build is identified by its unique cryptographic hash made from its inputs, this means a prebuilt package fetched from cache.nixos.org (or your own cache you may want to set up) is bit-for-bit identical to what you would have built locally. This is why Nix is both reproducible and fast.\nGarbage collection: Since Nix never overwrites anything in the store, old packages can pile up. Running nix-store --gc will run the garbage collector to free up space.\nOverlays: If you want to tweak a package or add your own without forking all of nixpkgs, you can use overlays. They let you extend or override existing definitions in a clean, composable way.\nFlakes: The newer way to define and pin Nix projects. Flakes make it easier to share and reuse Nix setups across machines and repositories. There is a lot of discussion around flakes, as they’re officially still considered not stable, even though they’ve been widely adopted by the community. But don’t worry, this is not something you’ll need to think about for this book.\n\nTogether, these features explain why Nix isn’t just another package manager. It’s more like a framework for reproducible environments that can scale from a single project to an entire operating system (called NixOS2).\nAs a little sidenote: I want also to highlight that Nix can even be used to declaratively and reproducibly configure your operating system (be it NixOS, macOS or other Linux distributions) using a tool that integrates with it called homemanager.3 This is outside the scope of this book, but I wanted to highlight it, as it’s extremely powerful. What this means in practice is that you could write a whole Nix expression that not only downloads and configures software, but even sets up users with their specific software, and preferences like wallpapers, colour schemes and so on.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "02-nix.html#caveats",
    "href": "02-nix.html#caveats",
    "title": "2  The Nix Package Manager",
    "section": "2.4 Caveats",
    "text": "2.4 Caveats\nWhile Nix is powerful, there are some limitations and practical hurdles to be aware of if you plan to use it for actual work:\n\nHardware acceleration: On non-NixOS systems, it can be difficult to set up GPU acceleration (CUDA, ROCm, OpenCL). Drivers are tightly coupled to the host kernel and libraries, while Nix builds aim for strict isolation. On NixOS this integration is smoother, but on macOS or other Linux distros you may encounter limitations or extra manual steps.\nmacOS-specific issues: Reproducibility is harder to achieve on macOS (both Intel and Apple Silicon) than on Linux. Nix packages often rely on Apple system frameworks (e.g., CoreFoundation, Security) that live outside the Nix store and compromise hermetic builds4. The macOS sandbox is also weaker than Linux’s and sometimes leaks system tools like Xcode or Rosetta into builds5. Hydra cache coverage is thinner for Darwin platforms, especially Apple Silicon, so cache misses and local builds are much more common6. Finally, pinned environments can break after macOS or Xcode updates because of changes in system libraries or compiler flags7.\nThat said, in practice most packages build fine on macOS, and the ecosystem continues to improve. When reproducibility problems do appear, the simplest fix is often just to use another nearby nixpkgs revision for pinning that is known to work. This makes the situation less fragile than it might first appear. Another solution would be to use Docker to deploy the Nix environment and use that as a dev container. All of this will be discussed in detail in this book.\nSteep learning curve: The Nix language and ecosystem (flakes, overlays, derivations) can be conceptually difficult if you come from traditional package managers. Even basic customizations require some ramp-up time. This was my main motivation to write {rix}, {rixpress} (for R) and ryxpress (for Python).\nDisk space and builds: Because Nix never mutates software in place, the store can accumulate large amounts of data. Cache misses sometimes force local builds, which can be slow and resource-intensive. However, it is of course possible to empty the Nix store to recover disk space, and it is also possible to set up your own project cache if you wish so. Setting up your own cache will also be something that we will explore in this book.\n\nThese caveats don’t diminish Nix’s strengths but highlight that its guarantees are strongest on Linux (and thus WSL), and especially on NixOS. On macOS, reproducibility is possible but sometimes requires extra work, a bit of flexibility, and occasionally picking a different nixpkgs snapshot.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "02-nix.html#in-summary",
    "href": "02-nix.html#in-summary",
    "title": "2  The Nix Package Manager",
    "section": "2.5 In summary",
    "text": "2.5 In summary\nNix makes it possible to actually build software reproducibly. To achieve this, it introduces several core concepts that are quite specific, and definitely worth taking the time to understand.\nIn the next chapter, we will learn how to install Nix, configure cachix, and set up Positron for a seamless development experience.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "02-nix.html#footnotes",
    "href": "02-nix.html#footnotes",
    "title": "2  The Nix Package Manager",
    "section": "",
    "text": "https://repology.org/repositories/graphs↩︎\nhttps://nixos.org/↩︎\nhttps://github.com/nix-community/home-manager↩︎\nhttps://github.com/NixOS/nixpkgs/issues/67166↩︎\nhttps://discourse.nixos.org/t/nix-macos-sandbox-issues-in-nix-2-4-and-later/17475↩︎\nhttps://www.reddit.com/r/NixOS/comments/17uxj6q/how_does_nix_package_manager_work_on_apple_silicon/↩︎\nhttps://github.com/NixOS/nix/issues/11679↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "03-setup.html",
    "href": "03-setup.html",
    "title": "3  Setting Up Your Environment",
    "section": "",
    "text": "3.1 Installing Nix\nIn this chapter, we will install Nix, configure the rstats-on-nix binary cache to speed up installations, and set up our development environment (Positron, VS Code, or Emacs) to work seamlessly with reproducible Nix environments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "03-setup.html#installing-nix",
    "href": "03-setup.html#installing-nix",
    "title": "3  Setting Up Your Environment",
    "section": "",
    "text": "3.1.1 For Windows Users: WSL2 Prerequisites\nIf you are on Windows, you need the Windows Subsystem for Linux 2 (WSL2) to run Nix. If you are on a recent version of Windows 10 or 11, you can simply run this as an administrator in PowerShell:\nwsl --install\nYou can find further installation notes at this official MS documentation.\nI recommend activating systemd in Ubuntu WSL2, mainly because this supports users other than root running Nix. To set this up, please follow this official Ubuntu blog entry:\n# in WSL2 Ubuntu shell\n\nsudo -i\nnano /etc/wsl.conf\nThis will open /etc/wsl.conf in nano, a command line text editor. Add the following line:\n[boot]\nsystemd=true\nSave the file with CTRL-O and then quit nano with CTRL-X. Then, type the following line in powershell:\nwsl --shutdown\nand then relaunch WSL (Ubuntu) from the start menu. For those of you running Windows, we will be working exclusively from WSL2 now. If that is not an option, then I highly recommend you set up a virtual machine with Ubuntu using VirtualBox for example, or dual-boot Ubuntu.\n\n\n3.1.2 The Determinate Systems installer\nInstalling (and uninstalling) Nix is quite simple, thanks to the installer from Determinate Systems, a company that provides services and tools built on Nix, and works the same way on Linux (native or WSL2) and macOS.\nDo not use your operating system’s package manager to install Nix. Instead, simply open a terminal and run the following line (on Windows, run this inside WSL, and after the prerequisites listed above):\n\ncurl --proto '=https' --tlsv1.2 -sSf \\\n  -L https://install.determinate.systems/nix | \\\n  sh -s -- install\n\nJust follow the instructions on screen, and in no time Nix will be available on your machine!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "03-setup.html#configuring-the-cache",
    "href": "03-setup.html#configuring-the-cache",
    "title": "3  Setting Up Your Environment",
    "section": "3.2 Configuring the Cache",
    "text": "3.2 Configuring the Cache\nNext, install the cachix client and configure the rstats-on-nix cache: this will install binary versions of many R packages which will speed up the building process of environments:\nnix-env -iA cachix -f https://cachix.org/api/v1/install\nthen use the cache:\ncachix use rstats-on-nix\nYou only need to do this once per machine you want to use {rix} on. Many thanks to Cachix for sponsoring the rstats-on-nix cache!\nIf you get this warning when trying to install software with Nix:\nwarning: ignoring the client-specified setting 'trusted-public-keys', because it is a restricted setting and you are not a trusted user\nwarning: ignoring untrusted substituter 'https://rstats-on-nix.cachix.org', you are not a trusted user.\nRun `man nix.conf` for more information on the `substituters` configuration option.\nwarning: ignoring the client-specified setting 'trusted-public-keys', because it is a restricted setting and you are not a trusted user\nThen this means that configuration was not successful. You need to add your user to /etc/nix/nix.custom.conf:\nsudo nano /etc/nix/nix.custom.conf\nthen simply add this line in the file:\ntrusted-users = root YOURUSERNAME\nwhere YOURUSERNAME is your current login user name.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "03-setup.html#verifying-installation-with-temporary-shells",
    "href": "03-setup.html#verifying-installation-with-temporary-shells",
    "title": "3  Setting Up Your Environment",
    "section": "3.3 Verifying installation with Temporary Shells",
    "text": "3.3 Verifying installation with Temporary Shells\nYou now have Nix installed; before continuing, let’s see if everything works (close all your terminals and reopen them) by dropping into a temporary shell with a tool you likely have not installed on your machine.\nOpen a terminal and run:\nwhich sl\nyou will likely see something like this:\nwhich: no sl in ....\nnow run this:\nnix-shell -p sl\nand then again:\nwhich sl\nthis time you should see something like:\n/nix/store/cndqpx74312xkrrgp842ifinkd4cg89g-sl-5.05/bin/sl\nThis is the path to the sl binary installed through Nix. The path starts with /nix/store: the Nix store is where all the software installed through Nix is stored. Now type sl and see what happens!\nTemporary shells are quite useful, especially if you want to simply run a command using some tool that you only need infrequently. But this is not how we are going to be using Nix.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "03-setup.html#configuring-your-ide",
    "href": "03-setup.html#configuring-your-ide",
    "title": "3  Setting Up Your Environment",
    "section": "3.4 Configuring your IDE",
    "text": "3.4 Configuring your IDE\n\n3.4.1 Pre-requisites\nWe now need to configure an IDE to use our Nix shells as development environments. You are free to use whatever IDE you want but the instructions below are going to focus on Positron, which is a fork of VS Code geared towards data science. It works well with both Python and R and makes it quite easy to choose the right R or Python interpreter (which you’ll have to do to make sure you’re using the one provided by Nix, see here).\nIf you want to use VS Code proper, you can follow all the instructions here, but you need to install the REditorSupport and the Python extension. You will also need to add the {languageserver} R package to your Nix shells.\nOn Windows, you need to install Positron on Windows, not inside WSL.\nIf you want to use RStudio, you can, but you will need to install it through Nix: an RStudio installed through the usual means for your system is not going to be able to interact with Nix! This is a limitation of RStudio and there is currently no workaround. If you want to use RStudio, just skip the rest of the chapter, as it’s irrelevant to you. Later, when we use {rix} to set up our first Nix development environment, I’ll tell you what to do to use RStudio.\n\n\n3.4.2 direnv\nOnce Positron is installed, you need to install a piece of software called direnv: direnv will automatically load Nix shells when you open a project that contains a default.nix file in an editor.1 It works on any operating system and many editors support it, including Positron. If you’re using Windows, install direnv in WSL (even though you’ve just installed Positron for Windows). To install direnv run this command:\nnix-env -f '&lt;nixpkgs&gt;' -iA direnv\nThis will install direnv and make it available even outside of Nix shells!\nThen, I highly recommend to install the nix-direnv extension:\nnix-env -f '&lt;nixpkgs&gt;' -iA nix-direnv\nIt is not mandatory to use nix-direnv if you already have direnv, but it’ll make loading environments much faster and seamless.\nFinally, if you haven’t used direnv before, don’t forget this last step to make your terminal detect and load direnv automatically.\nThen, in Positron, install the direnv extension. Finally, add a file called .envrc and simply write the following two lines in it (this .envrc file should be in the same folder as your project’s default.nix):\nuse nix\nmkdir $TMP\nin it. On Windows, remotely connect to WSL first, but on other operating systems, simply open the project’s folder using File &gt; Open Folder... and you will see a pop-up stating direnv: /PATH/TO/PROJECT/.envrc is blocked and a button to allow it. Click Allow and then open an R script. You might get another pop-up asking you to restart the extension, so click Restart. Be aware that at this point, direnv will run nix-shell and so will start building the environment. If that particular environment hasn’t been built and cached yet, it might take some time before Code will be able to interact with it. You might get yet another popup, this time from the R Code extension complaining that R can’t be found. In this case, simply restart Positron and open the project folder again: now it should work every time.\n\n\n3.4.3 In summary and next steps\nFor a new project, simply repeat this process:\n\nGenerate the project’s default.nix file (we will see how in the next chapter);\nBuild it using nix-build;\nCreate an .envrc and write the two lines from above in it;\nOpen the project’s folder in Positron and click allow when prompted;\nRestart the extension and Positron if necessary.\n\nAnother option is to create the .envrc file and write the two required lines, then open a terminal, navigate to the project’s folder, and run direnv allow. Doing this before opening Positron should not prompt you anymore.\nIf you’re on Windows, using Positron like this is particularly interesting, because it allows you to install Positron on Windows as usual, and then you can configure it to interact with a Nix shell, even if it’s running from WSL. This is a very seamless experience.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "03-setup.html#footnotes",
    "href": "03-setup.html#footnotes",
    "title": "3  Setting Up Your Environment",
    "section": "",
    "text": "A default.nix file is a file that contains the specification of our development environments, and is the file that will be automatically generated by {rix}↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "04-rix.html",
    "href": "04-rix.html",
    "title": "4  Reproducible Development Environments with rix",
    "section": "",
    "text": "4.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#introduction",
    "href": "04-rix.html#introduction",
    "title": "4  Reproducible Development Environments with rix",
    "section": "",
    "text": "4.1.1 The Reproducibility Challenge\nReproducibility in research and data science exists on a continuum. At one end, authors might only describe their methods in prose. Moving along the spectrum, they might share code, then data, and finally what we call a computational environment: the complete set of software required to execute an analysis.\nEven when researchers share code and data, they rarely specify the full software stack: the exact version of R, all package versions, and crucially, the system-level dependencies. Yet differences in any of these can lead to divergent results from the same code.\nTools like {renv} address part of the problem—they capture R package versions in a lockfile. But {renv} doesn’t manage the R version itself (you need rig for that), and neither handles system libraries. If {sf} requires GDAL 3.0 but your system has 2.4, {renv} can’t help. And if your project uses both R and Python? Now you’re coordinating multiple package managers, each with its own configuration.\n\n\n4.1.2 Component Closures: The Nix Approach\nThis is where Nix shines. Nix deploys component closures: when you install a package, Nix also installs all its dependencies, their dependencies, and so on. Think of it like packing for a trip—traditional package managers assume you’ll find essentials at your destination, while Nix packs everything you need.\nAs the original Nix paper explains:\n\nThe idea is to always deploy component closures: if we deploy a component, then we must also deploy its dependencies, their dependencies, and so on. Since closures are self-contained, they are the units of complete software deployment.\n\nThis means when you install {sf} through Nix, you automatically get the correct versions of GDAL, GEOS, and PROJ—no manual system configuration needed.\n\n\n4.1.3 The Polyglot Challenge\nModern data science is increasingly polyglot. Research shows that data scientists use, on average, nearly two programming languages in their work, with R and Python being the most common combination. Python dominates machine learning, R excels at statistical modelling, and Julia offers high-performance numerics. Projects increasingly combine these strengths.\nThis creates a reproducibility challenge: a project using R, Python, and Quarto requires coordinating multiple package managers. Nix solves this by providing a unified framework for all languages and system tools.\n\n\n4.1.4 Enter rix\nHowever, Nix has a steep learning curve. Its functional programming language can be daunting for researchers focused on their analysis, not system administration.\n{rix} bridges this gap. It’s an R package that generates Nix expressions from intuitive R function calls. You describe what you want, and {rix} figures out how to express it in Nix. The workflow is simple:\n\nDeclare your environment using rix()\nBuild the environment using nix-build\nUse the environment using nix-shell\n\nThis chapter covers everything you need to know to create project-specific, reproducible development environments for your R projects.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#the-rix-function",
    "href": "04-rix.html#the-rix-function",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.2 The rix() Function",
    "text": "4.2 The rix() Function\nThe rix() function is the heart of the package. It generates a default.nix file—a Nix expression that defines your development environment. Here are its main arguments:\n\nr_ver: the version of R you need (or use date instead)\ndate: a date corresponding to a CRAN snapshot\nr_pkgs: R packages to install from CRAN/Bioconductor\nsystem_pkgs: system tools like quarto or git\ngit_pkgs: R packages to install from GitHub\nlocal_r_pkgs: local .tar.gz packages to install\ntex_pkgs: TexLive packages for literate programming\nide: which IDE to configure (\"rstudio\", \"code\", \"positron\", \"none\")\nproject_path: where to save the default.nix file\noverwrite: whether to overwrite an existing default.nix\n\nLet’s create our first environment. Suppose you need R with {dplyr} and {ggplot2}:\n\nlibrary(rix)\n\nrix(\n  r_ver = \"4.4.2\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  ide = \"code\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nThis generates two files:\n\ndefault.nix: The Nix expression defining your environment\n.Rprofile: Created by rix_init() (called automatically), this file prevents conflicts with any system-installed R packages\n\nThe .Rprofile is important: it ensures that packages from your user library don’t get loaded into the Nix environment, and it redefines install.packages() to throw an error—because you should never install packages that way in a Nix environment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#choosing-an-r-version-or-date",
    "href": "04-rix.html#choosing-an-r-version-or-date",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.3 Choosing an R Version or Date",
    "text": "4.3 Choosing an R Version or Date\nThe r_ver argument (or alternatively date) controls which version of R and which package versions you’ll get. Here’s a summary of the options:\n\n\n\n\n\n\n\n\n\nr_ver / date\nIntended Use\nR Version\nPackage Versions\n\n\n\n\n\"latest-upstream\"\nNew project, versions don’t matter\nCurrent/previous\nUp to 6 months old\n\n\n\"4.4.2\" (or similar)\nReproduce old project or start new\nSpecified version\nUp to 2 months old\n\n\ndate = \"2024-12-14\"\nPrecise CRAN snapshot\nCurrent at that date\nExact versions from date\n\n\n\"bleeding-edge\"\nDevelop against latest CRAN\nAlways current\nAlways current\n\n\n\"frozen-edge\"\nLatest CRAN, manual updates\nCurrent at generation\nCurrent at generation\n\n\n\"r-devel\"\nTest against R development version\nR-devel\nAlways current\n\n\n\nTo see which R versions are available:\n\navailable_r()\n\nTo see which dates are available for snapshotting:\n\navailable_dates()\n\n\n4.3.1 Using dates vs versions\nUsing a specific date is often the best choice for reproducibility. When you specify a date, you get the exact state of CRAN on that day:\n\nrix(\n  date = \"2024-12-14\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  ide = \"code\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\n\n\n4.3.2 The rstats-on-nix fork\nWhen you use a specific R version or date (rather than \"latest-upstream\"), {rix} uses our rstats-on-nix fork of nixpkgs rather than the upstream repository. This fork:\n\nProvides better compatibility for older packages\nIncludes many fixes, especially for Apple Silicon\nOffers newer R releases faster than the official channels\nSnapshots CRAN more frequently",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#installing-r-packages",
    "href": "04-rix.html#installing-r-packages",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.4 Installing R Packages",
    "text": "4.4 Installing R Packages\n\n4.4.1 From CRAN/Bioconductor\nThe simplest case—just list package names in r_pkgs:\n\nrix(\n  r_ver = \"4.4.2\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\", \"tidyr\", \"readr\"),\n  ide = \"code\",\n  project_path = \".\"\n)\n\nBoth CRAN and Bioconductor packages can be specified this way.\n\n\n4.4.2 Installing archived versions\nNeed a specific old version of a package? Use the @ syntax:\n\nrix(\n  r_ver = \"4.2.1\",\n  r_pkgs = c(\"dplyr@0.8.0\", \"janitor@1.0.0\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nThis will install {dplyr} version 0.8.0 from the CRAN archives. Note that archived packages are built from source, which may fail for packages requiring compilation.\n\n\n4.4.3 Installing from GitHub\nFor packages on GitHub, use the git_pkgs argument with a list containing the package name, repository URL, and commit hash:\n\nrix(\n  r_ver = \"4.4.2\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  git_pkgs = list(\n    list(\n      package_name = \"housing\",\n      repo_url = \"https://github.com/rap4all/housing/\",\n      commit = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\"\n    )\n  ),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nAlways specify a commit hash, not a branch name. This ensures reproducibility— branch names can change, but commits are immutable.\nIf the R package lives in a subfolder of the repository, append the subfolder to the URL:\n\ngit_pkgs = list(\n  package_name = \"BPCells\",\n  repo_url = \"https://github.com/bnprks/BPCells/r\",  # Note the /r suffix\n  commit = \"16faeade0a26b392637217b0caf5d7017c5bdf9b\"\n)\n\n\n\n4.4.4 Installing local packages\nFor local .tar.gz archives, place them in the same directory as your default.nix and use local_r_pkgs:\n\nrix(\n  r_ver = \"4.3.1\",\n  local_r_pkgs = c(\"mypackage_1.0.0.tar.gz\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\n\n\n4.4.5 Why NOT to use install.packages()\nIt’s crucial to understand: never call install.packages() from within a Nix environment. Here’s why:\n\nDeclarative environments: If you install packages imperatively, your default.nix no longer matches your actual environment.\nLeaking packages: Packages installed via install.packages() go to your user library, not the Nix environment. They’ll be visible to all Nix shells, breaking isolation.\nReproducibility: The whole point of Nix is that your environment is fully defined by the default.nix. Ad-hoc installations defeat this.\n\nInstead, add packages to your rix() call and rebuild the environment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#system-tools-and-texlive",
    "href": "04-rix.html#system-tools-and-texlive",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.5 System Tools and TexLive",
    "text": "4.5 System Tools and TexLive\n\n4.5.1 Adding system packages\nNeed command-line tools like Quarto or Git? Add them via system_pkgs:\n\nrix(\n  r_ver = \"latest-upstream\",\n  r_pkgs = c(\"quarto\"),\n  system_pkgs = c(\"quarto\", \"git\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nNote that here we install both the R {quarto} package and the quarto command-line tool—they’re different things!\nTo find available packages, search at search.nixos.org.\n\n\n4.5.2 TexLive for literate programming\nFor PDF output from Quarto, R Markdown, or Sweave, you need TexLive packages:\n\nrix(\n  r_ver = \"latest-upstream\",\n  r_pkgs = c(\"quarto\"),\n  system_pkgs = \"quarto\",\n  tex_pkgs = c(\"amsmath\", \"framed\", \"fvextra\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nThis installs the scheme-small TexLive distribution plus the specified packages.\n\n\n4.5.3 Python and Julia integration\n{rix} can also add Python or Julia to your environment:\n\nrix(\n  date = \"2025-02-17\",\n  r_pkgs = \"ggplot2\",\n  py_conf = list(\n    py_version = \"3.12\",\n    py_pkgs = c(\"polars\", \"great-tables\")\n  ),\n  ide = \"positron\",\n  project_path = \".\"\n)\n\nThis creates a polyglot environment with R, Python, and the specified packages for each.\n\n\n4.5.4 Installing Python packages with uv (impure)\nNot all Python packages are available through Nix. Unlike CRAN, PyPI doesn’t get automatically mirrored—individual packages must be packaged by volunteers. If a Python package you need isn’t in nixpkgs, you can use uv as an escape hatch.\nThis approach is also useful when collaborating with colleagues who use uv but haven’t adopted Nix yet. uv is 10–100x faster than pip and generates a lock file for improved reproducibility.\nThe idea is to install uv in your shell (but not Python or Python packages through Nix):\n\nrix(\n  date = \"2025-02-17\",\n  r_pkgs = \"ggplot2\",\n  system_pkgs = c(\"uv\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nThen use uv from within your shell. We recommend:\n\nSpecify Python packages in a requirements.txt file with explicit versions (e.g., scanpy==1.11.4)\nSet up a shell hook to automatically configure the virtual environment\n\nHere’s a complete example with a shell hook:\n\nrix(\n  date = \"2025-02-17\",\n  r_pkgs = \"ggplot2\",\n  system_pkgs = c(\"uv\"),\n  shell_hook = \"\n    if [ ! -f pyproject.toml ]; then\n      uv init --python 3.13.5\n    fi\n    uv add --requirements requirements.txt\n    alias python='uv run python'\n  \",\n  ide = \"none\",\n  project_path = \".\"\n)\n\nAfter running nix-shell, uv initialises a Python project with the specified version and installs packages from requirements.txt. This happens each time you enter the shell, but uv caches everything so it’s nearly instant after the first run.\n\n4.5.4.1 Troubleshooting wheel issues\nWhen using wheels (pre-compiled Python packages), you may encounter errors like:\nImportError: libstdc++.so.6: cannot open shared object file\nThis happens because wheels expect certain libraries in certain locations. Add this to your shell hook to fix it:\nshellHook = ''\n  export LD_LIBRARY_PATH=\"${pkgs.lib.makeLibraryPath (with pkgs; [ \n    zlib gcc.cc glibc stdenv.cc.cc \n  ])}\":$LD_LIBRARY_PATH\n  # ... rest of your hook\n'';\nIf this seems complicated: yes, it is. This is exactly the kind of problem Nix aims to solve. When possible, prefer Python packages included in nixpkgs.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#building-and-using-environments",
    "href": "04-rix.html#building-and-using-environments",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.6 Building and Using Environments",
    "text": "4.6 Building and Using Environments\n\n4.6.1 Building with nix-build\nOnce you have a default.nix, build the environment:\nnix-build\nThis downloads/builds all required packages and creates a result symlink in your project directory. The result file prevents the environment from being garbage-collected.\n\n\n4.6.2 Entering with nix-shell\nTo use the environment interactively:\nnix-shell\nYou’ll drop into a shell where R and all your packages are available. Type R to start an R session, or start your editor directly.\n\n\n4.6.3 Running scripts\nYou can run scripts directly without entering the shell:\nnix-shell default.nix --run \"Rscript analysis.R\"\nOr run a {targets} pipeline:\nnix-shell default.nix --run \"Rscript -e 'targets::tar_make()'\"\n\n\n4.6.4 Pure shells for complete isolation\nBy default, nix-shell can still see programs installed on your system. For complete isolation:\nnix-shell --pure\nThis hides everything not explicitly included in your environment.\n\n\n4.6.5 Garbage collection\nNix never deletes old packages automatically. To clean up:\n\nDelete the result symlink\nRun nix-store --gc\n\nThis removes all packages that are no longer referenced by any environment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#converting-renv-projects",
    "href": "04-rix.html#converting-renv-projects",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.7 Converting renv Projects",
    "text": "4.7 Converting renv Projects\nIf you have existing projects using {renv}, the renv2nix() function can help you migrate:\n\nrenv2nix(\n  renv_lock_path = \"path/to/project/renv.lock\",\n  project_path = \"path/to/new_nix_project\"\n)\n\n\n4.7.1 Recommended workflow\n\nCopy the renv.lock file to a new, empty folder\nRun renv2nix() pointing to that folder\nBuild the environment with nix-build\n\nDo not convert in the same folder as the original {renv} project—the generated .Rprofile will conflict with {renv}’s .Rprofile.\n\n\n4.7.2 Caveats\n\nPackage versions may not match exactly due to how Nix handles snapshotting\nIf the renv.lock lists an old R version but recent packages, use the override_r_ver argument to specify a more appropriate R version",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#summary",
    "href": "04-rix.html#summary",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.8 Summary",
    "text": "4.8 Summary\nIn this chapter, we learned how to use {rix} to create reproducible development environments:\n\nThe rix() function generates default.nix files from R\nChoose R versions via r_ver or precise CRAN snapshots via date\nInstall packages from CRAN, Bioconductor, GitHub, or local archives\nAdd system tools, TexLive packages, and even Python/Julia\nBuild with nix-build, use with nix-shell\nConvert existing {renv} projects with renv2nix()\n\nThe key insight is that your environment is now declared in a text file. Anyone with that file can recreate the exact same environment, on any machine, at any point in the future. That’s the power of Nix.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "05-fp.html",
    "href": "05-fp.html",
    "title": "5  Functional Programming",
    "section": "",
    "text": "5.1 Introduction: From Scripts to Functions\nIn this chapter, we will see why functional programming is crucial for reproducible, testable, and collaborative data science. We will compare how to write self-contained, “pure” functions in both R and Python, and how to use functional concepts like map, filter, and reduce to replace error-prone loops. Finally, we will discuss how writing functions makes your code easier to review, debug, and even generate with LLMs.\nIn the previous chapter, we learned how to create reproducible development environments with {rix}. We can now ensure everyone has the exact same tools—R, Python, system libraries—to run our code.\nBut having the right tools is only half the battle. Now we turn to writing reproducible code itself. A common way to start a data analysis is by writing a script: a sequence of commands executed from top to bottom.\n# R script example\nlibrary(dplyr)\ndata(mtcars)\nheavy_cars &lt;- filter(mtcars, wt &gt; 4)\nmean_mpg_heavy &lt;- mean(heavy_cars$mpg)\nprint(mean_mpg_heavy)\nThis works, but it has a hidden, dangerous property: state. The script relies on variables like heavy_cars existing in the environment, making the code hard to reason about, debug, and test.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#introduction-from-scripts-to-functions",
    "href": "05-fp.html#introduction-from-scripts-to-functions",
    "title": "5  Functional Programming",
    "section": "",
    "text": "# Python script example\nimport pandas as pd\nmtcars = pd.read_csv(\"mtcars.csv\")\nheavy_cars = mtcars[mtcars['wt'] &gt; 4]\nmean_mpg_heavy = heavy_cars['mpg'].mean()\nprint(mean_mpg_heavy)\n\n\n5.1.1 The Notebook Problem\nIf scripting with state is a crack in the foundation of reproducibility, then using computational notebooks is a gaping hole.\nNotebooks like Jupyter introduce an even more insidious form of state: the cell execution order. You can execute cells out of order, meaning the visual layout of your code has no relation to how it actually ran. This is a recipe for non-reproducible results and a primary cause of the “it worked yesterday, why is it broken today?” problem.\n\n\n5.1.2 The Functional Solution\nThe solution is to embrace a paradigm that minimises state: Functional Programming (FP). Instead of a linear script, we structure our code as a collection of self-contained, predictable functions.\nThe power of FP comes from the concept of purity, borrowed from mathematics. A mathematical function has a beautiful property: for a given input, it always returns the same output. sqrt(4) is always 2. Its result doesn’t depend on what you calculated before or on a random internet connection.\nOur Nix environments handle the “right library” problem; purity handles the “right logic” problem. Our goal is to write our analysis code with this same level of rock-solid predictability.\n\n\n5.1.3 FP vs OOP: Transformations vs Actors\nTo appreciate what FP brings, it helps to contrast it with Object-Oriented Programming (OOP), arguably the dominant paradigm in many software systems.\nOOP organises computation around who does what—a network of objects communicating with each other and managing their own internal state. You send a message to an object, asking it to perform an action, without needing to know how it works internally.\nFunctional programming, by contrast, organises computation around how data changes. It replaces a network of interacting objects with a flow of transformations: data goes in, data comes out, and nothing else changes in the process.\nThis shift is especially powerful in data science:\n\nAnalyses are naturally expressed as pipelines of transformations (cleaning, filtering, aggregating, modelling)\nPure functions make results reproducible—same inputs always yield same outputs\nImmutability prevents accidental side effects on shared data\nBecause transformations can be composed, tested, and reused independently, FP encourages modular, maintainable analysis code\n\n\n\n5.1.4 Why Does This Matter for Data Science?\nAdopting a functional style brings massive benefits:\n\nUnit Testing is Now Possible: You can’t easily test a 200-line script. But you can easily test a small function that does one thing.\nCode Review is Easier: A Pull Request that just adds or modifies a single function is simple for your collaborators to understand and approve.\nWorking with LLMs is More Effective: It’s incredibly effective to ask, “Write a Python function that takes a pandas DataFrame and a column name, and returns the mean of that column, handling missing values. Also, write three pytest unit tests for it.”\nReadability: Well-named functions are self-documenting:\n\nstarwars %&gt;% \n  group_by(species) %&gt;% \n  summarize(mean_height = mean(height))\n\nis instantly understandable. The equivalent for loop is a puzzle.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#purity-and-side-effects",
    "href": "05-fp.html#purity-and-side-effects",
    "title": "5  Functional Programming",
    "section": "5.2 Purity and Side Effects",
    "text": "5.2 Purity and Side Effects\nA pure function has two rules:\n\nIt only depends on its inputs. It doesn’t use any “global” variables defined outside the function.\nIt doesn’t change anything outside of its own scope. It doesn’t modify a global variable or write a file to disk. This is called having “no side effects.”\n\nConsider this “impure” function in Python:\n# IMPURE: Relies on a global variable\ndiscount_rate = 0.10\n\ndef calculate_discounted_price(price):\n    return price * (1 - discount_rate)  # What if discount_rate changes?\n\nprint(calculate_discounted_price(100))\n# &gt; 90.0\ndiscount_rate = 0.20  # Someone changes the state\nprint(calculate_discounted_price(100))\n# &gt; 80.0  -- Same input, different output!\nThe pure version passes all its dependencies as arguments:\n# PURE: All inputs are explicit arguments\ndef calculate_discounted_price_pure(price, rate):\n    return price * (1 - rate)\n\nprint(calculate_discounted_price_pure(100, 0.10))\n# &gt; 90.0\nprint(calculate_discounted_price_pure(100, 0.20))\n# &gt; 80.0\nNow the function is predictable and self-contained.\n\n5.2.1 Handling “Impure” Operations like Randomness\nSome operations, like generating random numbers, are inherently impure. Each time you run rnorm(10) or numpy.random.rand(10), you get a different result.\nThe functional approach is not to avoid this, but to control it by making the source of impurity (the random seed) an explicit input.\nIn R, the {withr} package helps create a temporary, controlled context:\n\nlibrary(withr)\n\n# This function is now pure! For a given seed, the output is always the same.\npure_rnorm &lt;- function(n, seed) {\n  with_seed(seed, {\n    rnorm(n)\n  })\n}\n\npure_rnorm(n = 5, seed = 123)\npure_rnorm(n = 5, seed = 123)  # Same result!\n\nIn Python, numpy provides a more modern, object-oriented way:\nimport numpy as np\n\n# Create a random number generator instance with a seed\nrng = np.random.default_rng(seed=123)\nprint(rng.standard_normal(5))\n\n# If we re-create the same generator, we get the same numbers\nrng2 = np.random.default_rng(seed=123)\nprint(rng2.standard_normal(5))\nThe key is the same: the “state” (the seed) is explicitly managed, not hidden globally.\n\n\n5.2.2 The OOP Caveat in Python\nThis introduces a concept from OOP: the rng variable is an object that bundles together data (its internal seed state) and methods (.standard_normal()). This is encapsulation.\nThis is a double-edged sword for reproducibility. The rng object is now a stateful entity. If we called rng.standard_normal(5) a second time, it would produce different numbers because its internal state was mutated.\nCore Python libraries like pandas, scikit-learn, and matplotlib are fundamentally object-oriented. Our guiding principle must be:\n\nUse functions for the flow and logic of your analysis, and treat objects from libraries as values that are passed between these functions.\n\nAvoid building your own complex classes with hidden state for your data pipeline. A pipeline composed of functions (df2 = clean_data(df1); df3 =  analyze_data(df2)) is almost always more transparent than an OOP one (pipeline.load(); pipeline.clean(); pipeline.analyze()).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#writing-your-own-functions",
    "href": "05-fp.html#writing-your-own-functions",
    "title": "5  Functional Programming",
    "section": "5.3 Writing Your Own Functions",
    "text": "5.3 Writing Your Own Functions\nLet’s learn the syntax. The goal is always to encapsulate a single, logical piece of work.\n\n5.3.1 In R\nR functions are first-class citizens. You can assign them to variables and pass them to other functions.\n\n# A simple function\ncalculate_ci &lt;- function(x, level = 0.95) {\n  se &lt;- sd(x, na.rm = TRUE) / sqrt(length(x))\n  mean_val &lt;- mean(x, na.rm = TRUE)\n  \n  alpha &lt;- 1 - level\n  lower &lt;- mean_val - qnorm(1 - alpha/2) * se\n  upper &lt;- mean_val + qnorm(1 - alpha/2) * se\n  \n  c(mean = mean_val, lower = lower, upper = upper)\n}\n\n# Use it\ndata &lt;- c(1.2, 1.5, 1.8, 1.3, 1.6, 1.7)\ncalculate_ci(data)\n\nFor data analysis, you’ll often want functions that work with column names. The {dplyr} package uses “tidy evaluation”:\n\nlibrary(dplyr)\n\nsummarize_variable &lt;- function(dataset, var_to_summarize) {\n  dataset %&gt;%\n    summarise(\n      n = n(),\n      mean = mean({{ var_to_summarize }}, na.rm = TRUE),\n      sd = sd({{ var_to_summarize }}, na.rm = TRUE)\n    )\n}\n\n# The {{ }} syntax tells dplyr to use the column name passed in\nstarwars %&gt;%\n  group_by(species) %&gt;%\n  summarize_variable(height)\n\n\n\n5.3.2 In Python\nPython uses the def keyword. Type hints are a best practice:\nimport pandas as pd\n\ndef summarize_variable_py(dataset: pd.DataFrame, var_to_summarize: str) -&gt; pd.DataFrame:\n    \"\"\"Calculates summary statistics for a given column.\"\"\"\n    summary = dataset.groupby('species').agg(\n        n=(var_to_summarize, 'size'),\n        mean=(var_to_summarize, 'mean'),\n        sd=(var_to_summarize, 'std')\n    ).reset_index()\n    return summary",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#the-functional-toolkit-map-filter-and-reduce",
    "href": "05-fp.html#the-functional-toolkit-map-filter-and-reduce",
    "title": "5  Functional Programming",
    "section": "5.4 The Functional Toolkit: Map, Filter, and Reduce",
    "text": "5.4 The Functional Toolkit: Map, Filter, and Reduce\nMost for loops can be replaced by one of three core functional concepts: mapping, filtering, or reducing. These are “higher-order functions”—functions that take other functions as arguments.\n\n5.4.1 1. Mapping: Applying a Function to Each Element\nThe pattern: You have a list of things, and you want to perform the same action on each element, producing a new list of the same length.\n\n5.4.1.1 In R with purrr::map()\nThe {purrr} package is the gold standard for functional programming in R:\n\nmap(): Always returns a list\nmap_dbl(): Returns a vector of doubles (numeric)\nmap_chr(): Returns a vector of characters (strings)\nmap_lgl(): Returns a vector of logicals (booleans)\n\n\nlibrary(purrr)\n\n# The classic for-loop way (verbose)\nmeans_loop &lt;- vector(\"double\", ncol(mtcars))\nfor (i in seq_along(mtcars)) {\n  means_loop[[i]] &lt;- mean(mtcars[[i]], na.rm = TRUE)\n}\n\n# The functional way with map_dbl()\nmeans_functional &lt;- map_dbl(mtcars, mean, na.rm = TRUE)\n\nThe map() version is not just shorter; it’s safer. You can’t make an off-by-one error.\n\n\n5.4.1.2 In Python with List Comprehensions\nPython’s most idiomatic tool for mapping is the list comprehension:\nnumbers = [1, 2, 3, 4, 5]\nsquares = [n**2 for n in numbers]\n# &gt; [1, 4, 9, 16, 25]\nPython also has a built-in map() function:\ndef to_upper_case(s: str) -&gt; str:\n    return s.upper()\n\nwords = [\"hello\", \"world\"]\nupper_words = list(map(to_upper_case, words))\n# &gt; ['HELLO', 'WORLD']\n\n\n\n5.4.2 2. Filtering: Keeping Elements That Match a Condition\nThe pattern: You have a list of things, and you want to keep only the elements that satisfy a certain condition.\n\n5.4.2.1 In R with purrr::keep()\n\ndf1 &lt;- data.frame(x = 1:50)\ndf2 &lt;- data.frame(x = 1:200)\ndf3 &lt;- data.frame(x = 1:75)\nlist_of_dfs &lt;- list(a = df1, b = df2, c = df3)\n\n# Keep only data frames with more than 100 rows\nlarge_dfs &lt;- keep(list_of_dfs, ~ nrow(.x) &gt; 100)\n\n\n\n5.4.2.2 In Python with List Comprehensions\nList comprehensions have a built-in if clause:\nnumbers = [1, 10, 5, 20, 15, 30]\nlarge_numbers = [n for n in numbers if n &gt; 10]\n# &gt; [20, 15, 30]\n\n\n\n5.4.3 3. Reducing: Combining All Elements into a Single Value\nThe pattern: You have a list of things, and you want to iteratively combine them into a single summary value.\n\n5.4.3.1 In R with purrr::reduce()\n\n# Sum all elements\ntotal_sum &lt;- reduce(c(1, 2, 3, 4, 5), `+`)\n\n# Find common columns across multiple data frames\nlist_of_colnames &lt;- map(list_of_dfs, names)\ncommon_cols &lt;- reduce(list_of_colnames, intersect)\n\n\n\n5.4.3.2 In Python with functools.reduce\nfrom functools import reduce\nimport operator\n\nnumbers = [1, 2, 3, 4, 5]\ntotal_sum = reduce(operator.add, numbers)\n# &gt; 15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#the-power-of-composition",
    "href": "05-fp.html#the-power-of-composition",
    "title": "5  Functional Programming",
    "section": "5.5 The Power of Composition",
    "text": "5.5 The Power of Composition\nThe final, beautiful consequence of a functional style is composition. You can chain functions together to build complex workflows from simple, reusable parts.\nThis R code is a sequence of function compositions:\n\nstarwars %&gt;%\n  filter(!is.na(mass)) %&gt;%\n  select(species, sex, mass) %&gt;%\n  group_by(sex, species) %&gt;%\n  summarise(mean_mass = mean(mass), .groups = \"drop\")\n\nThe same idea in Python with pandas:\n(starwars_py\n .dropna(subset=['mass'])\n .filter(items=['species', 'sex', 'mass'])\n .groupby(['sex', 'species'])\n ['mass'].mean()\n .reset_index()\n)\nEach step is a function that takes a data frame and returns a new, transformed data frame. By combining map, filter, and reduce with this compositional style, you can express complex data manipulation pipelines without writing a single for loop.\n\n5.5.1 The Composition Challenge in Python\nMethod chaining in pandas is elegant but limited to the methods defined for DataFrame objects. R’s pipe operators (|&gt; and %&gt;%) are more flexible because functions are not strictly owned by objects—they can be more easily combined.\nThis reflects the languages’ different philosophies:\n\nR’s lineage traces back to Scheme (a Lisp dialect), making functional composition natural\nPython was designed around “one obvious way to do it,” favouring explicit R is fundamentally a functional language that acquired OOP features, while Python is an OOP language with powerful functional capabilities. Recognising this helps you leverage the native strengths of each.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#summary",
    "href": "05-fp.html#summary",
    "title": "5  Functional Programming",
    "section": "5.6 Summary",
    "text": "5.6 Summary\nThis chapter has laid the groundwork for writing reproducible code by embracing Functional Programming.\nKey takeaways:\n\nPure functions guarantee the same output for the same input—no hidden dependencies on global state\nMake impure operations (like randomness) explicit by controlling the seed\nReplace error-prone for loops with map, filter, and reduce\nUse composition to build complex pipelines from simple, reusable functions\nIn Python, treat stateful library objects as values passed between pure functions\n\nUnderstanding the distinction between R’s functional heritage and Python’s OOP nature is key to becoming an effective data scientist in either language. By mastering the functional paradigm, you’re building a foundation for code that is robust, easy to review, simple to debug, and truly reproducible.\nIn the next chapter, we’ll put these principles into practice with {rixpress}—a package that leverages functional composition and Nix to build fully reproducible, polyglot analytical pipelines.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html",
    "href": "06-rixpress.html",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "",
    "text": "6.1 Introduction: From Scripts and Notebooks to Pipelines\nSo far, we have learned about reproducible development environments with Nix and {rix}. We can now create project-specific environments with precise versions of R, Python, and all dependencies. But there’s one more piece to the puzzle: orchestration.\nHow do we take our collection of functions and data files and run them in the correct order to produce our final data product? This problem of managing computational workflows is not new, and a whole category of build automation tools has been created to solve it.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#introduction-from-scripts-and-notebooks-to-pipelines",
    "href": "06-rixpress.html#introduction-from-scripts-and-notebooks-to-pipelines",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "",
    "text": "6.1.1 The Evolution of Build Automation\nThe original solution, dating back to the 1970s, is make. Created by Stuart Feldman at Bell Labs in 1976, make reads a Makefile that describes the dependency graph of a project. If you change the code that generates plot.png, make is smart enough to only re-run the steps needed to rebuild the plot and the final report.\nThe strength of these tools is their language-agnosticism, but their weaknesses are twofold:\n\nFile-centric: You must manually handle all I/O—your first script saves data.csv, your second loads it. This adds boilerplate and surfaces for error.\nEnvironment-agnostic: They track files but know nothing about the software environment needed to create those files.\n\nThis is where R’s {targets} package shines. It tracks dependencies between R objects directly, automatically handling serialisation. But {targets} operates within a single R session—for polyglot pipelines, you must manually coordinate via {reticulate}.\n\n\n6.1.2 The Separation Problem\nAll these tools—from make to {targets} to Airflow—separate workflow management from environment management. You use one tool to run the pipeline and another (Docker, {renv}) to set up the software.\nThis separation creates friction. Running {targets} inside Docker ensures reproducibility, but forces the entire pipeline into one monolithic environment. What if your Python step requires TensorFlow 2.15 but your R step needs reticulate with Python 3.9? You’re stuck.\n\n\n6.1.3 The Imperative Approach: Make + Docker\nTo illustrate this, consider the traditional setup for a polyglot pipeline. You’d need:\n\nA Dockerfile to set up the environment\nA Makefile to orchestrate the workflow\nWrapper scripts for each step\n\nHere’s what a Makefile might look like:\n# Makefile for a Python → R pipeline\n\nDATA_DIR = data\nOUTPUT_DIR = output\n\n$(OUTPUT_DIR)/predictions.csv: $(DATA_DIR)/raw.csv scripts/train_model.py\n    python scripts/train_model.py $(DATA_DIR)/raw.csv $@\n\n$(OUTPUT_DIR)/plot.png: $(OUTPUT_DIR)/predictions.csv scripts/visualise.R\n    Rscript scripts/visualise.R $&lt; $@\n\n$(OUTPUT_DIR)/report.html: $(OUTPUT_DIR)/plot.png report.qmd\n    quarto render report.qmd -o $@\n\nall: $(OUTPUT_DIR)/report.html\n\nclean:\n    rm -rf $(OUTPUT_DIR)/*\nThis looks clean, but notice the procedural boilerplate required in each script. Your train_model.py must parse command-line arguments and handle file I/O:\n# scripts/train_model.py\nimport sys\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef main():\n    input_path = sys.argv[1]\n    output_path = sys.argv[2]\n    \n    # Load data\n    df = pd.read_csv(input_path)\n    \n    # ... actual ML logic ...\n    \n    # Save results\n    predictions.to_csv(output_path, index=False)\n\nif __name__ == \"__main__\":\n    main()\nAnd your visualise.R script needs the same boilerplate:\n# scripts/visualise.R\nargs &lt;- commandArgs(trailingOnly = TRUE)\ninput_path &lt;- args[1]\noutput_path &lt;- args[2]\n\n# Load data\npredictions &lt;- read.csv(input_path)\n\n# ... actual visualisation logic ...\n\n# Save plot\nggsave(output_path, plot)\nThe scientific logic is buried under file I/O scaffolding. And the environment? That’s a separate 100+ line Dockerfile you must maintain.\n\n\n6.1.4 rixpress: Unified Orchestration\n{rixpress} solves this by using Nix not just as a package manager, but as the build automation engine itself. Each pipeline step is a Nix derivation—a hermetically sealed build unit.\nCompare the same pipeline in {rixpress}:\n\nlibrary(rixpress)\n\nlist(\n  rxp_py_file(\n    name = raw_data,\n    path = \"data/raw.csv\",\n    read_function = \"lambda x: pandas.read_csv(x)\"\n  ),\n  \n  rxp_py(\n    name = predictions,\n    expr = \"train_model(raw_data)\",\n    user_functions = \"functions.py\"\n  ),\n  \n  rxp_py2r(\n    name = predictions_r,\n    expr = predictions\n  ),\n  \n  rxp_r(\n    name = plot,\n    expr = visualise(predictions_r),\n    user_functions = \"functions.R\"\n  ),\n  \n  rxp_qmd(\n    name = report,\n    qmd_file = \"report.qmd\"\n  )\n) |&gt;\n  rxp_populate()\n\nAnd your functions.py contains only the scientific logic:\n# functions.py\ndef train_model(df):\n    # ... pure ML logic, no file I/O ...\n    return predictions\nThe difference is stark:\n\n\n\n\n\n\n\n\nAspect\nMake + Docker\nrixpress\n\n\n\n\nFiles needed\nDockerfile, Makefile, wrapper scripts\ngen-env.R, gen-pipeline.R, function files\n\n\nI/O handling\nManual in every script\nAutomatic via encoders/decoders\n\n\nDependencies\nExplicit file rules\nInferred from object references\n\n\nEnvironment\nSeparate Docker setup\nUnified via {rix}\n\n\nExpertise needed\nLinux admin, Make syntax\nR programming\n\n\n\nThis provides two key benefits:\n\nTrue Polyglot Pipelines: Each step can have its own Nix environment. A Python step runs in a pure Python environment, an R step in an R environment, a Quarto step in yet another—all within the same pipeline.\nDeep Reproducibility: Each step is cached based on the cryptographic hash of all its inputs: the code, the data, and the environment. Any change in dependencies triggers a rebuild. This is reproducibility at the build level, not just the environment level.\n\nThe interface is heavily inspired by {targets}, so you get the ergonomic, object-passing feel you’re used to, combined with the bit-for-bit reproducibility of the Nix build system.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#what-is-rixpress",
    "href": "06-rixpress.html#what-is-rixpress",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.2 What is rixpress?",
    "text": "6.2 What is rixpress?\n{rixpress} streamlines creation of micropipelines—small-to-medium, single-machine analytic pipelines—by expressing a pipeline in idiomatic R while delegating build orchestration to the Nix build system.\nKey features:\n\nDefine pipeline derivations with concise rxp_*() helper functions\nSeamlessly mix R, Python, Julia, and Quarto steps\nReuse hermetic environments defined via {rix} and a default.nix\nVisualise and inspect the DAG; selectively read, load, or copy outputs\nAutomatic caching: only rebuild what changed\n\nHere is what a basic pipeline looks like:\n\nlibrary(rixpress)\n\nlist(\n  rxp_r_file(\n    mtcars,\n    'mtcars.csv',\n    \\(x) read.csv(file = x, sep = \"|\")\n  ),\n\n  rxp_r(\n    mtcars_am,\n    filter(mtcars, am == 1)\n  ),\n\n  rxp_r(\n    mtcars_head,\n    head(mtcars_am)\n  ),\n\n  rxp_qmd(\n    page,\n    \"page.qmd\"\n  )\n) |&gt;\n  rxp_populate()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#getting-started",
    "href": "06-rixpress.html#getting-started",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.3 Getting Started",
    "text": "6.3 Getting Started\n\n6.3.1 Initialising a project\nIf you’re starting fresh, you can bootstrap a project using a temporary shell:\nnix-shell --expr \"$(curl -sl https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix)\"\nOnce inside, start R and run:\n\nrixpress::rxp_init()\n\nThis creates two essential files:\n\ngen-env.R: Where you define your environment with {rix}\ngen-pipeline.R: Where you define your pipeline with {rixpress}\n\n\n\n6.3.2 Defining the environment\nOpen gen-env.R and define the tools your pipeline needs:\n\nlibrary(rix)\n\nrix(\n  date = \"2025-10-14\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\", \"quarto\", \"rixpress\"),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nRun this script to generate default.nix, then build and enter your environment:\nnix-build\nnix-shell\n\n\n6.3.3 Defining the pipeline\nOpen gen-pipeline.R and define your pipeline:\n\nlibrary(rixpress)\n\nlist(\n  rxp_r_file(\n    name = mtcars,\n    path = \"data/mtcars.csv\",\n    read_function = \\(x) read.csv(x, sep = \"|\")\n  ),\n\n  rxp_r(\n    name = mtcars_am,\n    expr = dplyr::filter(mtcars, am == 1)\n  ),\n\n  rxp_r(\n    name = mtcars_head,\n    expr = head(mtcars_am)\n  )\n) |&gt;\n  rxp_populate()\n\nRunning rxp_populate() generates a pipeline.nix file and builds the entire pipeline.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#core-functions",
    "href": "06-rixpress.html#core-functions",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.4 Core Functions",
    "text": "6.4 Core Functions\n\n6.4.1 Defining derivations\n{rixpress} provides several functions to define pipeline steps:\n\n\n\nFunction\nPurpose\n\n\n\n\nrxp_r()\nRun R code\n\n\nrxp_r_file()\nRead a file using R\n\n\nrxp_py()\nRun Python code\n\n\nrxp_py_file()\nRead a file using Python\n\n\nrxp_qmd()\nRender a Quarto document\n\n\nrxp_py2r()\nConvert Python object to R\n\n\nrxp_r2py()\nConvert R object to Python\n\n\n\n\n\n6.4.2 Building the pipeline\n\n# Generate pipeline.nix only (don't build)\nrxp_populate(build = FALSE)\n\n# Build the pipeline\nrxp_make()\n\n\n\n6.4.3 Inspecting outputs\nBecause outputs live in /nix/store/, {rixpress} provides helpers:\n\n# List all built artifacts\nrxp_inspect()\n\n# Read an artifact into R\nresult &lt;- rxp_read(\"mtcars_head\")\n\n# Load an artifact into the global environment\nrxp_load(\"mtcars_head\")\n\n# Copy an output file to current directory\nrxp_copy(\"page\")\n\n\n\n6.4.4 Visualizing the pipeline\n\n# Static DAG plot\nrxp_ggdag()\n\n# Interactive network\nrxp_visnetwork()\n\n# Text-based trace\nrxp_trace()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#polyglot-pipelines",
    "href": "06-rixpress.html#polyglot-pipelines",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.5 Polyglot Pipelines",
    "text": "6.5 Polyglot Pipelines\nOne of {rixpress}’s strengths is seamlessly mixing languages. Here’s a pipeline that reads data with Python’s polars, processes it with R’s dplyr, and renders a Quarto report:\n\nlibrary(rixpress)\n\nlist(\n  rxp_py_file(\n    name = mtcars_pl,\n    path = \"data/mtcars.csv\",\n    read_function = \"lambda x: polars.read_csv(x, separator='|')\"\n  ),\n\n  rxp_py(\n    name = mtcars_filtered,\n    expr = \"mtcars_pl.filter(polars.col('am') == 1).to_pandas()\"\n  ),\n\n  rxp_py2r(\n    name = mtcars_r,\n    expr = mtcars_filtered\n  ),\n\n  rxp_r(\n    name = mtcars_head,\n    expr = head(mtcars_r)\n  ),\n\n  rxp_qmd(\n    name = report,\n    qmd_file = \"report.qmd\"\n  )\n) |&gt;\n  rxp_populate()\n\n\n6.5.1 Method 1: Using language converters\nThe rxp_py2r() and rxp_r2py() functions use {reticulate} to convert objects between languages:\n\nrxp_py2r(\n  name = mtcars_r,\n  expr = mtcars_py\n)\n\n\n\n6.5.2 Method 2: Using universal data formats\nFor more control, use encoder and decoder arguments to serialize to formats like JSON:\n\n# Python step: serialize to JSON\nrxp_py(\n  name = mtcars_json,\n  expr = \"mtcars_pl.filter(polars.col('am') == 1)\",\n  user_functions = \"functions.py\",\n  encoder = \"serialize_to_json\"\n),\n\n# R step: deserialize from JSON\nrxp_r(\n  name = mtcars_head,\n  expr = my_head(mtcars_json),\n  user_functions = \"functions.R\",\n  decoder = \"jsonlite::fromJSON\"\n)\n\nThis approach makes your pipeline more modular—any language that can read JSON could be added in the future.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#caching-and-incremental-builds",
    "href": "06-rixpress.html#caching-and-incremental-builds",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.6 Caching and Incremental Builds",
    "text": "6.6 Caching and Incremental Builds\nOne of the most powerful features of using Nix for pipelines is automatic caching. Because Nix tracks all inputs to each derivation, it knows exactly what needs to be rebuilt when something changes.\nTry this:\n\nBuild your pipeline with rxp_make()\nChange one step in your pipeline\nRun rxp_make() again\n\nNix will detect that unchanged steps are already cached and instantly reuse them. It only rebuilds the steps affected by your change.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#build-logs-and-debugging",
    "href": "06-rixpress.html#build-logs-and-debugging",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.7 Build Logs and Debugging",
    "text": "6.7 Build Logs and Debugging\nEvery time you run rxp_populate(), a timestamped log is saved in the _rixpress/ directory. This is like having a Git history for your pipeline’s outputs.\n\n# List all past builds\nrxp_list_logs()\n\n# Load artifact from current build\nnew_result &lt;- rxp_read(\"mtcars_head\")\n\n# Load artifact from previous build\nold_result &lt;- rxp_read(\"mtcars_head\", which_log = \"z9y8x\")\n\n# Compare them\nidentical(new_result, old_result)\n\nThis is incredibly powerful for debugging and validation—you can go back in time to inspect any output from any previous pipeline run.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#ryxpress-the-python-interface",
    "href": "06-rixpress.html#ryxpress-the-python-interface",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.8 ryxpress: The Python Interface",
    "text": "6.8 ryxpress: The Python Interface\nIf you prefer working in Python, ryxpress provides the same functionality. You still define your pipeline in R (since that’s where {rixpress} runs), but you can build and inspect artifacts from Python.\nTo set up an environment with ryxpress:\n\nrix(\n  date = \"2025-10-14\",\n  r_pkgs = c(\"rixpress\"),\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"ryxpress\", \"rds2py\", \"biocframe\", \"pandas\")\n  ),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nThen from Python:\nfrom ryxpress import rxp_make, rxp_inspect, rxp_load\n\n# Build the pipeline\nrxp_make()\n\n# Inspect artifacts\nrxp_inspect()\n\n# Load an artifact\nrxp_load(\"mtcars_head\")\nryxpress handles the conversion automatically:\n\nTries pickle.load first\nFalls back to rds2py for R objects\nReturns file paths for complex outputs",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#running-someone-elses-pipeline",
    "href": "06-rixpress.html#running-someone-elses-pipeline",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.9 Running Someone Else’s Pipeline",
    "text": "6.9 Running Someone Else’s Pipeline\nThe ultimate test of reproducibility: can someone else run your pipeline?\nWith a Nix-based workflow, they need only:\n\ngit clone your repository\nRun nix-build && nix-shell\nRun source(\"gen-pipeline.R\") or rxp_make()\n\nThat’s it. Nix reads your default.nix and pipeline.nix files and builds the exact same environment and data product, bit-for-bit.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#exporting-and-importing-artifacts",
    "href": "06-rixpress.html#exporting-and-importing-artifacts",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.10 Exporting and Importing Artifacts",
    "text": "6.10 Exporting and Importing Artifacts\nFor CI/CD or sharing between machines:\n\n# Export build products to a tarball\nrxp_export_artifacts()\n\n# Import on another machine before building\nrxp_import_artifacts()\n\nThis speeds up continuous integration by avoiding unnecessary rebuilds.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#real-world-examples",
    "href": "06-rixpress.html#real-world-examples",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.11 Real-World Examples",
    "text": "6.11 Real-World Examples\nThe rixpress_demos repository contains many complete examples. Here are a few patterns worth studying.\n\n6.11.1 Example 1: Machine Learning with XGBoost\nThis pipeline trains an XGBoost classifier in Python, then passes predictions to R for evaluation with {yardstick}:\n\nlibrary(rixpress)\n\nlist(\n  # Load data as NumPy array\n  rxp_py_file(\n    name = dataset_np,\n    path = \"data/pima-indians-diabetes.csv\",\n    read_function = \"lambda x: loadtxt(x, delimiter=',')\"\n  ),\n\n  # Split features and target\n  rxp_py(name = X, expr = \"dataset_np[:,0:8]\"),\n  rxp_py(name = Y, expr = \"dataset_np[:,8]\"),\n\n  # Train/test split\n  rxp_py(\n    name = splits,\n    expr = \"train_test_split(X, Y, test_size=0.33, random_state=7)\"\n  ),\n\n  # Extract splits\n\n  rxp_py(name = X_train, expr = \"splits[0]\"),\n  rxp_py(name = X_test, expr = \"splits[1]\"),\n  rxp_py(name = y_train, expr = \"splits[2]\"),\n  rxp_py(name = y_test, expr = \"splits[3]\"),\n\n  # Train XGBoost model\n  rxp_py(\n    name = model,\n    expr = \"XGBClassifier(use_label_encoder=False, eval_metric='logloss').fit(X_train, y_train)\"\n  ),\n\n  # Make predictions\n  rxp_py(name = y_pred, expr = \"model.predict(X_test)\"),\n\n  # Export predictions to CSV for R\n  rxp_py(\n    name = combined_df,\n    expr = \"DataFrame({'truth': y_test, 'estimate': y_pred})\"\n  ),\n\n  rxp_py(\n    name = combined_csv,\n    expr = \"combined_df\",\n    user_functions = \"functions.py\",\n    encoder = \"write_to_csv\"\n  ),\n\n  # Compute confusion matrix in R\n  rxp_r(\n    combined_factor,\n    expr = mutate(combined_csv, across(everything(), factor)),\n    decoder = \"read.csv\"\n  ),\n\n  rxp_r(\n    name = confusion_matrix,\n    expr = yardstick::conf_mat(combined_factor, truth, estimate)\n  )\n) |&gt;\n  rxp_populate(build = FALSE)\n\n# Adjust Python imports\nadjust_import(\"import numpy\", \"from numpy import array, loadtxt\")\nadjust_import(\"import xgboost\", \"from xgboost import XGBClassifier\")\nadjust_import(\"import sklearn\", \"from sklearn.model_selection import train_test_split\")\nadd_import(\"from pandas import DataFrame\", \"default.nix\")\n\nrxp_make()\n\nThis demonstrates:\n\nPython-heavy computation with XGBoost\nCustom serialization via encoder/decoder\nAdjusting Python imports with adjust_import() and add_import()\nPassing results to R for evaluation\n\n\n\n6.11.2 Example 2: Reading Many Input Files\nWhen you have multiple CSV files in a directory:\n\nlibrary(rixpress)\n\nlist(\n  # R approach: read all files at once\n  rxp_r_file(\n    name = mtcars_r,\n    path = \"data\",\n    read_function = \\(x) {\n      readr::read_delim(list.files(x, full.names = TRUE), delim = \"|\")\n    }\n  ),\n\n  # Python approach: custom function\n  rxp_py_file(\n    name = mtcars_py,\n    path = \"data\",\n    read_function = \"read_many_csvs\",\n    user_functions = \"functions.py\"\n  ),\n\n  rxp_py(\n    name = head_mtcars,\n    expr = \"mtcars_py.head()\"\n  )\n) |&gt;\n  rxp_populate()\n\nThe key insight: rxp_r_file() and rxp_py_file() can point to a directory, and your read_function handles the logic.\n\n\n6.11.3 Example 3: Full Python→R→Quarto Workflow\nA complete pipeline that bounces data between languages and renders a report:\n\nlibrary(rixpress)\n\nlist(\n  # Read with Python polars\n  rxp_py_file(\n    name = mtcars_pl,\n    path = \"data/mtcars.csv\",\n    read_function = \"lambda x: polars.read_csv(x, separator='|')\"\n  ),\n\n  # Filter in Python, convert to pandas for reticulate\n  rxp_py(\n    name = mtcars_pl_am,\n    expr = \"mtcars_pl.filter(polars.col('am') == 1).to_pandas()\"\n  ),\n\n  # Convert to R\n  rxp_py2r(name = mtcars_am, expr = mtcars_pl_am),\n\n  # Process in R\n  rxp_r(\n    name = mtcars_head,\n    expr = my_head(mtcars_am),\n    user_functions = \"functions.R\"\n  ),\n\n  # Back to Python\n  rxp_r2py(name = mtcars_head_py, expr = mtcars_head),\n\n  # More Python processing\n  rxp_py(name = mtcars_tail_py, expr = \"mtcars_head_py.tail()\"),\n\n  # Back to R\n  rxp_py2r(name = mtcars_tail, expr = mtcars_tail_py),\n\n  # Final R step\n  rxp_r(name = mtcars_mpg, expr = dplyr::select(mtcars_tail, mpg)),\n\n  # Render Quarto document\n  rxp_qmd(\n    name = page,\n    qmd_file = \"my_doc/page.qmd\",\n    additional_files = c(\"my_doc/content.qmd\", \"my_doc/images\")\n  )\n) |&gt;\n  rxp_populate()\n\nNote the additional_files argument for rxp_qmd(): this includes child documents and images that the main Quarto file needs.\n\n\n6.11.4 More Examples\nThe rixpress_demos repository includes:\n\njl_example: Using Julia in pipelines\nr_qs: Using {qs} for faster serialization\npython_r_typst: Compiling to Typst documents\nr_multi_envs: Different Nix environments for different derivations\nyanai_lercher_2020: Reproducing a published paper’s analysis",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#summary",
    "href": "06-rixpress.html#summary",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.12 Summary",
    "text": "6.12 Summary\n{rixpress} unifies environment management and workflow orchestration:\n\nDefine pipelines with rxp_*() functions in familiar R syntax\nMix languages freely—R, Python, Julia, Quarto\nBuild with Nix for deterministic, cached execution\nInspect outputs with rxp_read(), rxp_load(), rxp_copy()\nDebug with timestamped build logs\nShare reproducible pipelines via Git\n\nThe Python port ryxpress provides the same experience for Python-first workflows.\nBy embracing structured, plain-text pipelines over notebooks for production work, your analysis becomes more reliable, more scalable, and fundamentally more reproducible.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "07-docker.html",
    "href": "07-docker.html",
    "title": "7  Containerisation With Docker",
    "section": "",
    "text": "7.1 Introduction\nUp until now, we’ve been using Nix as a powerful tool for creating reproducible development environments directly on our machines. Nix gives us fine-grained control over every package and dependency in our project, ensuring bit-for-bit reproducibility. However, when it comes to distributing a data product, another technology, Docker, is incredibly popular.\nWhile Nix manages dependencies for an application that runs on a host operating system, Docker takes a different approach: it packages an application along with a lightweight operating system and all its dependencies into a single, portable unit called a container. This container can then run on any machine that has Docker installed, regardless of its underlying OS.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#introduction",
    "href": "07-docker.html#introduction",
    "title": "7  Containerisation With Docker",
    "section": "",
    "text": "7.1.1 Spatial vs Temporal Reproducibility\nTo understand when and why to use Docker, it helps to distinguish between two types of reproducibility:\n\nSpatial reproducibility: The ability to execute an analysis identically across different machines right now. Docker excels here—a container runs the same way on your laptop, a colleague’s workstation, and a cloud server.\nTemporal reproducibility: The ability to execute an analysis identically over time. Docker is weaker here. The imperative commands in a Dockerfile (like apt-get update) are non-deterministic—rebuilding the same file at different times can yield different images.\n\nThis distinction is crucial. Docker’s true reproducibility promise is that a specific, pre-built image will always launch an identical container. It does not promise that building the same Dockerfile twice will yield an identical image.\nAs empirical research has shown, achieving deterministic builds requires systems designed for this purpose—like Nix’s functional model, where identical inputs always produce identical outputs. Studies rebuilding historical Nix packages found bit-for-bit reproducibility rates exceeding 90%.\n\n\n7.1.2 The Best of Both Worlds\nThis is why we combine Nix and Docker rather than choosing one or the other:\n\nNix provides strong temporal reproducibility—the guarantee that an environment built today will be identical when rebuilt years later.\nDocker provides strong spatial reproducibility and universal distribution—the guarantee that a container runs the same everywhere Docker runs.\n\nThe pattern we’ll use is simple: use Nix inside a Docker container. Start with a minimal base image that has Nix installed. Then, use Nix to declaratively build the precise environment within the image. Docker becomes a portable runtime for this Nix-managed environment, excellent for deployment to systems where Nix isn’t installed.\n\n\n7.1.3 Interactive Development vs Distribution\nThis approach also clarifies when to use each tool:\n\nUse Nix directly for interactive development. It integrates seamlessly with your IDE and filesystem. No volume mounts, no port forwarding, no graphical application headaches.\nUse Docker for distributing finished data products. Package your {rixpress} pipeline into an image that anyone can run with a single command.\n\nIf you’ve never heard of Docker before, this chapter will provide the basic knowledge required to get started. Let’s start by watching this very short video that introduces the core concepts.\nThe Rocker Project provides a large collection of ready-to-use Docker images for R users.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#docker-essentials",
    "href": "07-docker.html#docker-essentials",
    "title": "7  Containerisation With Docker",
    "section": "7.2 Docker Essentials",
    "text": "7.2 Docker Essentials\n\n7.2.1 Installing Docker\nThe first step is to install Docker. You’ll find the instructions for Ubuntu here, for Windows here (read the system requirements section as well!) and for macOS here (make sure to choose the right version for the architecture of your Mac, if you have an M1 Mac use Mac with Apple silicon).\nAfter installation, it might be a good idea to restart your computer, if the installation wizard does not invite you to do so. To check whether Docker was installed successfully, run the following command in a terminal:\ndocker run --rm hello-world\nThis should print the following message:\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\nIf you see this message, congratulations, you are ready to run Docker. If you see an error message about permissions, this means that something went wrong. If you’re running Linux, make sure that your user is in the Docker group by running:\ngroups $USER\nYou should see your username and a list of groups that your user belongs to. If a group called docker is not listed, then you should add yourself to the group by following these steps.\n\n\n7.2.2 The Rocker Project and Image Registries\nWhen running a command like:\ndocker run --rm hello-world\nwhat happens is that an image, in this case hello-world, gets pulled from a so-called registry. A registry is a storage and distribution system for Docker images. Think of it as a GitHub for Docker images, where you can push and pull images, much like you would with code repositories. The default public registry that Docker uses is called Docker Hub, but companies can also host their own private registries to store proprietary images.\nMany open source projects build and distribute Docker images through Docker Hub, for example the Rocker Project.\nThe Rocker Project is instrumental for R users that want to use Docker. The project provides a large list of images that are ready to run with a single command. As an illustration, open a terminal and paste the following line:\ndocker run --rm -e PASSWORD=yourpassword -p 8787:8787 rocker/rstudio\nOnce this stops running, go to http://localhost:8787/ and enter rstudio as the username and yourpassword as the password. You should login to a RStudio instance: this is the web interface of RStudio that allows you to work with R from a server. In this case, the server is the Docker container running the image. Yes, you’ve just pulled a Docker image containing Ubuntu with a fully working installation of RStudio web!\nLet’s open a new script and run the following lines:\n\ndata(mtcars)\nsummary(mtcars)\n\nYou can now stop the container (by pressing CTRL-C in the terminal). Let’s now rerun the container… you should realise that your script is gone! This is the first lesson: whatever you do inside a container will disappear once the container is stopped. This also means that if you install the R packages that you need while the container is running, you will need to reinstall them every time.\nThankfully, the Rocker Project provides a list of images with many packages already available. For example to run R with the {tidyverse} collection of packages already pre-installed, run:\ndocker run --rm -e PASSWORD=yourpassword -p 8787:8787 rocker/tidyverse\n\n\n7.2.3 Basic Docker Workflow\nYou already know about running containers using docker run. With the commands we ran before, your terminal will need to stay open, or else, the container will stop. Starting now, we will run Docker commands in the background using the -d flag (d as in detach):\ndocker run --rm -d -e PASSWORD=yourpassword -p 8787:8787 rocker/tidyverse\nYou can run several containers in the background simultaneously. List running containers with docker ps:\ndocker ps\nCONTAINER ID   IMAGE              COMMAND   CREATED         STATUS         PORTS                    NAMES\nc956fbeeebcb   rocker/tidyverse   \"/init\"   3 minutes ago   Up 3 minutes   0.0.0.0:8787-&gt;8787/tcp   elastic_morse\nStop the container using its ID:\ndocker stop c956fbeeebcb\nLet’s discuss the other flags:\n\n--rm: Removes the container once it’s stopped\n-e: Provides environment variables to the container (e.g., PASSWORD)\n-p: Sets the port mapping (host:container)\n--name: Gives the container a custom name\n\nRun a container with a name:\ndocker run -d --name my_r --rm -e PASSWORD=yourpassword -p 8787:8787 rocker/tidyverse\nYou can now interact with this container using its name:\ndocker exec -ti my_r bash\nYou are now inside a terminal session, inside the running container! This can be useful for debugging purposes.\nFinally, let’s solve the issue of our scripts disappearing. Create a folder somewhere on your computer, then run:\ndocker run -d --name my_r --rm -e PASSWORD=yourpassword -p 8787:8787 \\\n  -v /path/to/your/local/folder:/home/rstudio/scripts:rw rocker/tidyverse\nYou should now be able to save scripts inside the scripts/ folder from RStudio and they will appear in the folder you created on your host machine.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#making-our-own-images",
    "href": "07-docker.html#making-our-own-images",
    "title": "7  Containerisation With Docker",
    "section": "7.3 Making Our Own Images",
    "text": "7.3 Making Our Own Images\nTo create our own images, you can start from an image provided by an open source project like Rocker, or you can start from the base Ubuntu image. Since we are using Nix to set up the reproducible development environment, we can use ubuntu:latest—our development environment will always be exactly the same, thanks to Nix.\n\n7.3.1 A Minimal Dockerfile with Nix\nFROM ubuntu:latest\n\nRUN apt update -y\nRUN apt install curl -y\n\n# Download the default.nix that comes with {rix}\nRUN curl -O https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix\n\n# Install Nix via Determinate Systems installer\nRUN curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install linux \\\n  --extra-conf \"sandbox = false\" \\\n  --init none \\\n  --no-confirm\n\n# Add Nix to the path\nENV PATH=\"${PATH}:/nix/var/nix/profiles/default/bin\"\nENV user=root\n\n# Configure rstats-on-nix cache\nRUN mkdir -p /root/.config/nix && \\\n    echo \"substituters = https://cache.nixos.org https://rstats-on-nix.cachix.org\" &gt; /root/.config/nix/nix.conf && \\\n    echo \"trusted-public-keys = cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY= rstats-on-nix.cachix.org-1:vdiiVgocg6WeJrODIqdprZRUrhi1JzhBnXv7aWI6+F0=\" &gt;&gt; /root/.config/nix/nix.conf\n\n# Copy script to generate environment\nCOPY gen-env.R .\n\n# Generate default.nix from gen-env.R\nRUN nix-shell --run \"Rscript gen-env.R\"\n\n# Build the environment\nRUN nix-build\n\n# Run nix-shell when container starts\nCMD nix-shell\nEvery Dockerfile starts with a FROM statement specifying the base image. Then, every command starts with RUN. We install and configure Nix, copy an R script to generate the environment, and then build it. Finally, we run nix-shell when executing a container (the CMD statement).\n\n\n7.3.2 Splitting Into a Reusable Base Image\nBecause the Nix installation step is generic, we can split this into two stages. First, create a base image with just Nix installed:\n# nix-base/Dockerfile\nFROM ubuntu:latest AS nix-base\n\nRUN apt update -y && apt install -y curl\n\n# Install Nix via Determinate Systems installer\nRUN curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install linux \\\n  --extra-conf \"sandbox = false\" \\\n  --init none \\\n  --no-confirm\n\nENV PATH=\"/nix/var/nix/profiles/default/bin:${PATH}\"\nENV user=root\n\n# Configure Nix binary cache\nRUN mkdir -p /root/.config/nix && \\\n    echo \"substituters = https://cache.nixos.org https://rstats-on-nix.cachix.org\" &gt; /root/.config/nix/nix.conf && \\\n    echo \"trusted-public-keys = cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY= rstats-on-nix.cachix.org-1:vdiiVgocg6WeJrODIqdprZRUrhi1JzhBnXv7aWI6+F0=\" &gt;&gt; /root/.config/nix/nix.conf\nBuild and tag this image:\ndocker build -t nix-base:latest .\nNow, for any project, simply reuse it:\nFROM nix-base:latest\n\nCOPY gen-env.R .\n\nRUN curl -O https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix\nRUN nix-shell --run \"Rscript gen-env.R\"\nRUN nix-build\n\nCMD [\"nix-shell\"]\nWith gen-env.R:\n\nlibrary(rix)\n\nrix(\n  date = \"2025-08-04\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"polars\", \"great-tables\")\n  ),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nBuild and run:\ndocker build -t my-project .\ndocker run -it --rm --name my-project-container my-project\nThis drops you in an interactive Nix shell running inside Docker!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#publishing-images-on-docker-hub",
    "href": "07-docker.html#publishing-images-on-docker-hub",
    "title": "7  Containerisation With Docker",
    "section": "7.4 Publishing Images on Docker Hub",
    "text": "7.4 Publishing Images on Docker Hub\nIf you want to share Docker images through Docker Hub, you first need to create a free account. A free account gives you unlimited public repositories.\nList all images on your computer:\ndocker images\nLog in to Docker Hub:\ndocker login\nTag the image with your username:\ndocker tag IMAGE_ID your_username/nix-base:latest\nPush the image:\ndocker push your_username/nix-base:latest\nThis image can now be used as a stable base for other projects:\nFROM your_username/nix-base:latest\n\nRUN mkdir ...\n\n7.4.1 Sharing Without Docker Hub\nIf you can’t upload to Docker Hub, you can save the image to a file:\ndocker save nix-base | gzip &gt; nix-base.tgz\nLoad it on another machine:\ngzip -d nix-base.tgz\ndocker load &lt; nix-base.tar",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#what-if-you-dont-use-nix",
    "href": "07-docker.html#what-if-you-dont-use-nix",
    "title": "7  Containerisation With Docker",
    "section": "7.5 What If You Don’t Use Nix?",
    "text": "7.5 What If You Don’t Use Nix?\nUsing Nix inside of Docker makes it very easy to set up an environment, but what if you can’t use Nix for some reason? In this case, you would need to use other tools to install the right R or Python packages and it is likely going to be more difficult. The main issue you will face is missing development libraries.\nFor example, to install and use the R {stringr} package, you will need to first install libicu-dev:\nFROM rocker/r-ver:4.5.1\n\nRUN apt-get update && apt-get install -y \\\n    libglpk-dev \\\n    libxml2-dev \\\n    libcairo2-dev \\\n    libgit2-dev \\\n    libcurl4-openssl-dev \\\n    # ... many more\nAnother issue is that building the image is not a reproducible process, only running containers is. To mitigate this, use tagged images or better yet, a digest:\nFROM rocker/r-ver@sha256:1dbe7a6718b7bd8630addc45a32731624fb7b7ffa08c0b5b91959b0dbf7ba88e\nThis will always pull exactly the same layers. However, at some point, that version of Ubuntu will be outdated. Using Nix, you can stay on ubuntu:latest.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#building-docker-images-directly-with-nix",
    "href": "07-docker.html#building-docker-images-directly-with-nix",
    "title": "7  Containerisation With Docker",
    "section": "7.6 Building Docker Images Directly with Nix",
    "text": "7.6 Building Docker Images Directly with Nix\nSo far, we’ve used Nix inside Docker containers. But there’s an even more powerful approach: using Nix to build Docker images directly, bypassing Dockerfiles entirely.\nNix provides dockerTools, a set of functions that can create OCI-compliant container images. Because these images are built through Nix’s deterministic build system, they have stronger reproducibility guarantees than images built with docker build.\n\n7.6.1 Why Skip the Dockerfile?\nA Dockerfile is fundamentally imperative—it’s a script of commands executed in order. Commands like apt-get update are non-deterministic by nature. Even with careful pinning, you’re fighting the tool’s design.\nNix’s dockerTools.buildImage is declarative. You describe what should be in the image, and Nix figures out how to build it reproducibly. The resulting image is a pure function of its inputs.\n\n\n7.6.2 A Basic Example\nHere’s a Nix expression that builds a Docker image containing R and some packages:\n# docker-image.nix\nlet\n  pkgs = import (fetchTarball\n    \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-10-14.tar.gz\"\n  ) {};\n  \n  # Define the R environment\n  myR = pkgs.rWrapper.override {\n    packages = with pkgs.rPackages; [\n      dplyr\n      ggplot2\n      quarto\n    ];\n  };\n  \nin pkgs.dockerTools.buildImage {\n  name = \"my-r-env\";\n  tag = \"latest\";\n  \n  copyToRoot = pkgs.buildEnv {\n    name = \"image-root\";\n    paths = [ myR pkgs.quarto pkgs.coreutils pkgs.bash ];\n    pathsToLink = [ \"/bin\" ];\n  };\n  \n  config = {\n    Cmd = [ \"${pkgs.bash}/bin/bash\" ];\n    WorkingDir = \"/work\";\n  };\n}\nBuild and load it:\n# Build the image (outputs a .tar.gz file)\nnix-build docker-image.nix\n\n# Load into Docker\ndocker load &lt; result\n\n# Run it\ndocker run -it --rm my-r-env:latest\n\n\n7.6.3 Benefits Over Dockerfiles\n\n\n\n\n\n\n\n\nAspect\nDockerfile\nNix dockerTools\n\n\n\n\nReproducibility\nImperative, non-deterministic\nDeclarative, deterministic\n\n\nLayer caching\nBased on command order\nBased on content hashes\n\n\nImage size\nOften includes unnecessary packages\nMinimal—only what you specify\n\n\nComposition\nCopy-paste between files\nReuse Nix expressions\n\n\n\n\n\n7.6.4 Layered Images for Efficiency\nFor faster CI builds, you can create layered images where the base environment is cached:\npkgs.dockerTools.buildLayeredImage {\n  name = \"my-analysis\";\n  tag = \"latest\";\n  \n  contents = [ myR pkgs.quarto ];\n  \n  config = {\n    Cmd = [ \"${myR}/bin/R\" \"--vanilla\" ];\n  };\n}\nbuildLayeredImage creates separate layers for each package, so unchanged dependencies are cached between builds.\n\n\n7.6.5 When to Use This Approach\n\nCI/CD pipelines: When you need reproducible image builds as part of automated workflows\nMinimal images: When image size matters and you want only what you need\nComplex environments: When the environment is already defined in Nix and you want to deploy it as a container\n\nFor simpler use cases, the “Nix inside Docker” approach from earlier sections may be more accessible.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#dockerizing-a-rixpress-pipeline",
    "href": "07-docker.html#dockerizing-a-rixpress-pipeline",
    "title": "7  Containerisation With Docker",
    "section": "7.7 Dockerizing a rixpress Pipeline",
    "text": "7.7 Dockerizing a rixpress Pipeline\nWe can package our entire {rixpress} project into a single Docker image. This image can then be run by anyone with Docker installed, regardless of their host operating system or whether they have Nix.\nAssume your project directory has:\n.\n├── data/\n│   └── mtcars.csv\n├── gen-env.R\n├── gen-pipeline.R\n├── functions.R\n├── functions.py\n├── default.nix\n└── pipeline.nix\n\n7.7.1 Step 1: The Dockerfile\nFROM nix-base:latest\n# Or: FROM your-username/nix-base:latest\n\nWORKDIR /app\n\n# Copy all project files\nCOPY . .\n\n# Build the pipeline during image build\nRUN nix-build pipeline.nix\n\n# Export results when container runs\nCOPY export-results.R .\nCMD [\"nix-shell\", \"--run\", \"Rscript export-results.R\"]\n\n\n7.7.2 Step 2: The Export Script\n\n# export-results.R\nlibrary(rixpress)\nlibrary(jsonlite)\n\noutput_dir &lt;- \"/output\"\ndir.create(output_dir, showWarnings = FALSE)\n\nmessage(\"Reading target 'mtcars_head'...\")\nfinal_data &lt;- rxp_read(\"mtcars_head\")\n\noutput_path &lt;- file.path(output_dir, \"mtcars_analysis_result.json\")\nwrite_json(final_data, output_path, pretty = TRUE)\n\nmessage(paste(\"Successfully exported result to\", output_path))\n\n\n\n7.7.3 Step 3: Build and Run\nBuild:\ndocker build -t my-reproducible-pipeline .\nRun to get results:\nmkdir -p ./output\n\ndocker run --rm --name my_pipeline_run \\\n  -v \"$(pwd)/output\":/output \\\n  my-reproducible-pipeline\nCheck your local output directory—you’ll find the mtcars_analysis_result.json file containing the exact, reproducible result of your pipeline.\nYou have successfully packaged a complex, polyglot pipeline into a simple, portable Docker image. This workflow combines the best of both worlds: Nix’s power for creating reproducible builds and Docker’s universal standard for distributing and running applications.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#summary",
    "href": "07-docker.html#summary",
    "title": "7  Containerisation With Docker",
    "section": "7.8 Summary",
    "text": "7.8 Summary\nDocker and Nix complement each other:\n\nDocker provides a universal runtime and distribution mechanism\nNix provides bit-for-bit reproducible environment management\nTogether they enable truly reproducible, portable data products\n\nKey Docker concepts:\n\nImages are templates; containers are running instances\nContainers are ephemeral—use volumes for persistence\nUse registries (Docker Hub) to share images\nThe Rocker Project provides ready-made R images\n\nFor reproducibility:\n\nUse ubuntu:latest + Nix rather than pinning specific OS versions\nBuild a reusable nix-base image to share across projects\nPackage {rixpress} pipelines for distribution",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#further-reading",
    "href": "07-docker.html#further-reading",
    "title": "7  Containerisation With Docker",
    "section": "7.9 Further Reading",
    "text": "7.9 Further Reading\n\nRunning Your R Script in Docker\nDocker & R Reproducibility\nR Docker Tutorial",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "08-monads.html",
    "href": "08-monads.html",
    "title": "8  Robust Pipelines with Monads",
    "section": "",
    "text": "8.1 The Problem: Decorated Functions Don’t Compose\nIn the previous chapters, you learned functional programming fundamentals and how to build reproducible pipelines with {rixpress}. Now we’ll add another layer of robustness: monads.\nMonads might sound abstract, but they solve concrete problems involving Logging (tracing what each step does without cluttering your functions), Error handling (letting failures propagate gracefully instead of crashing), and Composition (keeping functions composable even when they need to do “extra” work).\nBy the end of this chapter, you’ll know how to integrate {chronicler} and talvez into your pipelines for more robust, observable data workflows.\nSuppose you want your functions to provide logs. You might rewrite sqrt() like this:\nmy_sqrt &lt;- function(x, log = \"\") {\n  list(\n    result = sqrt(x),\n    log = c(log, paste0(\"Running sqrt with input \", x))\n  )\n}\n\nmy_log &lt;- function(x, log = \"\") {\n  list(\n    result = log(x),\n    log = c(log, paste0(\"Running log with input \", x))\n  )\n}\nThese functions now return lists with both the result and a log. But there’s a problem—they don’t compose:\n# This works:\n10 |&gt; sqrt() |&gt; log()\n\n# This fails:\n10 |&gt; my_sqrt() |&gt; my_log()\n# Error: non-numeric argument to mathematical function\nmy_log() expects a number, but my_sqrt() returns a list. We’ve broken composition.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#the-solution-function-factories-and-bind",
    "href": "08-monads.html#the-solution-function-factories-and-bind",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.2 The Solution: Function Factories and Bind",
    "text": "8.2 The Solution: Function Factories and Bind\nA monad provides two things:\n\nA function factory that decorates functions so they can provide additional output without rewriting their core logic\nA bind() function that makes these decorated functions compose\n\nHere’s a simple function factory for logging:\n\nlog_it &lt;- function(.f) {\n  fstring &lt;- deparse(substitute(.f))\n  \n  function(..., .log = NULL) {\n    list(\n      result = .f(...),\n      log = c(.log, paste0(\"Running \", fstring, \" with argument \", ...))\n    )\n  }\n}\n\n# Create decorated functions\nl_sqrt &lt;- log_it(sqrt)\nl_log &lt;- log_it(log)\n\nl_sqrt(10)\n#&gt; $result\n#&gt; [1] 3.162278\n#&gt; $log\n#&gt; [1] \"Running sqrt with argument 10\"\n\nNow we need bind() to make them compose:\n\nbind &lt;- function(.l, .f) {\n  .f(.l$result, .log = .l$log)\n}\n\n# Now they compose!\n10 |&gt;\n  l_sqrt() |&gt;\n  bind(l_log)\n\n#&gt; $result\n#&gt; [1] 1.151293\n#&gt; $log\n#&gt; [1] \"Running sqrt with argument 10\"\n#&gt; [2] \"Running log with argument 3.16227766016838\"\n\nThis pattern of a function factory plus bind() is the essence of a monad.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#the-chronicler-package",
    "href": "08-monads.html#the-chronicler-package",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.3 The chronicler Package",
    "text": "8.3 The chronicler Package\nThe {chronicler} package implements this pattern properly for R. It provides:\n\nrecord(): A function factory that decorates functions\nbind_record(): The bind operation\nAutomatic logging of all operations\n\n\nlibrary(chronicler)\n\n# Decorate functions\nr_sqrt &lt;- record(sqrt)\nr_exp &lt;- record(exp)\nr_mean &lt;- record(mean)\n\n# Compose them\nresult &lt;- 1:10 |&gt;\n  r_sqrt() |&gt;\n  bind_record(r_exp) |&gt;\n  bind_record(r_mean)\n\n# View the result\nresult$value\n#&gt; [1] 5.187899\n\n# View the log\nread_log(result)\n#&gt; [1] \"Complete log:\"\n#&gt; [2] \"✔ sqrt ran successfully\"\n#&gt; [3] \"✔ exp ran successfully\"\n#&gt; [4] \"✔ mean ran successfully\"",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#the-maybe-monad-handling-errors",
    "href": "08-monads.html#the-maybe-monad-handling-errors",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.4 The Maybe Monad: Handling Errors",
    "text": "8.4 The Maybe Monad: Handling Errors\nAnother common monad is Maybe, which handles computations that might fail. Instead of crashing, functions return either:\n\nJust(value) if successful\nNothing if something went wrong\n\nThe {chronicler} package uses this under the hood:\n\nr_sqrt &lt;- record(sqrt)\n\n# This works\nr_sqrt(16)\n#&gt; ✔ Value: Just\n#&gt; [1] 4\n\n# This fails gracefully\nr_sqrt(\"not a number\")\n#&gt; ✖ Value: Nothing\n\nWhen Nothing is passed to a decorated function, it immediately returns Nothing—the error propagates through the pipeline without crashing.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#monads-in-python-talvez",
    "href": "08-monads.html#monads-in-python-talvez",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.5 Monads in Python: talvez",
    "text": "8.5 Monads in Python: talvez\nThe same concepts exist in Python. The talvez package provides a Maybe monad:\nfrom talvez import maybe, just\n\n@maybe()\ndef parse_int(s: str) -&gt; int:\n    return int(s)\n\n@maybe(ensure=lambda x: x != 0)\ndef reciprocal(n: int) -&gt; float:\n    return 1 / n\n\n# Successful computation\nresult = (\n    parse_int(\"25\")\n      .bind(reciprocal)\n      .fmap(lambda x: x * 100)\n)\nprint(result)            # Just(4.0)\nprint(result.get_or(-1)) # 4.0\n\n# Failed computation\nbad = (\n    parse_int(\"not a number\")\n      .bind(reciprocal)\n      .fmap(lambda x: x * 100)\n)\nprint(bad)  # Nothing\nThe key operations are:\n\nfmap(fn): Apply a pure function to the value inside the monad\nbind(fn): Apply a function that itself returns a monad",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#building-robust-pipelines",
    "href": "08-monads.html#building-robust-pipelines",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.6 Building Robust Pipelines",
    "text": "8.6 Building Robust Pipelines\nThe real power of monads becomes apparent when you combine them with pipeline orchestration. Consider a typical data pipeline:\n\n# Standard pipeline - fragile\nraw_data |&gt;\n  basic_cleaning() |&gt;\n  recodings() |&gt;\n  filter_arrivals() |&gt;\n  make_monthly() |&gt;\n  make_plot()\n\nIf recodings() fails halfway through, the entire pipeline crashes. You get an error message, but no information about what succeeded before the failure.\nNow imagine wrapping each function with record():\n\nlibrary(chronicler)\n\n# Robust pipeline with logging\nr_basic_cleaning &lt;- record(basic_cleaning)\nr_recodings &lt;- record(recodings)\nr_filter_arrivals &lt;- record(filter_arrivals)\nr_make_monthly &lt;- record(make_monthly)\nr_make_plot &lt;- record(make_plot)\n\nresult &lt;- raw_data |&gt;\n  r_basic_cleaning() |&gt;\n  bind_record(r_recodings) |&gt;\n  bind_record(r_filter_arrivals) |&gt;\n  bind_record(r_make_monthly) |&gt;\n  bind_record(r_make_plot)\n\n# Now you get:\n# - The result (or Nothing if any step failed)\n# - A complete log of which steps ran\n# - Exactly where and why it failed\nread_log(result)\n\nThis pattern transforms a fragile script into a robust, observable pipeline.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#integrating-with-rixpress",
    "href": "08-monads.html#integrating-with-rixpress",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.7 Integrating with rixpress",
    "text": "8.7 Integrating with rixpress\nYou can combine the power of {chronicler} with {rixpress} for even more robust pipelines. The key insight is that your user_functions can use record() internally:\n\n# functions.R\nlibrary(chronicler)\n\n# Create recorded versions of your functions\nr_basic_cleaning &lt;- record(function(data) {\n  data |&gt;\n    select(contains(\"TIME\"), contains(\"20\")) |&gt;\n    pivot_longer(cols = contains(\"20\"),\n                 names_to = \"date\",\n                 values_to = \"passengers\")\n})\n\nr_recodings &lt;- record(function(data) {\n  data |&gt;\n    mutate(tra_meas = fct_recode(tra_meas, ...)) |&gt;\n    mutate(passengers = as.numeric(passengers))\n})\n\n# Export a pipeline function that uses bind_record\nprocess_aviation_data &lt;- function(raw_data) {\n  raw_data |&gt;\n    r_basic_cleaning() |&gt;\n    bind_record(r_recodings)\n}\n\nThen in your {rixpress} pipeline:\n\nlibrary(rixpress)\n\nlist(\n  rxp_r_file(\n    name = avia_raw,\n    path = \"data/avia.tsv\",\n    read_function = readr::read_tsv\n  ),\n  \n  rxp_r(\n    name = processed_data,\n    expr = process_aviation_data(avia_raw),\n    user_functions = \"functions.R\"\n  ),\n  \n  # ... more steps\n) |&gt;\n  rxp_populate()\n\nThe {rixpress} derivation caches the result while {chronicler} provides logging and error handling within the step.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#python-integration-with-talvez",
    "href": "08-monads.html#python-integration-with-talvez",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.8 Python Integration with talvez",
    "text": "8.8 Python Integration with talvez\nThe same pattern works in Python pipelines:\n# functions.py\nfrom talvez import maybe, chain, just\n\n@maybe()\ndef basic_cleaning(df):\n    return df.dropna().reset_index(drop=True)\n\n@maybe()\ndef recodings(df):\n    df['category'] = df['category'].map(category_mapping)\n    return df\n\n@maybe()\ndef filter_arrivals(df):\n    return df[df['tra_meas'] == 'Arrivals']\n\ndef process_data(raw_df):\n    \"\"\"Returns Just(result) or Nothing with error handling.\"\"\"\n    return chain(\n        just(raw_df),\n        basic_cleaning,\n        recodings,\n        filter_arrivals\n    )\nThen in your {rixpress} pipeline:\n\nrxp_py(\n  name = processed_data,\n  expr = \"process_data(raw_data)\",\n  user_functions = \"functions.py\"\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#summary",
    "href": "08-monads.html#summary",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.9 Summary",
    "text": "8.9 Summary\nMonads add a layer of robustness to your pipelines:\n\nchronicler (record(), bind_record()): Logging and error handling for R\ntalvez (@maybe(), bind(), chain()): Error handling for Python\nJust/Nothing: Graceful failure propagation without crashes\nComposability preserved: Even with extra capabilities, functions still compose\n\nCombined with {rixpress}:\n\nNix provides hermetic, cached execution\nMonads provide logging and error handling\nTogether they give you robust, reproducible, observable pipelines\n\nThis completes our toolkit for reproducible data science: environments (Nix), functional code (FP), pipelines (rixpress), distribution (Docker), and robustness (monads).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Peng, Roger D. 2011. “Reproducible Research in Computational\nScience.” Science 334 (6060): 1226–27.",
    "crumbs": [
      "References"
    ]
  }
]