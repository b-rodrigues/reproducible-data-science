[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reproducible Polyglot Data Science",
    "section": "",
    "text": "Welcome!",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#a-modern-unified-and-language-agnostic-workflow-for-data-science-using-nix.",
    "href": "index.html#a-modern-unified-and-language-agnostic-workflow-for-data-science-using-nix.",
    "title": "Reproducible Polyglot Data Science",
    "section": "A modern, unified, and language-agnostic workflow for data science using Nix.",
    "text": "A modern, unified, and language-agnostic workflow for data science using Nix.\nThis is still WIP! Expected first finalized release at the end of Q1 2026. This book is a complete reimagining of my previous work, “Building Reproducible Analytical Pipelines with R.” If you’re looking for that book, you can find it here. But if you’re ready for the next step, you’re in the right place.\n\n  \n\nData scientists, statisticians, analysts, researchers, and many other professionals write a lot of code.\nNot only do they write a lot of code, but they must also read and review a lot of code as well. They either work in teams and need to review each other’s code, or need to be able to reproduce results from past projects, be it for peer review or auditing purposes. And yet, they never, or very rarely, get taught the tools and techniques that would make the process of writing, collaborating, reviewing and reproducing projects possible.\nWhich is truly unfortunate because software engineers face the same challenges and solved them decades ago.\nThe aim of this book is to teach you how to use some of the best practices from software engineering and DevOps to make your projects robust, reliable and reproducible. It doesn’t matter if you work alone, in a small or in a big team. It doesn’t matter if your work gets (peer-)reviewed or audited: the techniques presented in this book will make your projects more reliable and save you a lot of frustration!\nAs someone whose primary job is analysing data, you might think that you are not a developer. It seems as if developers are these genius types that write extremely high-quality code and create these super useful packages. The truth is that you are a developer as well. It’s just that your focus is on writing code for your purposes to get your analyses going instead of writing code for others. Or at least, that’s what you think. Because in others, your team-mates are included. Reviewers and auditors are included. Any people that will read your code are included, and there will be people that will read your code. At the very least future you will read your code. By learning how to set up projects and write code in a way that future you will understand and not want to murder you, you will actually work towards improving the quality of your work, naturally.\nThe book can be read for free on https://b-rodrigues.github.io/reproducible-data-science/ and you’ll be able buy a DRM-free Epub or PDF on Leanpub1 once there’s more content.\nThis book is the culmination of my previous works. I started by writing a book focused on R, and then began working on a Python edition. During that process, I had a realization: tackling reproducibility one language at a time was solving the symptoms, not the root cause. The real solution needed to be universal, powerful, and capable of handling any language or tool we might need.\nThat universal solution is Nix.\nThis book moves beyond language-specific tooling. It presents a holistic workflow where R, Python, and Julia are not competitors, but collaborators in a single, cohesive, and perfectly reproducible environment. We will cover:\n\nThe Nix Philosophy: Why Nix is the ultimate tool for solving the “it works on my machine” problem, once and for all.\nDeclarative Environments with {rix}: How to use a simple R interface to define exact, bit-for-bit reproducible software environments that include specific versions of R, Python, Julia, their packages, and any system-level dependencies.\nPolyglot Pipelines with {rixpress} (R) or ryxpress (Python): How to orchestrate complex analytical pipelines that seamlessly pass data between different languages, all managed by the Nix build system.\nUnit Testing and Functional Programming: Core principles for writing robust, testable, and maintainable code, no matter the language.\nDistribution and Automation: How to package your entire reproducible pipeline into a Docker container for easy sharing and automate your workflow with GitHub Actions.\n\nWhile this is not a book for beginners (you should be familiar with at least one data-centric programming language before reading this), I will not assume that you have any knowledge of the tools discussed. But be warned, this book will require you to take the time to read it, and then type on your computer. Type a lot.\nI hope that you will enjoy reading this book and applying the ideas in your day-to-day, ideas which hopefully should improve the reliability, traceability and reproducibility of your code.\nIf you find this book useful, don’t hesitate to let me know! You can submit issues, suggest improvements, and ask questions on the book’s Github repository.\nYou can also buy a physical copy of the book on Amazon.\nIf you want to get to know me better, read my bio2.\nYou’ll also be able to buy a physical copy of the book on Amazon once it’s done. In the meantime, you could buy the R edition.\nIf you find this book useful, don’t hesitate to let me know or leave a rating on Amazon!",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Reproducible Polyglot Data Science",
    "section": "",
    "text": "https://leanpub.com/↩︎\nhttps://www.brodrigues.co/about/me/↩︎",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Three years ago, I wrote a book with a straightforward premise: by borrowing a few key ideas from software engineering, people who analyse data could save themselves a great deal of frustration. The response to that book was more positive than I could have ever hoped for, and it confirmed a suspicion I had: we, as a community, are hungry for better ways to work.\nThat book, however, focused exclusively on the R ecosystem. A recurring question I received was, “This is great, but what about Python?” It was a fair question. The world of data science is not a monologue; it’s a conversation between languages. So, I began what felt like the logical next step: writing a Python edition.\nI mapped out the chapters and identified the equivalent tools, pipenv for dependency management, ploomber for pipelines, and started writing. But as I went deeper, a nagging feeling grew. I was solving the same problems all over again, just with a different set of tools. This feeling was compounded by the rapid churn within the Python ecosystem itself. How many package managers have been created to solve virtual environment management? As of writing, uv is all the rage, and while it may be here to stay, history suggests a new contender is always just around the corner.\nThis pointed to a larger issue. I am convinced that the future of data science is polyglot. An R user and a Python user, both following my original advice, would end up with reproducible projects, but their workflows would be fundamentally incompatible. They couldn’t easily share an environment or build a single pipeline that leverages the strengths of both languages. While companies like Posit have made excellent progress in making it easier to call Python from R, setting up a truly integrated development environment remains a challenge. And what if you wish to bring Julia, the other language of data analysis, into the fold? It is not as popular as Python or R, but it has its own distinct appeal and advantages.\nI realised I was treating the symptoms, not the disease. The root problem wasn’t “How do I make R reproducible?” or “How do I make Python reproducible?”. The real challenge was the lack of a universal foundation that could handle any language, any tool, and any system dependency with absolute, bit-for-bit precision.\nThat’s when I stopped writing the Python book.\nThe solution wasn’t to create another language-specific guide but to find a tool that operated at a more fundamental level. That tool, I am now convinced, is Nix.\nNix is not just another package manager; it is a powerful, declarative system for building and managing software environments. It allows us to define the entire computational environment—from the operating system libraries up to the specific versions of our R and Python packages—in a single, simple text file. When you use Nix, the phrase “it works on my machine” becomes obsolete. It is replaced by the guarantee: “it builds identically, everywhere, every time”—with a few caveats that we will explore, of course.\nThis book is the result of that realisation. It is a complete reimagining of the original. We are moving away from language-specific patchworks and toward a unified, polyglot workflow. We will use Nix as our bedrock, with the {rix} and {rixpress} R packages (or ryxpress for Python) serving as our friendly interface to its power. You will learn to build pipelines where R, Python, and Julia aren’t just neighbours; they are collaborators, working together in a single, perfectly reproducible environment.\nA note for Python-first users: do not be deterred by the fact that {rix} and {rixpress} are R packages: there is a Python version called ryxpress that will allow you to run your pipelines from an interactive Python session. You will be able to use them to define your environments (even integrating tools like uv) and orchestrate your pipelines, while doing all of your analytical work exclusively in Python. In this workflow, R simply becomes a convenient configuration language.\nThe core message from three years ago remains unchanged. You, as someone who writes code to analyse data, are a developer. Your work is important, and it deserves to be reliable. This book aims to give you the tools and the mindset to achieve that. The journey is more ambitious this time, but the payoff is far greater.\nI hope you’ll join me.\nYou can read this book for free online at https://b-rodrigues.github.io/reproducible-data-science/.\nYou can submit issues, suggest improvements, and ask questions on the book’s Github repository.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Who is this book for?\nJust like the previous, R-focused edition of this book, this one will not teach you about machine learning, statistics, or visualisation.\nThe goal is to teach you a set of tools, practices, and project management techniques that should make your projects easier to reproduce, replicate, and retrace, with a focus on polyglot (multilingual) projects. I believe that with LLMs, polyglot projects will become increasingly common.\nBefore LLMs, if you were a Python user, you would avoid R like the plague, and vice versa. This is understandable: even though both are high-level scripting languages, and an R user could likely read and understand Python code (and vice versa), it is still a pain in the loins to set up another language just to run a few lines of code. Now, with LLMs, you can have a model generate the code, and depending on what you’re doing, it’s likely to be correct. However, setting up another language is still quite annoying. This is where Nix comes in. Nix makes adding another language to a project extremely simple.\nEven though tools like {rix}, {rixpress}, and Docker might be new to you, you can become productive with them very quickly by leveraging LLMs. The key is to provide the LLM with the right context. I have built a tool called pkgctx1 that extracts structured, compact API specifications from R or Python packages for use in LLMs, minimising tokens while maximising context. Each of the repositories for these tools—{rix}, {rixpress} (R), and rixpress (Python)—contains a .pkgctx.yaml file that you can feed to an LLM to help it understand the package’s API. With this context, the LLM can assist you in writing correct code using these tools, even if you’ve never used them before.\nYou can run pkgctx directly without installing it, thanks to Nix:\nI encourage you to do the same for the packages you use in your analyses. By generating context files using pkgctx, you enable LLMs to help you write code more efficiently and correctly. This is especially useful for packages with complex APIs or for packages you don’t use frequently. Simply generate the context file once and include it in your project repository—or feed it to your LLM whenever you need assistance with that package.\nThis book is for anyone who uses raw data to build any type of output. This could be a simple quarterly report, in which data is used for tables and graphs, a scientific article for a peer-reviewed journal, or even an interactive web application. The specific output doesn’t matter, because the process is, at its core, always very similar:\nThis book assumes some familiarity with programming, particularly with the R and Python languages. I will not discuss Julia in great detail, as I am not familiar enough with it to do it justice. That being said, I will show you how to add Julia to a project and use it effectively if you need to.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#who-is-this-book-for",
    "href": "intro.html#who-is-this-book-for",
    "title": "1  Introduction",
    "section": "",
    "text": "Get the data;\nClean the data;\nWrite code to analyse the data;\nPut the results into the final product.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-is-the-aim-of-this-book",
    "href": "intro.html#what-is-the-aim-of-this-book",
    "title": "1  Introduction",
    "section": "1.2 What is the aim of this book?",
    "text": "1.2 What is the aim of this book?\nThe aim of this book is to make the process of analysing data as reliable, retraceable, and reproducible as possible, and to do this by design. This means that once you’re done with the analysis, you’re done. You don’t want to spend time, which you often don’t have anyway, to rewrite or refactor an analysis to make it reproducible after the fact. We both know this is not going to happen. Once an analysis is finished, it’s on to the next one. If you need to rerun an older analysis (for example, because the data has been updated), you’ll simply figure it out at that point, right? That’s a problem for Future You. Hopefully, Future You will remember every quirk of your code, know which script to run at which point, which comments are outdated, and what features of the data need to be checked… You had better hope Future You is a more diligent worker than you are!\nGoing forward, I’m going to refer to a project that is reproducible as a “Reproducible Analytical Pipeline”, or RAP for short. There are only two ways to create a RAP: either you are lucky enough to have someone on your team whose job is to turn your messy code into a RAP, or you do it yourself. The second option is by far the most common. The issue, as stated above, is that most of us simply don’t do it. We are always in a rush to get to the results and don’t think about making the process reproducible, because we assume it takes extra time that is better spent on the analysis itself. This is a misconception, for two reasons.\nThe first is that employing the techniques we will discuss in this book won’t actually take much time. As you will see, they are not things you “add on top of” the analysis, but are part of the analysis itself, and they will also help with managing the project. Some of these techniques, especially testing, will even save you time and headaches.\nThe second reason is that an analysis is never a one-shot. Only the simplest tasks, like pulling a single number from a database, might be. Even then, chances are that once you provide that number, you’ll be asked for a variation of it (for example, disaggregated by one or several variables). Or perhaps you’ll be asked for an update in six months. You will quickly learn to keep that SQL query in a script somewhere to ensure consistency. But what about more complex analyses? Is keeping the script enough? It is a good start, of course, but very often, there is no single script, or a script for each step of the analysis is missing.\nI’ve seen this play out many times in many different organisations. It’s that time of the year again, and a report needs to be written. Ten people are involved, and just gathering the data is already complicated. Some get their data from Word documents attached to emails, some from a website, some from a PDF report from another department. I remember a story a senior manager at my previous job used to tell: once, a client put out a call for a project that involved setting up a PDF scraper. They periodically needed data from another department that only came in PDFs. The manager asked what was, at least from our perspective, an obvious question: “Why can’t they send you the underlying data in a machine-readable format?” They had never thought to ask. So, my manager went to that department and talked to the people putting the PDF together. Their answer? “Well, we could send them the data in any format they want, but they’ve asked for the tables in a PDF.”\nSo the first, and probably most important, lesson here is: when starting to build a RAP, make sure you talk with all the people involved.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "1  Introduction",
    "section": "1.3 Prerequisites",
    "text": "1.3 Prerequisites\nYou should be comfortable with the command line. This book will not assume any particular Integrated Development Environment (IDE), so most of what I’ll show you will be done via the command line. That said, I will spend some time helping you set up a data science-focused IDE, Positron, to work seamlessly with this workflow. The command line may be over 50 years old, but it is not going anywhere. In fact, thanks to the rise of LLMs, it seems to be enjoying a resurgence. Since these models generate text, it is far simpler to ask one for a shell command to solve a problem than to have it produce detailed instructions on where to click in a graphical user interface (GUI). Knowing your way around the command line is also essential for working with modern data science infrastructure: continuous integration platforms, Docker, remote servers… they all live in the terminal. So, if you’re not at all familiar with the command line, you might need to brush up on the basics. Don’t worry, though; this isn’t a book about the intricacies of the Unix command line. The commands I’ll show you will be straightforward and directly applicable to the task at hand.\nThis means however that if you are using Windows, first of all, why? and second of all, you will have to set up Windows Subsystem for Linux. This is because there is no native Nix implementation for Windows, and so we need to run the Linux version through WSL. Don’t worry though, it’s not that hard, and you can then use an IDE from Windows to work with the environments managed by Nix, in a very seamless way (but seriously, consider, if you can, switching to Linux. How can you tolerate ads (ADS!) in the start menu).\nIdeally, you should be comfortable with either R or Python. This book will assume that you have been using at least one of these languages for some projects and want to improve how you manage complex projects. You should know about packages and how to install them, have written some functions, understand loops, and have a basic knowledge of data structures like lists. While this is not a book on visualisation, we will be making some graphs as well.\nOur aim is to write code that can be executed non-interactively. This is because a necessary condition for a workflow to be reproducible and to qualify as a RAP is for it to be executed by a machine, automatically, without any human intervention. This is the second lesson of building RAPs: there should be no human intervention needed to get the outputs once the RAP has started. If you achieve this, then your workflow is likely reproducible, or can at least be made so much more easily than if it requires special manipulation by a human somewhere in the loop.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#what-actually-is-reproducibility",
    "href": "intro.html#what-actually-is-reproducibility",
    "title": "1  Introduction",
    "section": "1.4 What actually is reproducibility?",
    "text": "1.4 What actually is reproducibility?\nA reproducible project means that it can be rerun by anyone at zero (or very minimal) cost. But there are different levels of reproducibility, which I will discuss in the next section. Let’s first outline the requirements that a project must meet to be considered a RAP.\n\n1.4.1 Using open-source tools is a hard requirement\nOpen source is a hard requirement for reproducibility. No ifs, ands, or buts. I’m not just talking about the code you wrote for your research paper or report; I’m talking about the entire ecosystem you used to write your code and build the workflow.\nIs your code open? Good. Or is it at least available to others in your organisation, in a way that they could re-execute it if needed? Also good.\nBut is it code written in a proprietary program, like STATA, SAS, or MATLAB? Then your project is not reproducible. It doesn’t matter if the code is well-documented, well-written, and available on a version control system. The project is simply not reproducible. Why?\nBecause on a long enough time horizon, there is no way to re-execute your code with the exact same version of the proprietary language and operating system that were used when the project was developed. As I’m writing these lines, MATLAB, for example, is at version R2025a. Buying an older version may not be simple. I’m sure if you contact their sales department, they might be able to sell you an older version. Maybe you can even re-download older versions you’ve already purchased from their website. But maybe it’s not that straightforward. Or maybe they won’t offer this option in the future. In any case, if you search for “purchase old version of Matlab,” you will see that many researchers and engineers have this need. ::: {.content-hidden when-format=“pdf”}\n\n\n\nWanting to run older versions of analytics software is a recurrent need.\n\n\n:::\nAnd if you’re running old code written for version, say, R2008a, there’s no guarantee that it will produce the exact same results on version R2025a. That’s without even mentioning the toolboxes (if you’re not familiar with them, they’re MATLAB’s equivalent of packages or libraries). These evolve as well, and there’s no guarantee that you can purchase older versions. It’s also likely that newer toolboxes cannot even run on older versions of MATLAB.\nLet me be clear: what I’m describing here for MATLAB could also be said for any other proprietary programs still commonly (and unfortunately) used in research and statistics, like STATA, SAS, or SPSS. Even if some of these vendors provide ways to run older versions of their software, the fact that you have to rely on them for this is a barrier to reproducibility. There is no guarantee they will provide this option forever. Who can guarantee that these companies will even be around forever? More likely, they might shift from a program you install on your machine to a subscription-based model.\nFor just 199€ a month, you can execute your SAS (or whatever) scripts on the cloud! Worried about data confidentiality? No problem, data is encrypted and stored safely on our secure servers! Run your analysis from anywhere and don’t worry about your cat knocking coffee over your laptop! And if you purchase the pro licence, for an additional 100€ a month, you can even execute your code in parallel!\nThink this is science fiction? There is a growing and concerning trend for vendors to move to a Software-as-a-Service model with monthly subscriptions. It happened to Adobe’s design software, and the primary reason it hasn’t yet happened for data analytics tools is data privacy concerns. Once those are deemed “solved,” I would not be surprised to see a similar shift.\n\n\n1.4.2 Hidden dependencies can hinder reproducibility\nThen there’s another problem. Let’s suppose you’ve written a thoroughly tested and documented workflow and made it available on GitHub (and let’s even assume the data is freely available and the paper is open access). Or, if you’re in the private sector, you’ve done all of the above, but the workflow is only available internally.\nLet’s further assume that you’ve used R, Python, or another open-source language. Can this analysis be said to be reproducible? Well, if it ran on a proprietary operating system, then the conclusion is: your project is not fully reproducible.\nThis is because the operating system the code runs on can also influence the outputs. There are particularities in operating systems that may cause certain things to work differently. Admittedly, this is rarely a problem in practice, but it does happen2, especially if you’re working with high-precision floating-point arithmetic, as you might in the financial sector, for instance.\nThankfully, as you will see in this book, there is no need to change your operating system to deal with this issue.\n\n\n1.4.3 The requirements of a RAP\nSo where does that leave us? For a project to be truly reproducible, it has to respect the following points:\n\nSource code must be available and thoroughly tested and documented (which is why we will be using Git and GitHub).\nAll dependencies must be easy to find and install (we will deal with this using dependency management tools).\nIt must be written in an open-source programming language (no-code tools like Excel are non-reproducible by default because they can’t be used non-interactively, which is why we will be using languages like R, Python, and Julia).\nThe project needs to run on an open-source operating system (we can deal with this without having to install and learn a new OS, thanks to tools like Docker).\nThe data and the final report must be accessible—if not publicly, then at least within your company. This means the concept of “scripts and/or data available upon request” belongs in the bin.\n\n\n\n\nA real sentence from a real paper published in THE LANCET Regional Health. How about make the data available and I won’t scratch your car, how’s that for a reasonable request?\n\n\n\n\n1.4.4 Are there different types of reproducibility?\nLet’s take one step back. We live in the real world, where constraints outside of our control can make it impossible to build a true RAP. Sometimes we need to settle for something that might not be perfect, but is the next best thing.\nIn what follows, let’s assume the code is tested and documented, so we will only discuss the pipeline’s execution.\nThe least reproducible pipeline would be something that works, but only on your machine. This could be due to hardcoded paths that only exist on your laptop. Anyone wanting to rerun the pipeline would need to change them. This should be documented in a README, which we’ve assumed is the case. But perhaps the pipeline only runs on your laptop because the computational environment is hard to reproduce. Maybe you use software, even open-source software, that is not easy to install (anyone who has tried to install R packages on Linux that depend on {rJava} knows what I’m talking about).\nA better, though still imperfect, pipeline would be one that could be run more easily on any similar machine. This could be achieved by avoiding hardcoded absolute paths and by providing instructions to set up the environment. For example, in Python, this could be as simple as providing a requirements.txt file that lists the project’s dependencies, which could be installed using pip:\npip install -r requirements.txt\nDoing this helps others (or Future You) install the required packages. However, this is not enough, as other software on your system, outside of what pip manages, can still impact the results.\nYou should also ensure that people run the same analysis on the same versions of R or Python that were used to create it. Just installing the right packages is not enough. The same code can produce different results on different versions of a language, or not run at all. If you’ve been using Python for some time, you certainly remember the switch from Python 2 to Python 3. Who knows, the switch to Python 4 might be just as painful!\nThe take-away message is that relying on the language itself being stable over time is not a sufficient condition for reproducibility. We have to set up our code in a way that is explicitly reproducible by dealing with the versions of the language itself.\nSo what does this all mean? It means that reproducibility exists on a continuum. Depending on the constraints you face, your project can be “not very reproducible” or “totally reproducible”. Let’s consider the following list of factors that can influence how reproducible your project truly is:\n\nVersion of the programming language used.\nVersions of the packages/libraries of said programming language.\nThe operating system and its version.\nVersions of the underlying system libraries (which often go hand-in-hand with the OS version, but not always).\nAnd even the hardware architecture that you run the software stack on.\n\nBy “reproducibility is on a continuum,” I mean that you can set up your project to take none, one, two, three, four, or all of the preceding items into consideration.\nThis is not a novel, or new idea. Peng (2011) already discussed this concept but named it the reproducibility spectrum:\n\n\n\nThe reproducibility spectrum from Peng’s 2011 paper.\n\n\nLet me finish this introduction by discussing the last item on the list: hardware architecture. In 2020, Apple changed the hardware architecture of their computers. Their new machines no longer use Intel CPUs, but instead Apple’s own proprietary architecture (Apple Silicon) based on the ARM specification. Concretely, this means that binary packages built for Intel-based Apple computers cannot run on their new machines, at least not without a compatibility layer. If you have a recent Apple Silicon Mac and need to install old packages to rerun a project (and we will learn how to do this), they need to be compiled to work on Apple Silicon first. While a compatibility layer called Rosetta 2 exists, my point is that you never know what might come in the future. The ability to compile from source is important because it requires the fewest dependencies outside of your control. Relying on pre-compiled binaries is not future-proof, which is another reason why open-source tools are a hard requirement for reproducibility.\nFor you Windows users, don’t think that the preceding paragraph does not concern you. It is very likely that Microsoft will push for OEM manufacturers to build more ARM-based computers in the future. There is already an ARM version of Windows, and I believe Microsoft will continue to support it. This is because ARM is much more energy-efficient than other architectures, and any manufacturer can build its own ARM CPUs by purchasing a license—a very interesting proposition from a business perspective.\nIt is also possible that we will move towards more cloud-based computing, though I think this is less likely than the hardware shift. In that case, it is quite likely that the actual code will be running on Linux servers that are ARM-based, due to energy and licensing costs. Here again, if you want to run your historical code, you’ll have to compile old packages and programming language versions from source.\nOk, this might all seem incredibly complicated. How on earth are we supposed to manage all these risks and balance the immediate need for results with the future need to rerun an old project? And what if rerunning it is never needed?\nAs you shall see, this is not as difficult as it sounds, thanks to Nix and the tireless efforts of the nixpkgs maintainers who work to build truly reproducible packages.\nLet’s dive in!\n\n\n\n\nPeng, Roger D. 2011. “Reproducible Research in Computational Science.” Science 334 (6060): 1226–27.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "https://github.com/b-rodrigues/pkgctx↩︎\nhttps://github.com/numpy/numpy/issues/9187↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-nix.html",
    "href": "02-nix.html",
    "title": "2  The Nix Package Manager",
    "section": "",
    "text": "2.1 Introduction\nNix is a package manager that can be installed on your computer, regardless of the operating system. If you are familiar with the Ubuntu Linux distribution, you have likely used apt-get to install software. On macOS, you may have used homebrew for similar purposes. Nix functions in a comparable way but has many advantages over classic package managers, as it focuses on reproducible builds and downloads packages from nixpkgs, currently the largest software repository1.\nIn this chapter, we will explore the critical need for environment reproducibility in modern workflows. We will see why ad-hoc tools often fail, and how Nix’s declarative approach and “component closures” provide a robust solution. We will also cover the core concepts of Nix—derivations, the store, and hermetic builds—that make this possible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "02-nix.html#why-reproducibility-why-nix",
    "href": "02-nix.html#why-reproducibility-why-nix",
    "title": "2  The Nix Package Manager",
    "section": "2.2 Why Reproducibility? Why Nix?",
    "text": "2.2 Why Reproducibility? Why Nix?\n\n2.2.1 Motivation: Reproducibility in Scientific and Data Workflows\nTo ensure that a project is reproducible you need to deal with at least four things:\n\nEnsure the required version of your programming language (R, Python, etc.) is installed.\nEnsure the required versions of all packages are installed.\nEnsure all necessary system dependencies are installed (for example, a working Java installation for the {rJava} R package on Linux).\nEnsure you can install all of this on the hardware you have on hand.\n\nBut in practice, one or most of these bullet points are missing from projects. The goal of this course is to learn how to fulfil all the requirements to build reproducible projects.\nThe current consensus for tackling the first three points is often a mixture of tools: Docker for system dependencies, {renv} or uv for package management, and tools like the R installation manager (rig) for language versions. As for the last point, hardware architecture, the only way out is to be able to compile the software for the target platform. This involves a lot of moving parts and requires significant knowledge to get right.\n\n\n2.2.2 Problems with Ad-Hoc Tools\nTools like Python’s venv or R’s renv only deal with some pieces of the reproducibility puzzle. Often, they assume an underlying OS, do not capture system-level dependencies (like libxml2, pandoc, or curl), and require users to “rebuild” their environments from partial metadata. Docker helps but introduces overhead, security challenges, and complexity, and just adding it to your project doesn’t make it reproducible if you don’t explicitly take some precautionary steps.\nTraditional approaches fail to capture the entire dependency graph of a project in a deterministic way. This leads to “it works on my machine” syndromes, onboarding delays, and subtle bugs.\n\n\n2.2.3 Nix: A Declarative Solution\nWith Nix, we can handle all of these challenges with a single tool.\nThe first advantage of Nix is that its repository, nixpkgs, is humongous. As of this writing, it contains over 120,000 pieces of software, including the entirety of CRAN and Bioconductor. This means you can use Nix to handle everything: R, Python, Julia, their respective packages, and any other software available through nixpkgs, making it particularly useful for polyglot pipelines.\nThe second and most crucial advantage is that Nix allows you to install software in (relatively) isolated environments. When you start a new project, you can use Nix to install a project-specific version of R and all its packages. These dependencies are used only for that project. If you switch to another project, you switch to a different, independent environment. But this also means that all the dependencies of R and R packages, plus all of their dependencies and so on get installed as well. Your project’s development environment will not depend on anything outside of it (well, there are some caveats which we will explore as we move on).\nThis is similar to {renv}, but the difference is profound: you get not only a project-specific library of R packages but also a project-specific R version and all the necessary system dependencies. For example, if you need {xlsx}, Nix automatically figures out that Java is required and installs and configures it for you, without any intervention.\nWhat’s more, you can pin your project to a specific revision of the nixpkgs repository. This ensures that every package Nix installs will always be at the exact same version, regardless of when or where the project is built. The environment is defined in a simple plain-text file, and anyone using that file will get a byte-for-byte identical environment, even on a different operating system.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "02-nix.html#important-concepts",
    "href": "02-nix.html#important-concepts",
    "title": "2  The Nix Package Manager",
    "section": "2.3 Important Concepts",
    "text": "2.3 Important Concepts\nBefore we start using Nix, it is important to spend some time learning about some Nix-centric concepts, starting with the derivation.\nIn Nix terminology, a derivation is a specification for running an executable on precisely defined input files to repeatably produce output files at uniquely determined file system paths. (source)\nIn simpler terms, a derivation is a recipe with precisely defined inputs, steps, and a fixed output. This means that given identical inputs and build steps, the exact same output will always be produced. To achieve this level of reproducibility, several important measures must be taken:\n\nAll inputs to a derivation must be explicitly declared (and “inputs” here is meant in a very broad sense; for example, configuration flags are also inputs!).\nInputs include not just data files but also software dependencies, configuration flags, and environment variables: essentially, anything necessary for the build process.\nThe build process takes place in a hermetic sandbox to ensure the exact same output is always produced.\n\nThe next sections explain these three points in more detail.\n\n2.3.1 Derivations\nHere is an example of a simple Nix expression:\nlet\n  pkgs = import (fetchTarball \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-04-11.tar.gz\") {};\nin\npkgs.stdenv.mkDerivation {\n  name = \"filtered_mtcars\";\n  buildInputs = [ pkgs.gawk ];\n  dontUnpack = true;\n  src = ./mtcars.csv;\n  installPhase = ''\n    mkdir -p $out\n    awk -F',' 'NR==1 || $9==\"1\" { print }' $src &gt; $out/filtered.csv\n  '';\n}\nWithout going into too much detail, this code uses awk, a common Unix data processing tool, to filter the mtcars.csv file. As you can see, a significant amount of boilerplate is required for this simple operation. However, this approach is completely reproducible: the dependencies are declared and pinned to a specific version of the nixpkgs repository. The only thing that could make this small pipeline fail is if the mtcars.csv file is not provided to it.\nNix builds the filtered.csv output file in two steps: it first generates a derivation from this expression, and only then does it build the output. For clarity, I will refer to code like the example above as a derivation rather than an expression, to avoid confusion with the concept of an expression in R.\nThe goal of the tools we will use in this book, {rix} and {rixpress} (or ryxpress if you prefer using Python), is to help you create pipelines from such derivations without needing to learn the Nix language itself, while still benefiting from its powerful reproducibility features.\n\n\n2.3.2 Dependencies of derivations\nNix requires that the dependencies of any derivation be explicitly listed and managed by Nix itself. If you are building an output that requires Quarto, then Quarto must be explicitly listed as an input, even if you already have it installed on your system. The same applies to Quarto’s dependencies, and their dependencies, all the way down. To run a linear regression with R, you essentially need Nix to build the entire universe of software that R depends on first.\nIn Nix terms, this complete set of packages is what its author, Eelco Dolstra, refers to as a component closure:\n\nThe idea is to always deploy component closures: if we deploy a component, then we must also deploy its dependencies, their dependencies, and so on. That is, we must always deploy a set of components that is closed under the ‘depends on’ relation.\n\n(Nix: A Safe and Policy-Free System for Software Deployment, Dolstra et al., 2004).\n\n\n\nFigure 4 of Dolstra et al. (2004)\n\n\nIn the figure, subversion depends on openssl, which itself depends on glibc. Similarly, if you write a derivation to filter mtcars, it requires an input file, R, {dplyr}, and all of their respective dependencies. All of these must be managed by Nix. If any dependency exists “outside” this closure, the pipeline will only work on your machine, defeating the purpose of reproducibility.\n\n\n2.3.3 The Nix store and hermetic builds\nWhen building derivations, their outputs are saved into the Nix store. Typically located at /nix/store/, this folder contains all the software and build artefacts produced by Nix.\nFor example, the output of a derivation might be stored at a path like /nix/store/81k4s9q652jlka0c36khpscnmr8wk7jb-filtered. The long cryptographic hash uniquely identifies the build output and is computed based on the content of the derivation and all its inputs. This ensures that the build is fully reproducible.\nAs a result, building the same derivation on two different machines will yield the same cryptographic hash. You can substitute the built artefact with the derivation that generates it one-to-one, just as in mathematics, where writing \\(f(2)\\) is the same as writing \\(4\\) for the function \\(f(x) := x^2\\).\nTo guarantee that derivations always produce identical outputs, builds must occur in an isolated environment known as a hermetic sandbox. This process ensures that the build is unaffected by external factors, such as the state of the host system. This isolation extends to environment variables and even network access. If you need to download data from an API, for example, it will not work from within the build sandbox. This may seem restrictive, but it makes perfect sense for reproducibility: an API’s output can change over time.\nFor a truly reproducible result, you should obtain the data once, version it, and use that archived data as an input to your analysis.\n\n\n2.3.4 Other key Nix concepts\nWe’ve covered derivations, dependencies, closures, the Nix store, and hermetic builds. That’s the core of what makes Nix tick. But there are a few more concepts worth knowing about before we move on:\n\nPurity: Nix tries very hard to keep builds “pure”: the output only depends on what you explicitly list as inputs. If a build script tries to reach out to the internet or read some random file on your machine, Nix will block it. That can feel restrictive at first, but it’s what guarantees reproducibility.\nBinary caches: You don’t always need to build everything yourself. Think back to the math analogy: if you already know that \\(f(2) = 4\\), there’s no need to compute it again; just reuse the result! Nix does the same with binary caches: because every build is identified by its unique cryptographic hash made from its inputs, this means a prebuilt package fetched from cache.nixos.org (or your own cache you may want to set up) is bit-for-bit identical to what you would have built locally. This is why Nix is both reproducible and fast.\nGarbage collection: Since Nix never overwrites anything in the store, old packages can pile up. Running nix-store --gc will run the garbage collector to free up space.\nOverlays: If you want to tweak a package or add your own without forking all of nixpkgs, you can use overlays. They let you extend or override existing definitions in a clean, composable way.\nFlakes: The newer way to define and pin Nix projects. Flakes make it easier to share and reuse Nix setups across machines and repositories. There is a lot of discussion around flakes, as they’re officially still considered not stable, even though they’ve been widely adopted by the community. But don’t worry, this is not something you’ll need to think about for this book.\n\nTogether, these features explain why Nix isn’t just another package manager. It’s more like a framework for reproducible environments that can scale from a single project to an entire operating system (called NixOS2).\nAs a little sidenote: I want also to highlight that Nix can even be used to declaratively and reproducibly configure your operating system (be it NixOS, macOS or other Linux distributions) using a tool that integrates with it called homemanager.3 This is outside the scope of this book, but I wanted to highlight it, as it’s extremely powerful. What this means in practice is that you could write a whole Nix expression that not only downloads and configures software, but even sets up users with their specific software, and preferences like wallpapers, colour schemes and so on.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "02-nix.html#caveats",
    "href": "02-nix.html#caveats",
    "title": "2  The Nix Package Manager",
    "section": "2.4 Caveats",
    "text": "2.4 Caveats\nWhile Nix is powerful, there are some limitations and practical hurdles to be aware of if you plan to use it for actual work:\n\nHardware acceleration: On non-NixOS systems, it can be difficult to set up GPU acceleration (CUDA, ROCm, OpenCL). Drivers are tightly coupled to the host kernel and libraries, while Nix builds aim for strict isolation. On NixOS this integration is smoother, but on macOS or other Linux distros you may encounter limitations or extra manual steps.\nmacOS-specific issues: Reproducibility is harder to achieve on macOS (both Intel and Apple Silicon) than on Linux. Nix packages often rely on Apple system frameworks (e.g., CoreFoundation, Security) that live outside the Nix store and compromise hermetic builds4. The macOS sandbox is also weaker than Linux’s and sometimes leaks system tools like Xcode or Rosetta into builds5. Hydra cache coverage is thinner for Darwin platforms, especially Apple Silicon, so cache misses and local builds are much more common6. Finally, pinned environments can break after macOS or Xcode updates because of changes in system libraries or compiler flags7.\nThat said, in practice most packages build fine on macOS, and the ecosystem continues to improve. When reproducibility problems do appear, the simplest fix is often just to use another nearby nixpkgs revision for pinning that is known to work. This makes the situation less fragile than it might first appear. Another solution would be to use Docker to deploy the Nix environment and use that as a dev container. All of this will be discussed in detail in this book.\nSteep learning curve: The Nix language and ecosystem (flakes, overlays, derivations) can be conceptually difficult if you come from traditional package managers. Even basic customizations require some ramp-up time. This was my main motivation to write {rix}, {rixpress} (for R) and ryxpress (for Python).\nDisk space and builds: Because Nix never mutates software in place, the store can accumulate large amounts of data. Cache misses sometimes force local builds, which can be slow and resource-intensive. However, it is of course possible to empty the Nix store to recover disk space, and it is also possible to set up your own project cache if you wish so. Setting up your own cache will also be something that we will explore in this book.\n\nThese caveats don’t diminish Nix’s strengths but highlight that its guarantees are strongest on Linux (and thus WSL), and especially on NixOS. On macOS, reproducibility is possible but sometimes requires extra work, a bit of flexibility, and occasionally picking a different nixpkgs snapshot.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "02-nix.html#in-summary",
    "href": "02-nix.html#in-summary",
    "title": "2  The Nix Package Manager",
    "section": "2.5 In summary",
    "text": "2.5 In summary\nNix makes it possible to actually build software reproducibly. To achieve this, it introduces several core concepts that are quite specific, and definitely worth taking the time to understand.\nIn the next chapter, we will learn how to install Nix, configure cachix, and set up Positron for a seamless development experience.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "02-nix.html#footnotes",
    "href": "02-nix.html#footnotes",
    "title": "2  The Nix Package Manager",
    "section": "",
    "text": "https://repology.org/repositories/graphs↩︎\nhttps://nixos.org/↩︎\nhttps://github.com/nix-community/home-manager↩︎\nhttps://github.com/NixOS/nixpkgs/issues/67166↩︎\nhttps://discourse.nixos.org/t/nix-macos-sandbox-issues-in-nix-2-4-and-later/17475↩︎\nhttps://www.reddit.com/r/NixOS/comments/17uxj6q/how_does_nix_package_manager_work_on_apple_silicon/↩︎\nhttps://github.com/NixOS/nix/issues/11679↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Nix Package Manager</span>"
    ]
  },
  {
    "objectID": "03-setup.html",
    "href": "03-setup.html",
    "title": "3  Setting Up Your Environment",
    "section": "",
    "text": "3.1 Installing Nix\nIn this chapter, we will install Nix, configure the rstats-on-nix binary cache to speed up installations, and set up our development environment (Positron, VS Code, or Emacs) to work seamlessly with reproducible Nix environments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "03-setup.html#installing-nix",
    "href": "03-setup.html#installing-nix",
    "title": "3  Setting Up Your Environment",
    "section": "",
    "text": "3.1.1 For Windows Users: WSL2 Prerequisites\nIf you are on Windows, you need the Windows Subsystem for Linux 2 (WSL2) to run Nix. If you are on a recent version of Windows 10 or 11, you can simply run this as an administrator in PowerShell:\nwsl --install\nYou can find further installation notes at this official MS documentation.\nI recommend activating systemd in Ubuntu WSL2, mainly because this supports users other than root running Nix. To set this up, please follow this official Ubuntu blog entry:\n# in WSL2 Ubuntu shell\n\nsudo -i\nnano /etc/wsl.conf\nThis will open /etc/wsl.conf in nano, a command line text editor. Add the following line:\n[boot]\nsystemd=true\nSave the file with CTRL-O and then quit nano with CTRL-X. Then, type the following line in powershell:\nwsl --shutdown\nand then relaunch WSL (Ubuntu) from the start menu. For those of you running Windows, we will be working exclusively from WSL2 now. If that is not an option, then I highly recommend you set up a virtual machine with Ubuntu using VirtualBox for example, or dual-boot Ubuntu.\n\n\n3.1.2 The Determinate Systems installer\nInstalling (and uninstalling) Nix is quite simple, thanks to the installer from Determinate Systems, a company that provides services and tools built on Nix, and works the same way on Linux (native or WSL2) and macOS.\nDo not use your operating system’s package manager to install Nix. Instead, simply open a terminal and run the following line (on Windows, run this inside WSL, and after the prerequisites listed above):\n{sh parsermd-chunk-1, eval = FALSE} curl --proto '=https' --tlsv1.2 -sSf \\   -L https://install.determinate.systems/nix | \\   sh -s -- install\nJust follow the instructions on screen, and in no time Nix will be available on your machine!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "03-setup.html#configuring-the-cache",
    "href": "03-setup.html#configuring-the-cache",
    "title": "3  Setting Up Your Environment",
    "section": "3.2 Configuring the Cache",
    "text": "3.2 Configuring the Cache\nNext, install the cachix client and configure the rstats-on-nix cache: this will install binary versions of many R packages which will speed up the building process of environments:\nnix-env -iA cachix -f https://cachix.org/api/v1/install\nthen use the cache:\ncachix use rstats-on-nix\nYou only need to do this once per machine you want to use {rix} on. Many thanks to Cachix for sponsoring the rstats-on-nix cache!\nIf you get this warning when trying to install software with Nix:\nwarning: ignoring the client-specified setting 'trusted-public-keys', because it is a restricted setting and you are not a trusted user\nwarning: ignoring untrusted substituter 'https://rstats-on-nix.cachix.org', you are not a trusted user.\nRun `man nix.conf` for more information on the `substituters` configuration option.\nwarning: ignoring the client-specified setting 'trusted-public-keys', because it is a restricted setting and you are not a trusted user\nThen this means that configuration was not successful. You need to add your user to /etc/nix/nix.custom.conf:\nsudo nano /etc/nix/nix.custom.conf\nthen simply add this line in the file:\ntrusted-users = root YOURUSERNAME\nwhere YOURUSERNAME is your current login user name.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "03-setup.html#verifying-installation-with-temporary-shells",
    "href": "03-setup.html#verifying-installation-with-temporary-shells",
    "title": "3  Setting Up Your Environment",
    "section": "3.3 Verifying installation with Temporary Shells",
    "text": "3.3 Verifying installation with Temporary Shells\nYou now have Nix installed; before continuing, let’s see if everything works (close all your terminals and reopen them) by dropping into a temporary shell with a tool you likely have not installed on your machine.\nOpen a terminal and run:\nwhich sl\nyou will likely see something like this:\nwhich: no sl in ....\nnow run this:\nnix-shell -p sl\nand then again:\nwhich sl\nthis time you should see something like:\n/nix/store/cndqpx74312xkrrgp842ifinkd4cg89g-sl-5.05/bin/sl\nThis is the path to the sl binary installed through Nix. The path starts with /nix/store: the Nix store is where all the software installed through Nix is stored. Now type sl and see what happens!\nTemporary shells are quite useful, especially if you want to simply run a command using some tool that you only need infrequently. But this is not how we are going to be using Nix.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "03-setup.html#configuring-your-ide",
    "href": "03-setup.html#configuring-your-ide",
    "title": "3  Setting Up Your Environment",
    "section": "3.4 Configuring your IDE",
    "text": "3.4 Configuring your IDE\n\n3.4.1 Pre-requisites\nWe now need to configure an IDE to use our Nix shells as development environments. You are free to use whatever IDE you want but the instructions below are going to focus on Positron, which is a fork of VS Code geared towards data science. It works well with both Python and R and makes it quite easy to choose the right R or Python interpreter (which you’ll have to do to make sure you’re using the one provided by Nix, see here).\nIf you want to use VS Code proper, you can follow all the instructions here, but you need to install the REditorSupport and the Python extension. You will also need to add the {languageserver} R package to your Nix shells.\nOn Windows, you need to install Positron on Windows, not inside WSL.\nIf you want to use RStudio, you can, but you will need to install it through Nix: an RStudio installed through the usual means for your system is not going to be able to interact with Nix! This is a limitation of RStudio and there is currently no workaround. If you want to use RStudio, just skip the rest of the chapter, as it’s irrelevant to you. Later, when we use {rix} to set up our first Nix development environment, I’ll tell you what to do to use RStudio.\nOther editors work well with Nix too. Emacs users can use the envrc or emacs-direnv packages to automatically load Nix environments. Neovim users can use direnv.nvim. The key is that any editor with direnv support will work with the setup described below.\n\n\n3.4.2 direnv\nOnce Positron is installed, you need to install a piece of software called direnv: direnv will automatically load Nix shells when you open a project that contains a default.nix file in an editor.1 It works on any operating system and many editors support it, including Positron. If you’re using Windows, install direnv in WSL (even though you’ve just installed Positron for Windows). To install direnv run this command:\nnix-env -f '&lt;nixpkgs&gt;' -iA direnv\nThis will install direnv and make it available even outside of Nix shells!\nThen, I highly recommend to install the nix-direnv extension:\nnix-env -f '&lt;nixpkgs&gt;' -iA nix-direnv\nIt is not mandatory to use nix-direnv if you already have direnv, but it’ll make loading environments much faster and seamless.\nFinally, if you haven’t used direnv before, don’t forget this last step to make your terminal detect and load direnv automatically.\nThen, in Positron, install the direnv extension. Finally, add a file called .envrc and simply write the following two lines in it (this .envrc file should be in the same folder as your project’s default.nix):\nuse nix\nmkdir $TMP\nin it. On Windows, remotely connect to WSL first, but on other operating systems, simply open the project’s folder using File &gt; Open Folder... and you will see a pop-up stating direnv: /PATH/TO/PROJECT/.envrc is blocked and a button to allow it. Click Allow and then open an R script. You might get another pop-up asking you to restart the extension, so click Restart. Be aware that at this point, direnv will run nix-shell and so will start building the environment. If that particular environment hasn’t been built and cached yet, it might take some time before Code will be able to interact with it. You might get yet another popup, this time from the R Code extension complaining that R can’t be found. In this case, simply restart Positron and open the project folder again: now it should work every time.\n\n\n3.4.3 In summary and next steps\nFor a new project, simply repeat this process:\n\nGenerate the project’s default.nix file (we will see how in the next chapter);\nBuild it using nix-build;\nCreate an .envrc and write the two lines from above in it;\nOpen the project’s folder in Positron and click allow when prompted;\nRestart the extension and Positron if necessary.\n\nAnother option is to create the .envrc file and write the two required lines, then open a terminal, navigate to the project’s folder, and run direnv allow. Doing this before opening Positron should not prompt you anymore.\nIf you’re on Windows, using Positron like this is particularly interesting, because it allows you to install Positron on Windows as usual, and then you can configure it to interact with a Nix shell, even if it’s running from WSL. This is a very seamless experience.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "03-setup.html#footnotes",
    "href": "03-setup.html#footnotes",
    "title": "3  Setting Up Your Environment",
    "section": "",
    "text": "A default.nix file is a file that contains the specification of our development environments, and is the file that will be automatically generated by {rix}↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Setting Up Your Environment</span>"
    ]
  },
  {
    "objectID": "04-rix.html",
    "href": "04-rix.html",
    "title": "4  Reproducible Development Environments with rix",
    "section": "",
    "text": "4.1 Introduction\nNow that we have Nix installed, and our IDE configured we can actually tackle the reproducibility puzzle.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#introduction",
    "href": "04-rix.html#introduction",
    "title": "4  Reproducible Development Environments with rix",
    "section": "",
    "text": "4.1.1 The Reproducibility Challenge\nReproducibility in research and data science exists on a continuum. At one end, authors might only describe their methods in prose. Moving along the spectrum, they might share code, then data, and finally what we call a computational environment: the complete set of software required to execute an analysis.\nEven when researchers share code and data, they rarely specify the full software stack: the exact version of R, all package versions, and crucially, the system-level dependencies. Yet differences in any of these can lead to divergent results from the same code.\nTools like {renv} address part of the puzzle: they capture R package versions in a lockfile. But {renv} does not manage the R version itself (you need rig for that), and neither handles system libraries. If {sf} requires GDAL 3.0 but your system has 2.4, {renv} can’t help. And if your project uses both R and Python? Now you’re coordinating multiple package managers, each with its own configuration.\n\n\n4.1.2 Component Closures: The Nix Approach\nThis is where Nix shines. Nix deploys component closures: when you install a package, Nix also installs all its dependencies, their dependencies, and so on. Think of it like packing for a trip—traditional package managers assume you’ll find essentials at your destination, while Nix packs everything you need.\nAs the original Nix paper explains:\n\nThe idea is to always deploy component closures: if we deploy a component, then we must also deploy its dependencies, their dependencies, and so on. Since closures are self-contained, they are the units of complete software deployment.\n\nThis means when you install {sf} through Nix, you automatically get the correct versions of GDAL, GEOS, and PROJ—no manual system configuration needed.\n\n\n4.1.3 The Polyglot Challenge\nModern data science is increasingly polyglot. Research shows that data scientists use, on average, nearly two programming languages in their work, with R and Python being the most common combination. Python dominates machine learning, R excels at statistical modelling, and Julia offers high-performance numerics. Projects increasingly combine these strengths.\nThis creates a reproducibility challenge: a project using R, Python, and Quarto requires coordinating multiple package managers. Nix solves this by providing a unified framework for all languages and system tools.\n\n\n4.1.4 Enter rix\nHowever, Nix has a steep learning curve. Its functional programming language can be daunting for researchers focused on their analysis, not system administration.\n{rix} bridges this gap. It’s an R package that generates Nix expressions from intuitive R function calls. You describe what you want, and {rix} figures out how to express it in Nix. The workflow is simple:\n\nDeclare your environment using rix()\nBuild the environment using nix-build\nUse the environment using nix-shell\n\nThis chapter covers everything you need to know to create project-specific, reproducible development environments for your R projects.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#transitioning-to-nix-managed-r",
    "href": "04-rix.html#transitioning-to-nix-managed-r",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.2 Transitioning to Nix-Managed R",
    "text": "4.2 Transitioning to Nix-Managed R\nNow that Nix is installed, I strongly recommend uninstalling any system-wide R installation and removing the packages in your user library (typically found in ~/R on Linux or ~/Library/R on macOS). From this point forward, let Nix handle everything. If you are using Windows, you can keep your Windows-specific R installation, since Nix will not interfere with it (remember, Nix is installed inside WSL and Positron will automatically load Nix environments installed in WSL as well).\nIf you are not ready to take this step, you can continue using your local R library. The {rix} package makes efforts not to interfere with your existing setup, and the .Rprofile it generates helps keep Nix environments isolated. However, for the cleanest experience and to avoid subtle conflicts, I recommend fully committing to Nix.\n\n4.2.1 Bootstrapping {rix} without a local R installation\nBut if R is uninstalled, how do you use {rix} to generate environments? The answer lies in the temporary shells we saw in the previous chapter. Use Nix itself to bootstrap a temporary R session with {rix} available.\nRunning the following line in a terminal will drop you into an interactive R session:\nnix-shell -p R rPackages.rix\nThis gives you a temporary shell with R and {rix} ready to use. From here, you can generate project-specific Nix expressions.\nFor example, navigate to an empty directory for a new project:\nmkdir my-project\ncd my-project\nThen start R:\nR\nLoad {rix} and generate an expression:\nlibrary(rix)\n\nrix(\n  date = \"2025-04-11\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\nThis writes a default.nix file to your project directory. The expression defines a shell with R, {dplyr}, and {ggplot2} as they were on the 11th of April 2025 on CRAN. The ide argument is set to \"none\" because we configured Positron in the previous chapter rather than having Nix manage an IDE.\nI recommend saving this code in a file called gen-env.R in your project directory. If you later need to add packages or change the R version, simply edit gen-env.R, run it to regenerate default.nix, and rebuild with nix-build. This keeps your environment definition readable and versioned alongside your project.\n\n\n\n\n\n\nGetting LLM assistance with {rix}\n\n\n\nIf the {rix} syntax is new to you, remember that you can use pkgctx to generate LLM-ready context (as mentioned in the introduction). The {rix} repository includes a .pkgctx.yaml file you can feed to your LLM to help it understand the package’s API. You can also generate your own context file:\nnix run github:b-rodrigues/pkgctx -- r github:ropensci/rix &gt; rix.pkgctx.yaml\nWith this context, your LLM can help you write correct rix() calls, even if you’ve never used the package before. You can do so for any package hosted on CRAN, GitHub, or local .tar.gz files.\n\n\nYou can now exit this temporary session (type q() in R, then exit in the shell) and build your new environment:\nnix-build\nOnce built, use nix-shell to enter your project’s environment. This workflow of bootstrapping {rix} via a temporary shell, generating a default.nix, and then building it is the pattern you will follow for every new project.\nThis is also the reason why there is no Python version of {rix}: you can bootstrap a Python environment using {rix} in the same way, by only temporarily having the R interpreter available through the Nix shell.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#the-rix-function",
    "href": "04-rix.html#the-rix-function",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.3 The rix() Function",
    "text": "4.3 The rix() Function\nThe rix() function is the heart of the package. It generates a default.nix file—a Nix expression that defines your development environment. Here are its main arguments:\n\nr_ver: the version of R you need (or use date instead)\ndate: a date corresponding to a CRAN snapshot\nr_pkgs: R packages to install from CRAN/Bioconductor\nsystem_pkgs: system tools like quarto or git\ngit_pkgs: R packages to install from GitHub\nlocal_r_pkgs: local .tar.gz packages to install\ntex_pkgs: TexLive packages for literate programming\nide: which IDE to configure (\"rstudio\", \"code\", \"positron\", \"none\"). Since we’ve configured Positron in the previous chapter, you’ll want to set this to \"none\". If you want to use RStudio, then set it to \"rstudio\". This will install RStudio using Nix and make it available from the development shell. If you set it to \"code\" or \"positron\", this will then install VS Code or Positron using Nix.\nproject_path: where to save the default.nix file\noverwrite: whether to overwrite an existing default.nix\n\nLet’s create our first environment. Suppose you need R with {dplyr} and {ggplot2}:\n\nlibrary(rix)\n\nrix(\n  r_ver = \"4.4.2\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  ide = \"rstudio\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nThis generates two files:\n\ndefault.nix: The Nix expression defining your environment\n.Rprofile: Created by rix_init() (called automatically), this file prevents conflicts with any system-installed R packages\n\nThe .Rprofile is important: it ensures that packages from your user library don’t get loaded into the Nix environment, and it redefines install.packages() to throw an error, because you should never install packages that way in a Nix environment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#choosing-an-r-version-or-date",
    "href": "04-rix.html#choosing-an-r-version-or-date",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.4 Choosing an R Version or Date",
    "text": "4.4 Choosing an R Version or Date\nThe r_ver argument (or alternatively date) controls which version of R and which package versions you’ll get. Here’s a summary of the options:\n\n\n\n\n\n\n\n\n\nr_ver / date\nIntended Use\nR Version\nPackage Versions\n\n\n\n\n\"latest-upstream\"\nNew project, versions don’t matter\nCurrent/previous\nUp to 6 months old\n\n\n\"4.4.2\" (or similar)\nReproduce old project or start new\nSpecified version\nUp to 2 months old\n\n\ndate = \"2024-12-14\"\nPrecise CRAN snapshot\nCurrent at that date\nExact versions from date\n\n\n\"bleeding-edge\"\nDevelop against latest CRAN\nAlways current\nAlways current\n\n\n\"frozen-edge\"\nLatest CRAN, manual updates\nCurrent at generation\nCurrent at generation\n\n\n\"r-devel\"\nTest against R development version\nR-devel\nAlways current\n\n\n\nTo see which R versions are available:\n\navailable_r()\n\nTo see which dates are available for snapshotting:\n\navailable_dates()\n\n\n4.4.1 Using dates vs versions\nUsing a specific date is often the best choice for reproducibility. When you specify a date, you get the exact state of CRAN on that day:\n\nrix(\n  date = \"2024-12-14\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nI find that this is often easier and clearer than using an R version. Be careful though, as using a date doesn’t reflect the state of PyPi on that date. So if you also need Python packages, the versions of packages for Python that will be provided are the ones available from nixpkgs on that day (but more on this later).\n\n\n4.4.2 The rstats-on-nix fork\nWhen you use a specific R version or date (rather than \"latest-upstream\"), {rix} uses our rstats-on-nix fork of nixpkgs rather than the upstream repository. This fork:\n\nSnapshots CRAN more frequently\nOffers newer R releases faster than the official channels\nIncludes many fixes, especially for Apple Silicon (even though as time goes by, this is becoming much rarer, because Nix is getting better and more support for Apple Silicon)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#installing-r-packages",
    "href": "04-rix.html#installing-r-packages",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.5 Installing R Packages",
    "text": "4.5 Installing R Packages\n\n4.5.1 From CRAN/Bioconductor\nThe simplest case—just list package names in r_pkgs:\n\nrix(\n  r_ver = \"4.4.2\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\", \"tidyr\", \"readr\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nBoth CRAN and Bioconductor packages can be specified this way.\n\n\n4.5.2 Installing archived versions\nNeed a specific old version of a package? Use the @ syntax:\n\nrix(\n  r_ver = \"4.2.1\",\n  r_pkgs = c(\"dplyr@0.8.0\", \"janitor@1.0.0\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nThis will install {dplyr} version 0.8.0 from the CRAN archives. Note that archived packages are built from source, which may fail for packages requiring compilation. Thus I recommend you use a date on which that specific version of {dplyr} was current.\n\n\n4.5.3 Installing from GitHub\nFor packages on GitHub, use the git_pkgs argument with a list containing the package name, repository URL, and commit hash:\n\nrix(\n  r_ver = \"4.4.2\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  git_pkgs = list(\n    list(\n      package_name = \"housing\",\n      repo_url = \"https://github.com/rap4all/housing/\",\n      commit = \"1c860959310b80e67c41f7bbdc3e84cef00df18e\"\n    )\n  ),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nAlways specify a commit hash, not a branch name. This ensures reproducibility: branch names can change, but commits are immutable.\nIf the R package lives in a subfolder of the repository, append the subfolder to the URL:\n\ngit_pkgs = list(\n  package_name = \"BPCells\",\n  repo_url = \"https://github.com/bnprks/BPCells/r\",  # Note the /r suffix\n  commit = \"16faeade0a26b392637217b0caf5d7017c5bdf9b\"\n)\n\nNote that this will install the package from source, so if it’s a package that requires specific system dependencies, you will need to specify them manually (Nix takes care of this for you for packages that are available through nixpkgs, but not for ad-hoc packages from other sources).\n\n\n4.5.4 Installing local packages\nFor local .tar.gz archives, place them in the same directory as your default.nix and use local_r_pkgs:\n\nrix(\n  r_ver = \"4.3.1\",\n  local_r_pkgs = c(\"mypackage_1.0.0.tar.gz\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nJust like with packages from Git, note that this will install the package from source, so if it’s a package that requires specific system dependencies, you will need to specify them manually (Nix takes care of this for you for packages that are available through nixpkgs, but not for ad-hoc packages from other sources).\n\n\n4.5.5 Why NOT to use install.packages()\nIt’s crucial to understand: never call install.packages() from within a Nix environment. Here’s why:\n\nDeclarative environments: If you install packages imperatively, your default.nix no longer matches your actual environment.\nLeaking packages: Packages installed via install.packages() go to your user library, not the Nix environment. They’ll be visible to all Nix shells, breaking isolation.\nReproducibility: The whole point of Nix is that your environment is fully defined by the default.nix. Ad-hoc installations defeat this.\n\nInstead, add packages to your rix() call and rebuild the environment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#system-tools-and-texlive",
    "href": "04-rix.html#system-tools-and-texlive",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.6 System Tools and TexLive",
    "text": "4.6 System Tools and TexLive\n\n4.6.1 Adding system packages\nNeed command-line tools like Quarto or Git? Add them via system_pkgs:\n\nrix(\n  r_ver = \"latest-upstream\",\n  r_pkgs = c(\"quarto\"),\n  system_pkgs = c(\"quarto\", \"git\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nNote that here we install both the R {quarto} package and the quarto command-line tool—they’re different things!\nTo find available packages, search at search.nixos.org.\n\n\n4.6.2 TexLive for literate programming\nFor PDF output from Quarto, R Markdown, or Sweave, you need TexLive packages:\n\nrix(\n  r_ver = \"latest-upstream\",\n  r_pkgs = c(\"quarto\"),\n  system_pkgs = \"quarto\",\n  tex_pkgs = c(\"amsmath\", \"framed\", \"fvextra\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nThis installs the scheme-small TexLive distribution plus the specified packages.\n\n\n4.6.3 Python and Julia integration\n{rix} can also add Python or Julia to your environment:\n\nrix(\n  date = \"2025-02-17\",\n  r_pkgs = \"ggplot2\",\n  py_conf = list(\n    py_version = \"3.12\",\n    py_pkgs = c(\"polars\", \"great-tables\")\n  ),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nThis creates a polyglot environment with R, Python, and the specified packages for each.\n\n\n4.6.4 Installing Python packages with uv (impure)\nNot all Python packages (or versions of packages) are available through Nix. Unlike CRAN, PyPI doesn’t get automatically mirrored—individual packages must be packaged by volunteers. If a Python package you need isn’t in nixpkgs, you can use uv as an escape hatch.\nThis approach is also useful when collaborating with colleagues who use uv but haven’t adopted Nix yet. uv is 10–100x faster than pip and generates a lock file for improved reproducibility.\nThe idea is to install uv in your shell (but not Python or Python packages through Nix):\n\nrix(\n  date = \"2025-02-17\",\n  r_pkgs = \"ggplot2\",\n  system_pkgs = c(\"uv\"),\n  ide = \"none\",\n  project_path = \".\"\n)\n\nThen use uv from within your shell. We recommend:\n\nSpecify Python packages in a requirements.txt file with explicit versions (e.g., scanpy==1.11.4)\nSet up a shell hook to automatically configure the virtual environment\n\nHere’s a complete example with a shell hook:\n\nrix(\n  date = \"2025-02-17\",\n  r_pkgs = \"ggplot2\",\n  system_pkgs = c(\"uv\"),\n  shell_hook = \"\n    if [ ! -f pyproject.toml ]; then\n      uv init --python 3.13.5\n    fi\n    uv add --requirements requirements.txt\n    alias python='uv run python'\n  \",\n  ide = \"none\",\n  project_path = \".\"\n)\n\nAfter running nix-shell, uv initialises a Python project with the specified version and installs packages from requirements.txt. This happens each time you enter the shell, but uv caches everything so it’s nearly instant after the first run.\n\n4.6.4.1 Troubleshooting wheel issues\nWhen using wheels (pre-compiled Python packages), you may encounter errors like:\nImportError: libstdc++.so.6: cannot open shared object file\nThis happens because wheels expect certain libraries in certain locations. Add this to your shell hook to fix it:\nshellHook = ''\n  export LD_LIBRARY_PATH=\"${pkgs.lib.makeLibraryPath (with pkgs; [ \n    zlib gcc.cc glibc stdenv.cc.cc \n  ])}\":$LD_LIBRARY_PATH\n  # ... rest of your hook\n'';\nIf this seems complicated: yes, it is. This is exactly the kind of problem Nix aims to solve. When possible, prefer Python packages included in nixpkgs.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#building-and-using-environments",
    "href": "04-rix.html#building-and-using-environments",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.7 Building and Using Environments",
    "text": "4.7 Building and Using Environments\n\n4.7.1 Building with nix-build\nOnce you have a default.nix, build the environment:\nnix-build\nThis downloads/builds all required packages and creates a result symlink in your project directory. The result file prevents the environment from being garbage-collected.\n\n\n4.7.2 Entering with nix-shell\nTo use the environment interactively:\nnix-shell\nYou’ll drop into a shell where R and all your packages are available. Type R to start an R session. If you’re using RStudio (and specified ide = \"rstudio\" in the call to rix()) then type rstudio to launch RStudio. If instead you configured Positron, (or any other IDE), open Positron, and open the project folder that contains the default.nix (make sure you also have an .envrc there for direnv to load the environment directly).\nYou can even run scripts directly without entering the shell:\nnix-shell default.nix --run \"Rscript analysis.R\"\nThis is quite useful on CI/CD platforms.\n\n\n4.7.3 Pure shells for complete isolation\nBy default, nix-shell can still see programs installed on your system. This is quite important to understand: building the environment happens in an isolated, hermetic sandbox, but when inside a shell, isolation is more porous and it is possible to use other systems tools. For example, you don’t need to install git inside the Nix development environment: you can just keep using the git executable already available on your system.\nBut it is possible to run the environment with increased isolation:\nnix-shell --pure\nThis hides everything not explicitly included in your environment.\n\n\n4.7.4 Garbage collection\nNix never deletes old packages automatically. To clean up:\n\nDelete the result symlink that will appear in your project’s folder after calling nix-build\nRun nix-store --gc\n\nThis removes all packages that are no longer referenced by any environment.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#converting-renv-projects",
    "href": "04-rix.html#converting-renv-projects",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.8 Converting renv Projects",
    "text": "4.8 Converting renv Projects\nIf you have existing projects using {renv}, the renv2nix() function can help you migrate:\n\nrenv2nix(\n  renv_lock_path = \"path/to/project/renv.lock\",\n  project_path = \"path/to/new_nix_project\"\n)\n\n\n4.8.1 Recommended workflow\n\nCopy the renv.lock file to a new, empty folder\nRun renv2nix() pointing to that folder\nBuild the environment with nix-build\n\nDo not convert in the same folder as the original {renv} project—the generated .Rprofile will conflict with {renv}’s .Rprofile.\n\n\n4.8.2 Caveats\n\nPackage versions may not match exactly due to how Nix handles snapshotting\nIf the renv.lock lists an old R version but recent packages, use the override_r_ver argument to specify a more appropriate R version",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "04-rix.html#summary",
    "href": "04-rix.html#summary",
    "title": "4  Reproducible Development Environments with rix",
    "section": "4.9 Summary",
    "text": "4.9 Summary\nLet’s step back and recap why we have gone through all this trouble.\nTraditional tools like {renv} or Python’s venv only capture part of the reproducibility puzzle. They track package versions but not the language version itself, nor system-level dependencies like GDAL or Java. This means your project can still break on a different machine, or even on your own machine after a system update.\nNix solves this by managing everything: R, Python, all packages, and all system dependencies. When you define an environment with {rix}, you get a complete, self-contained specification that anyone can use to recreate the exact same environment, on any machine, at any point in the future.\n\n4.9.1 Quick Reference: Starting a New Project\nHere is the workflow you will follow for every new project:\n\nCreate an empty folder for your project:\nmkdir my-project\ncd my-project\nWrite a gen-env.R file with your environment definition:\nlibrary(rix)\n\nrix(\n  date = \"2025-04-11\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  ide = \"none\",  # or \"rstudio\" if you prefer\n  project_path = \".\",\n  overwrite = TRUE\n)\nAdd a .envrc file (only needed for Positron or VS Code, skip if using RStudio):\nuse nix\nmkdir $TMP\nBootstrap {rix} and generate default.nix:\nnix-shell -p R rPackages.rix\nThen in R:\nsource(\"gen-env.R\")\nExit R with q() and the shell with exit.\nBuild the environment:\nnix-build\n# you could also run `direnv allow` to allow direnv to load the environment\n# automatically and build the environment on first use\nStart working: Open the folder in Positron (which will load the environment via direnv), or run nix-shell followed by rstudio if you set ide = \"rstudio\".\n\nThat’s it. Your project is now reproducible. Anyone with Nix installed can clone your repository, run nix-build, and get the exact same environment you have.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Reproducible Development Environments with rix</span>"
    ]
  },
  {
    "objectID": "05-fp.html",
    "href": "05-fp.html",
    "title": "5  Functional Programming",
    "section": "",
    "text": "5.1 Introduction: From Scripts to Functions\nIn this chapter, we will see why functional programming is crucial for reproducible, testable, and collaborative data science. We will compare how to write self-contained, “pure” functions in both R and Python, and how to use functional concepts like map, filter, and reduce to replace error-prone loops. Finally, we will discuss how writing functions makes your code easier to review, debug, and even generate with LLMs.\nIn the previous chapter, we learned how to create reproducible development environments with {rix}. We can now ensure everyone has the exact same tools (R, Python, system libraries) to run our code.\nWe could have stopped there: after all, we have now reproducible environments, as many as we need for each of our projects. But having the right tools is only half the battle. Now we turn to writing reproducible code itself. A common way to start a data analysis is by writing a script: a sequence of commands executed from top to bottom.\n# R script example\nlibrary(dplyr)\ndata(mtcars)\nheavy_cars &lt;- filter(mtcars, wt &gt; 4)\nmean_mpg_heavy &lt;- mean(heavy_cars$mpg)\nprint(mean_mpg_heavy)\n# Python script example\nimport pandas as pd\nmtcars = pd.read_csv(\"mtcars.csv\")\nheavy_cars = mtcars[mtcars['wt'] &gt; 4]\nmean_mpg_heavy = heavy_cars['mpg'].mean()\nprint(mean_mpg_heavy)\nThis works, but it has a hidden, dangerous property: state. The script relies on an implicit execution order and on variables like heavy_cars existing in the global environment at the right moment. This makes the code surprisingly fragile: rename a variable, reorder a few lines, or run a subset of the script, and things silently break, or worse, silently produce wrong answers.\nOf course, this is a four-line toy example. Now imagine the reality of production data science: dozens of scripts, each hundreds of lines long, maintained by multiple people over months or years. Variables are reused, overwritten, and passed around in an unspoken contract of “run these files in this order.” Debugging becomes archaeology: you must reconstruct not just what the code says, but what the global environment was when it ran. Testing is nearly impossible, because there is no clear boundary between inputs and outputs. And onboarding a new team member? They inherit a minefield.\nThe root problem is implicit dependencies. When code depends on global state, the dependencies are invisible. A function call like mean(heavy_cars$mpg) looks self-contained, but it secretly relies on a hidden precondition: that heavy_cars exists, has the expected columns, and was computed correctly upstream. If any step in that chain fails silently, the error propagates, and may only surface as a mysterious wrong number in a final report, weeks later.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#introduction-from-scripts-to-functions",
    "href": "05-fp.html#introduction-from-scripts-to-functions",
    "title": "5  Functional Programming",
    "section": "",
    "text": "5.1.1 The Notebook Problem\nIf scripting with state is a crack in the foundation of reproducibility, then using computational notebooks is a gaping hole. I will not make many friends in the Python community with the following paragraphs, but that’s because only truth hurts.\nNotebooks like Jupyter introduce an even more insidious form of state: the cell execution order. You can execute cells out of order, meaning the visual layout of your code has no relation to how it actually ran. This is a recipe for non-reproducible results and a primary cause of the “it worked yesterday, why is it broken today?” problem.\nIn a famous talk from JupyterCon 2018, Joel Grus (a research engineer at the Allen Institute for AI) played the contrarian at a conference dedicated to the very tool he was criticising.1 His central thesis is that data science code should follow software engineering best practices, and Jupyter notebooks actively discourage those practices.\nHis biggest complaint is hidden state and out-of-order execution. The state of the variables depends on the execution history, not the order of the code on the screen. You can delete a cell that defined a variable, but that variable still exists in memory. This leads to unreproducible results where the code works right now (because of hidden state in memory) but will fail if you restart the kernel and run top-to-bottom. Worse, it confuses beginners who do not understand why their code works one minute and breaks the next.\nNotebooks also discourage modular code. Because it is difficult to import code from one notebook into another, users tend to write massive, monolithic scripts, copy and paste the same code blocks into multiple notebooks, and avoid creating functions or modules that can be tested and reused.\nThen there is the “works on my machine” problem. Notebooks often lack clear dependency specifications, users hardcode file paths that only exist on their specific computer, and reusing someone else’s work usually involves manually copying and pasting cells, which is error-prone.\nGrus also argues that notebooks have poor tooling compared to IDEs. Actual text editors provide linting (identifying stylistic errors or unused variables), type checking, superior autocomplete, and the ability to run unit tests. All of these are difficult or impossible in notebooks.\nFinally, version control is a nightmare. Notebooks are JSON files. If two people edit a notebook and try to merge their changes in Git, the diffs are unreadable blocks of JSON metadata, and merge conflicts are incredibly difficult to resolve. This encourages workflows where people email files back and forth rather than using proper version control.\nWhat does Grus suggest instead? Write code in modules (.py or .R files) using a proper editor, write unit tests to ensure the code works, and use notebooks only for the final step: importing those modules to visualise the data or present the results. Ideally, the notebook should contain very little logic and mostly just function calls.\nThis is exactly the approach we will take in this book.\n\n\n5.1.2 The Functional Solution\nThe solution is to embrace a paradigm that minimises state: Functional Programming (FP). Instead of a linear script, we structure our code as a collection of self-contained, predictable functions.\nThe power of FP comes from the concept of purity, borrowed from mathematics. A mathematical function has a beautiful property: for a given input, it always returns the same output. sqrt(4) is always 2. Its result doesn’t depend on what you calculated before or on a random internet connection.\nOur Nix environments handle the “right library” problem; purity handles the “right logic” problem. Our goal is to write our analysis code with this same level of rock-solid predictability.\n\n\n5.1.3 FP vs OOP: Transformations vs Actors\nTo appreciate what FP brings, it helps to contrast it with Object-Oriented Programming (OOP), arguably the dominant paradigm in many software systems.\nOOP organises computation around who does what: a network of objects communicating with each other and managing their own internal state. You send a message to an object, asking it to perform an action, without needing to know how it works internally.\nFunctional programming, by contrast, organises computation around how data changes. It replaces a network of interacting objects with a flow of transformations: data goes in, data comes out, and nothing else changes in the process.\nThis shift is especially powerful in data science:\n\nAnalyses are naturally expressed as pipelines of transformations (cleaning, filtering, aggregating, modelling)\nPure functions make results reproducible: same inputs always yield same outputs\nImmutability prevents accidental side effects on shared data\nBecause transformations can be composed, tested, and reused independently, FP encourages modular, maintainable analysis code\n\nThere is also a structural reason why FP fits data science better than OOP. OOP excels when you have many different types of objects, each with a small set of methods. A graphical user interface, for example, has buttons, menus, windows, and dialogs, each responding to a few actions like click() or resize(). But data science is the opposite: we typically work with a small number of data structures (data frames, arrays, models) and apply a large number of operations to them (filtering, grouping, joining, summarising, plotting). FP handles this case naturally: adding a new function is trivial, while OOP would require modifying every class.\n\n\n5.1.4 Why Does This Matter for Data Science?\nAdopting a functional style brings massive benefits:\n\nUnit Testing is Now Possible: You can’t easily test a 200-line script. But you can easily test a small function that does one thing.\nCode Review is Easier: A Pull Request that just adds or modifies a single function is simple for your collaborators to understand and approve.\nWorking with LLMs is More Effective: It’s incredibly effective to ask, “Write a Python function that takes a pandas DataFrame and a column name, and returns the mean of that column, handling missing values. Also, write three pytest unit tests for it.”\nReadability: Well-named functions are self-documenting:\n\nstarwars %&gt;%\n  group_by(species) %&gt;%\n  summarize(mean_height = mean(height))\n\nis instantly understandable. The equivalent for loop is a puzzle.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#purity-and-side-effects",
    "href": "05-fp.html#purity-and-side-effects",
    "title": "5  Functional Programming",
    "section": "5.2 Purity and Side Effects",
    "text": "5.2 Purity and Side Effects\nA pure function has two rules:\n\nIt only depends on its inputs. It doesn’t use any “global” variables defined outside the function.\nIt doesn’t change anything outside of its own scope. It doesn’t modify a global variable or write a file to disk. This is called having “no side effects.”\n\nConsider this “impure” function in Python:\n\n# IMPURE: Relies on a global variable\ndiscount_rate = 0.10\n\ndef calculate_discounted_price(price):\n    return price * (1 - discount_rate)  # What if discount_rate changes?\n\nprint(calculate_discounted_price(100))\n\n90.0\n\ndiscount_rate = 0.20  # Someone changes the state\nprint(calculate_discounted_price(100))\n\n80.0\n\n\nThe pure version passes all its dependencies as arguments:\n\n# PURE: All inputs are explicit arguments\ndef calculate_discounted_price_pure(price, rate):\n    return price * (1 - rate)\n\nprint(calculate_discounted_price_pure(100, 0.10))\n\n90.0\n\nprint(calculate_discounted_price_pure(100, 0.20))\n\n80.0\n\n\nNow the function is predictable and self-contained.\n\n5.2.1 Handling “Impure” Operations like Randomness\nSome operations, like generating random numbers, are inherently impure. Each time you run rnorm(10) or numpy.random.rand(10), you get a different result.\nThe functional approach is not to avoid this, but to control it by making the source of impurity (the random seed) an explicit input.\nIn R, the {withr} package helps create a temporary, controlled context:\n\nlibrary(withr)\n\n# This function is now pure! For a given seed, the output is always the same.\npure_rnorm &lt;- function(n, seed) {\n  with_seed(seed, {\n    rnorm(n)\n  })\n}\n\npure_rnorm(n = 5, seed = 123)\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\npure_rnorm(n = 5, seed = 123)  # Same result!\n\n[1] -0.56047565 -0.23017749  1.55870831  0.07050839  0.12928774\n\n\nIn Python, numpy provides an object-oriented way to handle this:\n\nimport numpy as np\n\n# Create a random number generator instance with a seed\nrng = np.random.default_rng(seed=123)\nprint(rng.standard_normal(5))\n\n[-0.98912135 -0.36778665  1.28792526  0.19397442  0.9202309 ]\n\n# If we re-create the same generator, we get the same numbers\nrng2 = np.random.default_rng(seed=123)\nprint(rng2.standard_normal(5))\n\n[-0.98912135 -0.36778665  1.28792526  0.19397442  0.9202309 ]\n\n\nThe key is the same: the “state” (the seed) is explicitly managed, not hidden globally.\n\n\n5.2.2 The OOP Caveat in Python\nThis introduces a concept from OOP: the rng variable is an object that bundles together data (its internal seed state) and methods (.standard_normal()). This is encapsulation.\nThis is a double-edged sword for reproducibility. The rng object is now a stateful entity. If we called rng.standard_normal(5) a second time, it would produce different numbers because its internal state was mutated.\nCore Python libraries like pandas, scikit-learn, and matplotlib are fundamentally object-oriented. Our guiding principle must be:\n\nUse functions for the flow and logic of your analysis, and treat objects from libraries as values that are passed between these functions.\n\nAvoid building your own complex classes with hidden state for your data pipeline. A pipeline composed of functions (df2 = clean_data(df1); df3 = analyze_data(df2)) is almost always more transparent than an OOP one (pipeline.load(); pipeline.clean(); pipeline.analyze(), where pipeline is an object that keeps mutating after each call of one of its methods).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#functions-a-refresher-and-beyond",
    "href": "05-fp.html#functions-a-refresher-and-beyond",
    "title": "5  Functional Programming",
    "section": "5.3 Functions: A Refresher and Beyond",
    "text": "5.3 Functions: A Refresher and Beyond\nYou likely already know how to write functions in R and Python. This section serves as a quick refresher, but also introduces some concepts you may not have encountered: higher-order functions, closures, and decorators.\n\n5.3.1 The Basics\nIn R, functions are first-class citizens. You assign them to variables and pass them around like any other value:\n\ncalculate_ci &lt;- function(x, level = 0.95) {\n  se &lt;- sd(x, na.rm = TRUE) / sqrt(length(x))\n  mean_val &lt;- mean(x, na.rm = TRUE)\n  alpha &lt;- 1 - level\n  lower &lt;- mean_val - qnorm(1 - alpha/2) * se\n  upper &lt;- mean_val + qnorm(1 - alpha/2) * se\n  c(mean = mean_val, lower = lower, upper = upper)\n}\n\nIn Python, the def keyword defines functions. Type hints are recommended:\n\nimport statistics\nimport scipy.stats as stats\n\ndef calculate_ci(x: list[float], level: float = 0.95) -&gt; dict:\n    \"\"\"Calculate confidence interval for a list of numbers.\"\"\"\n    n = len(x)\n    mean_val = statistics.mean(x)\n    se = statistics.stdev(x) / (n ** 0.5)\n    alpha = 1 - level\n    z = stats.norm.ppf(1 - alpha / 2)\n    return {\"mean\": mean_val, \"lower\": mean_val - z * se, \"upper\": mean_val + z * se}\n\n\n\n\n\n\n\nImports and Language Design\n\n\n\nNotice that the Python version requires importing statistics and scipy.stats for basic statistical operations. This is a consequence of Python being a general-purpose language: statistical functions are not built in, so we must import libraries that provide them.\nR, by contrast, is a language designed for statistics. Functions like mean(), sd(), and qnorm() are available out of the box. When you do need a function from an external package, R offers the :: notation (e.g., dplyr::filter(df, x &gt; 0)) to call a single function without loading the entire package into your namespace.\nBut this raises a question: if you have many functions spread across multiple files, how do you manage which imports or library() calls each file needs? This is exactly what Joel Grus advocates: write your code in proper modules (.py or .R files), not notebooks, and structure them as packages. When you do, each module declares its own dependencies, and the package manager ensures everything is available. We will explore packaging in detail later in this book.\n\n\n\n\n5.3.2 Higher-Order Functions\nA higher-order function is a function that takes another function as an argument, returns a function, or both. This is the foundation of functional programming.\nYou have already seen examples: map(), filter(), and reduce() are all higher-order functions because they take a function as their first argument.\nHere is a simple example in R:\n\napply_twice &lt;- function(f, x) {\n f(f(x))\n}\n\napply_twice(sqrt, 16)  # sqrt(sqrt(16)) = sqrt(4) = 2\n\n[1] 2\n\n\nAnd in Python:\n\ndef apply_twice(f, x):\n    return f(f(x))\n\napply_twice(lambda x: x ** 2, 2)  # (2^2)^2 = 16\n\n16\n\n\n\n\n5.3.3 Closures: Functions That Remember\nA closure is a function that “remembers” variables from its enclosing scope, even after that scope has finished executing. This is useful for creating specialised functions. These are sometimes called function factories.\nIn R:\n\nmake_power &lt;- function(n) {\n  function(x) x^n\n}\n\nsquare &lt;- make_power(2)\ncube &lt;- make_power(3)\n\nsquare(4)  # 16\n\n[1] 16\n\ncube(4)    # 64\n\n[1] 64\n\n\nIn Python:\n\ndef make_power(n):\n    def power(x):\n        return x ** n\n    return power\n\nsquare = make_power(2)\ncube = make_power(3)\n\nsquare(4)  # 16\n\n16\n\ncube(4)    # 64\n\n64\n\n\nThe inner function “closes over” the variable n, preserving its value.\n\n\n5.3.4 Decorators (Python)\nPython has a special syntax for a common use of higher-order functions: decorators. A decorator wraps a function to extend its behaviour without modifying its code.\n\nimport time\n\ndef timer(func):\n    \"\"\"A decorator that prints how long a function takes to run.\"\"\"\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(f\"{func.__name__} took {end - start:.4f} seconds\")\n        return result\n    return wrapper\n\n@timer\ndef slow_sum(n):\n    return sum(range(n))\n\nslow_sum(1_000_000)\n\nslow_sum took 0.0221 seconds\n499999500000\n\n\nThe @timer syntax is equivalent to slow_sum = timer(slow_sum). Decorators are widely used in Python frameworks (Flask, FastAPI, pytest) for logging, authentication, and caching.\nIf you find decorators elegant, you will appreciate the chapter on monads later in this book. Monads take the idea of wrapping and chaining functions further, providing a principled way to handle errors, missing values, and side effects in a purely functional style.\nR does not have built-in decorator syntax, but you can achieve the same effect with higher-order functions:\n\ntimer &lt;- function(f) {\n  function(...) {\n    start &lt;- Sys.time()\n    result &lt;- f(...)\n    end &lt;- Sys.time()\n    message(sprintf(\"Elapsed: %.4f seconds\", end - start))\n    result\n  }\n}\n\nslow_sum &lt;- timer(function(n) sum(seq_len(n)))\nslow_sum(1e6)\n\nElapsed: 0.0000 seconds\n\n\n[1] 500000500000\n\n\n\n\n5.3.5 Tidy Evaluation in R\nFor data analysis, you will often want functions that work with column names. The {dplyr} package uses “tidy evaluation” with the { } (curly-curly) syntax:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsummarise_variable &lt;- function(data, var) {\n  data %&gt;%\n    summarise(\n      n = n(),\n      mean = mean({{ var }}, na.rm = TRUE),\n      sd = sd({{ var }}, na.rm = TRUE)\n    )\n}\n\nstarwars %&gt;%\n  group_by(species) %&gt;%\n  summarise_variable(height)\n\n# A tibble: 38 × 4\n   species       n  mean    sd\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Aleena        1   79   NA  \n 2 Besalisk      1  198   NA  \n 3 Cerean        1  198   NA  \n 4 Chagrian      1  196   NA  \n 5 Clawdite      1  168   NA  \n 6 Droid         6  131.  49.1\n 7 Dug           1  112   NA  \n 8 Ewok          1   88   NA  \n 9 Geonosian     1  183   NA  \n10 Gungan        3  209.  14.2\n# ℹ 28 more rows\n\n\nThe { var } tells dplyr to treat var as a column name rather than a literal variable.\nPython has no equivalent to tidy evaluation. In pandas, column names must always be passed as strings:\n\ndef summarise_variable(df, var):\n    return df[[var]].agg(['count', 'mean', 'std'])\n\nsummarise_variable(starwars_py, 'height')\n\nThis works, but there are trade-offs. With tidy evaluation, you write unquoted column names, which means your editor can provide autocomplete suggestions if it knows the data frame’s structure. With strings, you are on your own. More importantly, writing programmatic functions that accept column names as arguments is more cumbersome: you end up passing strings around and using methods like .loc[] or .agg() with string keys, rather than the natural “column as variable” style that {dplyr} enables.\n\n\n5.3.6 Anonymous Functions and Lambdas\nSometimes you need a quick, throwaway function that does not deserve a name. Both R and Python support anonymous functions.\nIn Python, the lambda keyword creates a small function in a single expression:\n\nsquares = list(map(lambda x: x ** 2, [1, 2, 3, 4]))\nsquares\n\n[1, 4, 9, 16]\n\n\nIn R, the base syntax is verbose, but {purrr} introduced the formula shorthand using ~ and .x:\n\nlibrary(purrr)\n\n# Full anonymous function\nmap_dbl(1:4, function(x) x^2)\n\n[1]  1  4  9 16\n\n# Formula shorthand (purrr style)\nmap_dbl(1:4, ~ .x^2)\n\n[1]  1  4  9 16\n\n\nR 4.1 also introduced a shorter base syntax with \\(x):\n\n# Base R shorthand (R &gt;= 4.1)\nsapply(1:4, \\(x) x^2)\n\n[1]  1  4  9 16\n\n\nUse anonymous functions when the logic is simple and a full function definition would be overkill.\n\n\n5.3.7 Partial Application\nPartial application means fixing some arguments of a function to create a more specialised version. This is closely related to function factories but works with existing functions rather than defining new ones.\nIn R, use purrr::partial():\n\nlibrary(purrr)\n\n# Create a function that always rounds to 2 decimal places\nround2 &lt;- partial(round, digits = 2)\n\nround2(3.14159)  # 3.14\n\n[1] 3.14\n\nround2(2.71828)  # 2.72\n\n[1] 2.72\n\n\nIn Python, use functools.partial():\n\nfrom functools import partial\n\n# Create a function that always rounds to 2 decimal places\nround2 = partial(round, ndigits=2)\n\nround2(3.14159)  # 3.14\n\n3.14\n\nround2(2.71828)  # 2.72\n\n2.72\n\n\nPartial application is useful for creating callbacks, simplifying repetitive code, and making functions fit the signature expected by map() or similar.\n\n\n5.3.8 Immutability: Data That Does Not Change\nA core principle of functional programming is immutability: once data is created, it is never modified in place. Instead, transformations produce new copies of the data.\nR has copy-on-modify semantics. When you “modify” a data frame, R actually creates a new copy:\n\ndf &lt;- data.frame(x = 1:3)\ndf2 &lt;- df\ndf2$x[1] &lt;- 99\ndf$x[1]  # Still 1, df was not mutated\n\n[1] 1\n\n\nPython, by contrast, has mutable defaults, which can surprise newcomers:\n\nimport pandas as pd\n\ndf = pd.DataFrame({\"x\": [1, 2, 3]})\ndf2 = df  # This is a reference, not a copy!\ndf2.loc[0, \"x\"] = 99\ndf.loc[0, \"x\"]  # Now 99, df was mutated!\n\nnp.int64(99)\n\n\n# To avoid this, explicitly copy:\ndf2 = df.copy()\n\nImmutability prevents entire classes of bugs where one part of your code unexpectedly modifies data used elsewhere. When using Python, be explicit about copying when you need independent data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#the-functional-toolkit-map-filter-and-reduce",
    "href": "05-fp.html#the-functional-toolkit-map-filter-and-reduce",
    "title": "5  Functional Programming",
    "section": "5.4 The Functional Toolkit: Map, Filter, and Reduce",
    "text": "5.4 The Functional Toolkit: Map, Filter, and Reduce\nMost for loops can be replaced by one of three core functional concepts: mapping, filtering, or reducing. These are “higher-order functions”: functions that take other functions as arguments.\n\n5.4.1 1. Mapping: Applying a Function to Each Element\nThe pattern: You have a list of things, and you want to perform the same action on each element, producing a new list of the same length.\n\n5.4.1.1 In R with purrr::map()\nThe {purrr} package is the gold standard for functional programming in R:\n\nmap(): Always returns a list\nmap_dbl(): Returns a vector of doubles (numeric)\nmap_chr(): Returns a vector of characters (strings)\nmap_lgl(): Returns a vector of logicals (booleans)\n\n\nlibrary(purrr)\n\n# The classic for-loop way (verbose)\nmeans_loop &lt;- vector(\"double\", ncol(mtcars))\nfor (i in seq_along(mtcars)) {\n  means_loop[[i]] &lt;- mean(mtcars[[i]], na.rm = TRUE)\n}\n\n# The functional way with map_dbl()\nmeans_functional &lt;- map_dbl(mtcars, mean, na.rm = TRUE)\n\nThe map() version is not just shorter; it’s safer. You can’t make an off-by-one error.\n\n\n5.4.1.2 In Python with List Comprehensions\nPython’s most idiomatic tool for mapping is the list comprehension:\n\nnumbers = [1, 2, 3, 4, 5]\nsquares = [n**2 for n in numbers]\n# &gt; [1, 4, 9, 16, 25]\n\nPython also has a built-in map() function:\n\ndef to_upper_case(s: str) -&gt; str:\n    return s.upper()\n\nwords = [\"hello\", \"world\"]\nupper_words = list(map(to_upper_case, words))\n# &gt; ['HELLO', 'WORLD']\n\n\n\n\n5.4.2 2. Filtering: Keeping Elements That Match a Condition\nThe pattern: You have a list of things, and you want to keep only the elements that satisfy a certain condition (don’t confuse this with filtering rows of a data frame).\n\n5.4.2.1 In R with purrr::keep()\n\ndf1 &lt;- data.frame(x = 1:50)\ndf2 &lt;- data.frame(x = 1:200)\ndf3 &lt;- data.frame(x = 1:75)\nlist_of_dfs &lt;- list(a = df1, b = df2, c = df3)\n\n# Keep only data frames with more than 100 rows\nlarge_dfs &lt;- keep(list_of_dfs, ~ nrow(.x) &gt; 100)\n\n\n\n5.4.2.2 In Python with List Comprehensions\nList comprehensions have a built-in if clause:\n\nnumbers = [1, 10, 5, 20, 15, 30]\nlarge_numbers = [n for n in numbers if n &gt; 10]\n# &gt; [20, 15, 30]\n\n\n\n\n5.4.3 3. Reducing: Combining All Elements into a Single Value\nThe pattern: You have a list of things, and you want to iteratively combine them into a single summary value.\n\n5.4.3.1 In R with purrr::reduce()\n\n# Sum all elements\ntotal_sum &lt;- reduce(c(1, 2, 3, 4, 5), `+`)\n\n# Find common columns across multiple data frames\nlist_of_colnames &lt;- map(list_of_dfs, names)\ncommon_cols &lt;- reduce(list_of_colnames, intersect)\n\n\n\n5.4.3.2 In Python with functools.reduce\n\nfrom functools import reduce\nimport operator\n\nnumbers = [1, 2, 3, 4, 5]\ntotal_sum = reduce(operator.add, numbers)\n# &gt; 15",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#the-power-of-composition",
    "href": "05-fp.html#the-power-of-composition",
    "title": "5  Functional Programming",
    "section": "5.5 The Power of Composition",
    "text": "5.5 The Power of Composition\nThe final, beautiful consequence of a functional style is composition. You can chain functions together to build complex workflows from simple, reusable parts.\nThis R code is a sequence of function compositions:\n\nstarwars %&gt;%\n  filter(!is.na(mass)) %&gt;%\n  select(species, sex, mass) %&gt;%\n  group_by(sex, species) %&gt;%\n  summarise(mean_mass = mean(mass), .groups = \"drop\")\n\nMethod chaining provides something similar to function composition:\n\n(starwars_py\n .dropna(subset=['mass'])\n .filter(items=['species', 'sex', 'mass'])\n .groupby(['sex', 'species'])\n ['mass'].mean()\n .reset_index()\n)\n\nEach step is a function that takes a data frame and returns a new, transformed data frame. By combining map, filter, and reduce with this compositional style, you can express complex data manipulation pipelines without writing a single for loop.\n\n5.5.1 Composition in Python\nMethod chaining in pandas is elegant but limited to the methods defined for DataFrame objects. R’s pipe operators (|&gt; and %&gt;%) are more flexible because functions are not strictly owned by objects, and they can be more easily combined.\nThis reflects the languages’ different philosophies:\n\nR’s lineage traces back to Scheme (a Lisp dialect), making functional composition natural\nPython was designed as an imperative, object-oriented language. In fact, Guido van Rossum, Python’s creator, once proposed removing map(), filter(), and reduce() from the language entirely.2 The community pushed back, but functional programming remains a second-class citizen in Python’s design.\n\nR is fundamentally a functional language that acquired OOP features, while Python is an OOP language with functional capabilities that got almost entirely removed from the language. I think that this the reason “Python feels weird” to R programmers and vice-versa.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#handling-errors-functionally",
    "href": "05-fp.html#handling-errors-functionally",
    "title": "5  Functional Programming",
    "section": "5.6 Handling Errors Functionally",
    "text": "5.6 Handling Errors Functionally\nWhat happens when a function in your pipeline fails? In imperative code, you might wrap everything in try/catch blocks. Functional programming offers a cleaner approach: functions that capture errors rather than throw them.\nBut first, it is worth noting that throwing an error is inherently impure. When a function raises an exception, it does not return a value; instead, it transfers control flow to some unknown handler elsewhere in the program. This is a side effect. A pure function should always return a value, even when something goes wrong. The functional solution is to return a value that represents failure, rather than throwing an exception that breaks the normal flow.\n\n5.6.1 In R with purrr::safely() and purrr::possibly()\nThe {purrr} package provides wrappers that turn error-prone functions into safe ones:\n\nlibrary(purrr)\n\n# A function that might fail\nrisky_log &lt;- function(x) {\n  if (x &lt;= 0) stop(\"x must be positive\")\n  log(x)\n}\n\n# safely() returns a list with $result and $error\nsafe_log &lt;- safely(risky_log)\nsafe_log(10)   # list(result = 2.302585, error = NULL)\n\n$result\n[1] 2.302585\n\n$error\nNULL\n\nsafe_log(-1)   # list(result = NULL, error = &lt;error&gt;)\n\n$result\nNULL\n\n$error\n&lt;simpleError in .f(...): x must be positive&gt;\n\n# possibly() returns a default value on error\nmaybe_log &lt;- possibly(risky_log, otherwise = NA)\nmap_dbl(c(10, -1, 5), maybe_log)  # c(2.30, NA, 1.61)\n\n[1] 2.302585       NA 1.609438\n\n\nThis lets your pipeline continue even when some elements fail, and you can inspect failures afterwards.\n\n\n5.6.2 In Python with Decorators\nWe saw decorators earlier as a way to wrap functions with extra behaviour. We can use the same pattern to capture errors:\n\nimport math\nfrom functools import wraps\n\ndef maybe(default=None):\n    \"\"\"A decorator that catches exceptions and returns a default value.\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception:\n                return default\n        return wrapper\n    return decorator\n\n@maybe(default=None)\ndef safe_log(x):\n    if x &lt;= 0:\n        raise ValueError(\"x must be positive\")\n    return math.log(x)\n\nresults = [safe_log(x) for x in [10, -1, 5]]\n\nThis is cleaner than scattering try/except blocks throughout your code, and the @maybe decorator makes the error-handling strategy explicit.\n\n\n5.6.3 The Limits of This Approach\nWhile safely() and decorators help, they are still working around the fundamental impurity of exceptions. The returned None or NA values can propagate silently, causing confusing errors downstream. For a more principled approach, you need monads: data structures that explicitly represent success or failure and force you to handle both cases. We will explore this in the chapter on monads, where we introduce {chronicler} for R and talvez for Python.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#when-not-to-use-functional-programming",
    "href": "05-fp.html#when-not-to-use-functional-programming",
    "title": "5  Functional Programming",
    "section": "5.7 When NOT to Use Functional Programming",
    "text": "5.7 When NOT to Use Functional Programming\nFunctional programming is powerful, but it is not always the best choice. Here are situations where imperative or object-oriented code may be clearer:\n\nComplex stateful algorithms: Some algorithms (like graph traversals or simulations) naturally require mutable state. Forcing them into a functional style can make the code harder to read.\nPerformance-critical inner loops: Functional abstractions like map() introduce overhead. In tight loops where microseconds matter, a simple for loop may be faster.\n\nThe goal is not functional purity for its own sake, but clarity and correctness. Use functional techniques where they help, and step back to simpler approaches when they do not. Thankfully, LLMs here are quite useful again, as you can use them if you need to write complex imperative code with loops.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#summary",
    "href": "05-fp.html#summary",
    "title": "5  Functional Programming",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nThis chapter has laid the groundwork for writing reproducible code by embracing Functional Programming.\nKey takeaways:\n\nPure functions guarantee the same output for the same input, with no hidden dependencies on global state\nMake impure operations (like randomness) explicit by controlling the seed\nReplace error-prone for loops with map, filter, and reduce\nUse composition to build complex pipelines from simple, reusable functions\nIn Python, treat stateful library objects as values passed between pure functions\n\nUnderstanding the distinction between R’s functional heritage and Python’s OOP nature is key to becoming an effective data scientist in either language. By mastering the functional paradigm, you’re building a foundation for code that is robust, easy to review, simple to debug, and truly reproducible.\nIn the next chapter, we’ll put these principles into practice with {rixpress}, a package that leverages functional composition and Nix to build fully reproducible, polyglot analytical pipelines.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "05-fp.html#footnotes",
    "href": "05-fp.html#footnotes",
    "title": "5  Functional Programming",
    "section": "",
    "text": "I don’t like notebooks↩︎\nThe fate of reduce() in Python 3000↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Programming</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html",
    "href": "06-rixpress.html",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "",
    "text": "6.1 Introduction: From Scripts and Notebooks to Pipelines\nSo far, we have learned about reproducible development environments with Nix and {rix}. We can now create project-specific environments with precise versions of R, Python, and all dependencies. But there’s one more piece to the puzzle: orchestration.\nHow do we take our collection of functions and data files and run them in the correct order to produce our final data product? This problem of managing computational workflows is not new, and a whole category of build automation tools has been created to solve it.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#introduction-from-scripts-and-notebooks-to-pipelines",
    "href": "06-rixpress.html#introduction-from-scripts-and-notebooks-to-pipelines",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "",
    "text": "6.1.1 The Evolution of Build Automation\nThe original solution, dating back to the 1970s, is make. Created by Stuart Feldman at Bell Labs in 1976, make reads a Makefile that describes the dependency graph of a project. If you change the code that generates plot.png, make is smart enough to only re-run the steps needed to rebuild the plot and the final report.\nThe strength of these tools is their language-agnosticism, but their weaknesses are twofold:\n\nFile-centric: You must manually handle all I/O. Your first script saves data.csv, your second loads it. This adds boilerplate and creates surfaces for error.\nEnvironment-agnostic: They track files but know nothing about the software environment needed to create those files.\n\nThis is where R’s {targets} package shines. It tracks dependencies between R objects directly, automatically handling serialisation. But {targets} operates within a single R session; for polyglot pipelines, you must manually coordinate via {reticulate}.\n\n\n6.1.2 The Separation Problem\nAll these tools (from make to {targets} to Airflow) separate workflow management from environment management. You use one tool to run the pipeline and another (Docker, {renv}) to set up the software.\nThis separation creates friction. Running {targets} inside Docker ensures reproducibility, but forces the entire pipeline into one monolithic environment. What if your Python step requires TensorFlow 2.15 but your R step needs reticulate with Python 3.9? You’re stuck.\n\n\n6.1.3 The Imperative Approach: Make + Docker\nTo illustrate this, consider the traditional setup for a polyglot pipeline. You’d need:\n\nA Dockerfile to set up the environment\nA Makefile to orchestrate the workflow\nWrapper scripts for each step\n\nHere’s what a Makefile might look like:\n# Makefile for a Python → R pipeline\n\nDATA_DIR = data\nOUTPUT_DIR = output\n\n$(OUTPUT_DIR)/predictions.csv: $(DATA_DIR)/raw.csv scripts/train_model.py\npython scripts/train_model.py $(DATA_DIR)/raw.csv $@\n\n$(OUTPUT_DIR)/plot.png: $(OUTPUT_DIR)/predictions.csv scripts/visualise.R\nRscript scripts/visualise.R $&lt; $@\n\n$(OUTPUT_DIR)/report.html: $(OUTPUT_DIR)/plot.png report.qmd\nquarto render report.qmd -o $@\n\nall: $(OUTPUT_DIR)/report.html\n\nclean:\nrm -rf $(OUTPUT_DIR)/*\nThis looks clean, but notice the procedural boilerplate required in each script. Your train_model.py must parse command-line arguments and handle file I/O:\n# scripts/train_model.py\nimport sys\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef main():\n    input_path = sys.argv[1]\n    output_path = sys.argv[2]\n\n    # Load data\n    df = pd.read_csv(input_path)\n\n    # ... actual ML logic ...\n\n    # Save results\n    predictions.to_csv(output_path, index=False)\n\nif __name__ == \"__main__\":\n    main()\nAnd your visualise.R script needs the same boilerplate:\n# scripts/visualise.R\nargs &lt;- commandArgs(trailingOnly = TRUE)\ninput_path &lt;- args[1]\noutput_path &lt;- args[2]\n\n# Load data\npredictions &lt;- read.csv(input_path)\n\n# ... actual visualisation logic ...\n\n# Save plot\nggsave(output_path, plot)\nThe scientific logic is buried under file I/O scaffolding. And the environment? That’s a separate 100+ line Dockerfile you must maintain.\n\n\n6.1.4 rixpress: Unified Orchestration\nThis brings us to a key insight: a reproducible pipeline should be nothing more than a composition of pure functions, each with explicit inputs and outputs, no hidden state, and no reliance on execution order beyond what the data dependencies require.\n{rixpress} solves this by using Nix not just as a package manager, but as the build automation engine itself. Each pipeline step is a Nix derivation: a hermetically sealed build unit.\nCompare the same pipeline in {rixpress}:\n\nlibrary(rixpress)\n\nlist(\n  rxp_py_file(\n    name = raw_data,\n    path = \"data/raw.csv\",\n    read_function = \"lambda x: pandas.read_csv(x)\"\n  ),\n\n  rxp_py(\n    name = predictions,\n    expr = \"train_model(raw_data)\",\n    user_functions = \"functions.py\"\n  ),\n\n  rxp_py2r(\n    # rxp_py2r uses reticulate for conversion\n    name = predictions_r,\n    expr = predictions\n  ),\n\n  rxp_r(\n    name = plot,\n    expr = visualise(predictions_r),\n    user_functions = \"functions.R\"\n  ),\n\n  rxp_qmd(\n    name = report,\n    qmd_file = \"report.qmd\"\n  )\n) |&gt;\n  rxp_populate()\n\nAnd your functions.py contains only the scientific logic:\n# functions.py\ndef train_model(df):\n    # ... pure ML logic, no file I/O ...\n    return predictions\nThe difference is stark:\n\n\n\n\n\n\n\n\nAspect\nMake + Docker\nrixpress\n\n\n\n\nFiles needed\nDockerfile, Makefile, wrapper scripts\ngen-env.R, gen-pipeline.R, function files\n\n\nI/O handling\nManual in every script\nAutomatic via encoders/decoders\n\n\nDependencies\nExplicit file rules\nInferred from object references\n\n\nEnvironment\nSeparate Docker setup\nUnified via {rix}\n\n\nExpertise needed\nLinux admin, Make syntax\nR programming\n\n\n\nThis provides two key benefits:\n\nTrue Polyglot Pipelines: Each step can have its own Nix environment. A Python step runs in a pure Python environment, an R step in an R environment, a Quarto step in yet another, all within the same pipeline. Julia is also supported, and {reticulate} can be used for data transfer, but arbitrary encoder and decoder as well (more on this later).\nDeep Reproducibility: Each step is cached based on the cryptographic hash of all its inputs: the code, the data, and the environment. Any change in dependencies triggers a rebuild. This is reproducibility at the build level, not just the environment level.\n\nThe interface is heavily inspired by {targets}, so you get the ergonomic, object-passing feel you’re used to, combined with the bit-for-bit reproducibility of the Nix build system.\n\n\n\n\n\n\nGetting LLM assistance with {rixpress} and ryxpress\n\n\n\nIf the {rixpress} syntax is new to you, remember that you can use pkgctx to generate LLM-ready context (as mentioned in the introduction). Both the {rixpress} (R) and ryxpress (Python) repositories include .pkgctx.yaml files you can feed to your LLM to help it understand the package’s API. You can also generate your own context files:\n# For the R package\nnix run github:b-rodrigues/pkgctx -- r github:b-rodrigues/rixpress &gt; rixpress.pkgctx.yaml\n\n# For the Python package\nnix run github:b-rodrigues/pkgctx -- python ryxpress &gt; ryxpress.pkgctx.yaml\nWith this context, your LLM can help you write correct pipeline definitions, even if the syntax is completely new to you. You can do so for any package hosted on CRAN, GitHub, or local .tar.gz files.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#what-is-rixpress",
    "href": "06-rixpress.html#what-is-rixpress",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.2 What is rixpress?",
    "text": "6.2 What is rixpress?\n{rixpress} streamlines creation of micropipelines (small-to-medium, single-machine analytic pipelines) by expressing a pipeline in idiomatic R while delegating build orchestration to the Nix build system.\nKey features:\n\nDefine pipeline derivations with concise rxp_*() helper functions\nSeamlessly mix R, Python, Julia, and Quarto steps\nReuse hermetic environments defined via {rix} and a default.nix\nVisualise and inspect the DAG; selectively read, load, or copy outputs\nAutomatic caching: only rebuild what changed\n\n{rixpress} provides several functions to define pipeline steps:\n\n\n\nFunction\nPurpose\n\n\n\n\nrxp_r()\nRun R code\n\n\nrxp_r_file()\nRead a file using R\n\n\nrxp_py()\nRun Python code\n\n\nrxp_py_file()\nRead a file using Python\n\n\nrxp_qmd()\nRender a Quarto document\n\n\nrxp_py2r()\nConvert Python object to R using reticulate\n\n\nrxp_r2py()\nConvert R object to Python using reticulate\n\n\n\nHere is what a basic pipeline looks like:\n\nlibrary(rixpress)\n\nlist(\n  rxp_r_file(\n    mtcars,\n    'mtcars.csv',\n    \\(x) read.csv(file = x, sep = \"|\")\n  ),\n\n  rxp_r(\n    mtcars_am,\n    filter(mtcars, am == 1)\n  ),\n\n  rxp_r(\n    mtcars_head,\n    head(mtcars_am)\n  ),\n\n  rxp_qmd(\n    page,\n    \"page.qmd\"\n  )\n) |&gt;\n  rxp_populate()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#getting-started",
    "href": "06-rixpress.html#getting-started",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.3 Getting Started",
    "text": "6.3 Getting Started\n\n6.3.1 Initialising a project\nIf you’re starting fresh, you can bootstrap a project using a temporary shell:\nnix-shell -p R rPackages.rix rPackages.rixpress\nOnce inside, start R and run:\n\nrixpress::rxp_init()\n\nThis creates two essential files:\n\ngen-env.R: Where you define your environment with {rix}\ngen-pipeline.R: Where you define your pipeline with {rixpress}\n\n\n\n6.3.2 Defining the environment\nOpen gen-env.R and define the tools your pipeline needs:\n\nlibrary(rix)\n\nrix(\n  date = \"2025-10-14\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\", \"quarto\", \"rixpress\"),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nRun this script to generate default.nix, then build and enter your environment:\nnix-build\nnix-shell\n\n\n6.3.3 Defining the pipeline\nOpen gen-pipeline.R and define your pipeline:\n\nlibrary(rixpress)\n\nlist(\n  rxp_r_file(\n    name = mtcars,\n    path = \"data/mtcars.csv\",\n    read_function = \\(x) read.csv(x, sep = \"|\")\n  ),\n\n  rxp_r(\n    name = mtcars_am,\n    expr = dplyr::filter(mtcars, am == 1)\n  ),\n\n  rxp_r(\n    name = mtcars_head,\n    expr = head(mtcars_am)\n  )\n) |&gt;\n  rxp_populate()\n\nRunning rxp_populate() generates a pipeline.nix file and builds the entire pipeline. It also creates a _rixpress folder with required files for the project.\nYou don’t need to write gen-pipeline.R completely in one go. Instead, start with a single derivation, usually the one loading the data, and build the pipeline using source(\"gen-pipeline.R\") and then rxp_make(). Alternatively, if you set build = TRUE in rxp_populate(), sourcing the script alone would be enough, as the pipeline would be built automatically.\nIn an interactive session, you can use rxp_read(), rxp_load() to read or load artifacts into the R session respectively and rxp_trace() to check the lineage of an artifact. This way, you can also work interactively with {rixpress}: add derivations one by one and incrementally build the pipeline. If you change previous derivations or functions that affect them, you don’t need to worry about re-running past steps—these will be executed automatically since they’ve changed!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#real-world-examples",
    "href": "06-rixpress.html#real-world-examples",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.4 Real-World Examples",
    "text": "6.4 Real-World Examples\nThe rixpress_demos repository contains many complete examples. Here are a few practical patterns to get you started.\n\n6.4.1 Example 1: Reading Many Input Files\nWhen you have multiple CSV files in a directory:\n\nlibrary(rixpress)\n\nlist(\n  # R approach: read all files at once\n  rxp_r_file(\n    name = mtcars_r,\n    path = \"data\",\n    read_function = \\(x) {\n      readr::read_delim(list.files(x, full.names = TRUE), delim = \"|\")\n    }\n  ),\n\n  # Python approach: custom function\n  rxp_py_file(\n    name = mtcars_py,\n    path = \"data\",\n    read_function = \"read_many_csvs\",\n    user_functions = \"functions.py\"\n  ),\n\n  rxp_py(\n    name = head_mtcars,\n    expr = \"mtcars_py.head()\"\n  )\n) |&gt;\n  rxp_populate()\n\nThe key insight: rxp_r_file() and rxp_py_file() can point to a directory, and your read_function handles the logic.\n\n\n6.4.2 Example 2: Machine Learning with XGBoost\nThis pipeline trains an XGBoost classifier in Python, then passes predictions to R for evaluation with {yardstick}:\n\nlibrary(rixpress)\n\nlist(\n  # Load data as NumPy array\n  rxp_py_file(\n    name = dataset_np,\n    path = \"data/pima-indians-diabetes.csv\",\n    read_function = \"lambda x: loadtxt(x, delimiter=',')\"\n  ),\n\n  # Split features and target\n  rxp_py(name = X, expr = \"dataset_np[:,0:8]\"),\n  rxp_py(name = Y, expr = \"dataset_np[:,8]\"),\n\n  # Train/test split\n  rxp_py(\n    name = splits,\n    expr = \"train_test_split(X, Y, test_size=0.33, random_state=7)\"\n  ),\n\n  # Extract splits\n\n  rxp_py(name = X_train, expr = \"splits[0]\"),\n  rxp_py(name = X_test, expr = \"splits[1]\"),\n  rxp_py(name = y_train, expr = \"splits[2]\"),\n  rxp_py(name = y_test, expr = \"splits[3]\"),\n\n  # Train XGBoost model\n  rxp_py(\n    name = model,\n    expr = \"XGBClassifier(use_label_encoder=False, eval_metric='logloss').fit(X_train, y_train)\"\n  ),\n\n  # Make predictions\n  rxp_py(name = y_pred, expr = \"model.predict(X_test)\"),\n\n  # Export predictions to CSV for R\n  rxp_py(\n    name = combined_df,\n    expr = \"DataFrame({'truth': y_test, 'estimate': y_pred})\"\n  ),\n\n  rxp_py(\n    name = combined_csv,\n    expr = \"combined_df\",\n    user_functions = \"functions.py\",\n    encoder = \"write_to_csv\"\n  ),\n\n  # Compute confusion matrix in R\n  rxp_r(\n    combined_factor,\n    expr = mutate(combined_csv, across(everything(), factor)),\n    decoder = \"read.csv\"\n  ),\n\n  rxp_r(\n    name = confusion_matrix,\n    expr = yardstick::conf_mat(combined_factor, truth, estimate)\n  )\n) |&gt;\n  rxp_populate(build = FALSE)\n\n# Adjust Python imports\nadjust_import(\"import numpy\", \"from numpy import array, loadtxt\")\nadjust_import(\"import xgboost\", \"from xgboost import XGBClassifier\")\nadjust_import(\"import sklearn\", \"from sklearn.model_selection import train_test_split\")\nadd_import(\"from pandas import DataFrame\", \"default.nix\")\n\nrxp_make()\n\nThis demonstrates:\n\nPython-heavy computation with XGBoost\nCustom serialization via encoder/decoder\nAdjusting Python imports with adjust_import() and add_import()\nPassing results to R for evaluation\n\n\n\n6.4.3 Example 3: Simple Python→R→Quarto Workflow\nA complete pipeline that bounces data between languages and renders a report:\n\nlibrary(rixpress)\n\nlist(\n  # Read with Python polars\n  rxp_py_file(\n    name = mtcars_pl,\n    path = \"data/mtcars.csv\",\n    read_function = \"lambda x: polars.read_csv(x, separator='|')\"\n  ),\n\n  # Filter in Python, convert to pandas for reticulate\n  rxp_py(\n    name = mtcars_pl_am,\n    expr = \"mtcars_pl.filter(polars.col('am') == 1).to_pandas()\"\n  ),\n\n  # Convert to R\n  rxp_py2r(name = mtcars_am, expr = mtcars_pl_am),\n\n  # Process in R\n  rxp_r(\n    name = mtcars_head,\n    expr = my_head(mtcars_am),\n    user_functions = \"functions.R\"\n  ),\n\n  # Back to Python\n  rxp_r2py(name = mtcars_head_py, expr = mtcars_head),\n\n  # More Python processing\n  rxp_py(name = mtcars_tail_py, expr = \"mtcars_head_py.tail()\"),\n\n  # Back to R\n  rxp_py2r(name = mtcars_tail, expr = mtcars_tail_py),\n\n  # Final R step\n  rxp_r(name = mtcars_mpg, expr = dplyr::select(mtcars_tail, mpg)),\n\n  # Render Quarto document\n  rxp_qmd(\n    name = page,\n    qmd_file = \"my_doc/page.qmd\",\n    additional_files = c(\"my_doc/content.qmd\", \"my_doc/images\")\n  )\n) |&gt;\n  rxp_populate()\n\nNote the additional_files argument for rxp_qmd(): this includes child documents and images that the main Quarto file needs.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#working-with-pipelines",
    "href": "06-rixpress.html#working-with-pipelines",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.5 Working with Pipelines",
    "text": "6.5 Working with Pipelines\n\n6.5.1 Building the pipeline\nYou can build the pipeline in two steps:\n\n# Generate pipeline.nix only (don't build)\nrxp_populate(build = FALSE)\n\n# Build the pipeline\nrxp_make()\n\nOr in a single step with rxp_populate(build = TRUE). Once you’ve built the pipeline, you can inspect the results.\n\n\n6.5.2 Inspecting outputs\nBecause outputs live in /nix/store/, {rixpress} provides helpers:\n\n# List all built artifacts\nrxp_inspect()\n\n# Read an artifact into R\nrxp_read(\"mtcars_head\")\n\n# Load an artifact into the global environment\nrxp_load(\"mtcars_head\")\n\nIt is important to understand that all artifacts are stored in the /nix/store/. So if you want to save the outputs somewhere else, you need to use rxp_copy().\nrxp_copy(\"clean_data\")\nThis will copy the clean_data artifacts into the current working directory. If you call rxp_copy() without arguments, every artifact will be copied over.\n\n\n6.5.3 Visualising the pipeline\nThere are three ways to visualise pipelines. The first is to look at the trace of a derivation or of the full pipeline. This is similar to a genealogy tree. You can start by running rxp_inspect() to double check how the derivation names are written:\nrxp_inspect()\nderivation build_success\n1              X_test          TRUE\n2             X_train          TRUE\n3               alpha          TRUE\n4                beta          TRUE\n5     all-derivations          TRUE\n6               delta          TRUE\n7        final_report          TRUE\n8   model_predictions          TRUE\n9         output_plot          TRUE\n10        predictions          TRUE\n11     processed_data          TRUE\n12                rho          TRUE\n13              sigma          TRUE\n14            sigma_z          TRUE\n15 simulated_rbc_data          TRUE\n16      trained_model          TRUE\n17             y_test          TRUE\n18            y_train          TRUE\nLet’s check the trace of output_plot:\nrxp_trace(\"output_plot\")\n==== Lineage for: output_plot ====\nDependencies (ancestors):\n  - predictions\n    - y_test*\n      - processed_data*\n        - simulated_rbc_data*\n          - alpha*\n          - beta*\n          - delta*\n          - rho*\n          - sigma*\n          - sigma_z*\n    - model_predictions*\n      - X_test*\n        - processed_data*\n      - trained_model*\n        - X_train*\n          - processed_data*\n        - y_train*\n          - processed_data*\n\nReverse dependencies (children):\n  - final_report\n\nNote: '*' marks transitive dependencies (depth &gt;= 2).\nWe see that output_plot depends directly on predictions, and predictions on y_test, which itself depends on all the other listed dependencies. The children of the derivation are also listed, in this case final_report.\nCalling rxp_trace() without arguments shows the entire pipeline:\nrxp_trace()\n- final_report\n  - simulated_rbc_data\n    - alpha*\n    - beta*\n    - delta*\n    - rho*\n    - sigma*\n    - sigma_z*\n  - output_plot\n    - predictions*\n      - y_test*\n        - processed_data*\n          - simulated_rbc_data*\n      - model_predictions*\n        - X_test*\n          - processed_data*\n        - trained_model*\n          - X_train*\n            - processed_data*\n          - y_train*\n            - processed_data*\n\nNote: '*' marks transitive dependencies (depth &gt;= 2).\nAnother way to visualise the pipeline is by using rxp_ggdag(). This requires the {ggdag} package to be installed in the environment. Calling rxp_ggdag() shows the following plot:\n\n\n\nDerivations are coloured by programming language.\n\n\nFinally, you can also use rxp_visnetwork() (which requires the {visNetwork} package). This opens an interactive JavaScript visualization in your browser, allowing you to zoom into and explore different parts of your pipeline.\n\n\n6.5.4 Caching, Incremental Builds, Build Logs and Debugging\nOne of the most powerful features of using Nix for pipelines is automatic caching. Because Nix tracks all inputs to each derivation, it knows exactly what needs to be rebuilt when something changes.\nTry this:\n\nBuild your pipeline with rxp_make()\nChange one step in your pipeline\nRun rxp_make() again\n\nNix will detect that unchanged steps are already cached and instantly reuse them. It only rebuilds the steps affected by your change. Also, your pipeline is actually also a child of the environment. So if the environment changes (for example, you update the date argument in the rix() call), your entire pipeline will be rebuilt. While this might seem like overkill, this is actually the only way you can guarantee that the pipeline is reproducible and works. If the pipeline weren’t rebuilt when updating the environment, and cached results were re-used, you’d never know if a non-backwards-compatible change in a function in some package would break your pipeline!\nEvery time you run rxp_populate(), a timestamped log is saved in the _rixpress/ directory. This is like having a Git history for your pipeline’s outputs. Suppose I update a derivation in my pipeline, and build the pipeline again. Calling rxp_list_logs() shows me this:\n\nrxp_list_logs()\n\n&gt; rxp_list_logs()\nfilename\n1 build_log_20260131_104726_bpjnnq2nax40w45rzbl83ggp5y766dsv.json\n2 build_log_20260131_101941_y4gz22nl1aym5yfzp3wgj5glmzv3wx5v.json\nmodification_time size_kb\n1 2026-01-31 10:47:26    3.06\n2 2026-01-31 10:19:41    3.06\nI also recommend committing the change, so the time of the rixpress log matches the time of the change:\ngit log\ncommit 043dd99cb56966af449796f636470462b5624d85\nAuthor: Bruno Rodrigues &lt;bruno@brodrigues.co&gt;\nDate:   Sat Jan 31 10:46:02 2026 +0100\n\n  updated alpha from 0.3 to 0.4\nI can now update the derivation again, and rebuild the pipeline. rxp_list_logs() shows the following:\n                                                         filename\n1 build_log_20260131_110120_v470crwiqx2k0g5bfvd4mqm73ssjdaa5.json\n2 build_log_20260131_104726_bpjnnq2nax40w45rzbl83ggp5y766dsv.json\n3 build_log_20260131_101941_y4gz22nl1aym5yfzp3wgj5glmzv3wx5v.json\n    modification_time size_kb\n1 2026-01-31 11:01:20    3.06\n2 2026-01-31 10:47:26    3.06\n3 2026-01-31 10:19:41    3.06\nI can now compare results:\n#| eval: false\nrxp_list_logs()\n\nold_result &lt;- rxp_read(\"output_plot\", which_log = \"20260131_101941\")\n\nnew_result &lt;- rxp_read(\"output_plot\", which_log = \"20260131_110120\")\nThe which_log argument is used to select the log you want. This is incredibly powerful for debugging and validation. You can go back in time to inspect any output from any previous pipeline run.\n\n\n6.5.5 Running Someone Else’s Pipeline\nThe ultimate test of reproducibility: can someone else run your pipeline?\nWith a Nix-based workflow, they need only:\n\ngit clone your repository\nRun nix-build && nix-shell\nRun source(\"gen-pipeline.R\") or rxp_make()\n\nThat’s it. Nix reads your default.nix and pipeline.nix files and builds the exact same environment and data product, bit-for-bit.\n\n\n6.5.6 Exporting and Importing Artifacts\nFor CI/CD or sharing between machines:\n\n# Export build products to a tarball\nrxp_export_artifacts()\n\n# Import on another machine before building\nrxp_import_artifacts()\n\nThis speeds up continuous integration by avoiding unnecessary rebuilds.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#organizing-large-projects-with-sub-pipelines",
    "href": "06-rixpress.html#organizing-large-projects-with-sub-pipelines",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.6 Organizing Large Projects with Sub-Pipelines",
    "text": "6.6 Organizing Large Projects with Sub-Pipelines\nAs pipelines grow, a single gen-pipeline.R file can become difficult to manage. Consider the typical data science project:\n\nData extraction and cleaning (ETL)\nFeature engineering\nModel training\nModel evaluation\nReport generation\n\nPutting all derivations in one file makes it hard to:\n\nNavigate the code\nUnderstand which derivations belong to which phase\nCollaborate across team members\nReuse pipeline components in other projects\n\nTo solve this issue, you can define your project using sub-pipelines and join them into a master pipeline using rxp_pipeline().\nThis allows you to organise derivations into named groups and even colour-code them for making it easy to visualise.\nA project with sub-pipelines would look something like this:\nmy-project/\n├── default.nix           # Nix environment (generated by rix)\n├── gen-env.R             # Script to generate default.nix\n├── gen-pipeline.R        # MASTER SCRIPT: combines all sub-pipelines\n└── pipelines/\n    ├── 01_data_prep.R    # Data preparation sub-pipeline\n    ├── 02_analysis.R     # Analysis sub-pipeline\n    └── 03_reporting.R    # Reporting sub-pipeline\nEach sub-pipeline file returns a list of derivations:\n\n# Data Preparation Sub-Pipeline\n# pipelines/01_data_prep.R\nlibrary(rixpress)\n\nlist(\n  rxp_r(\n    name = raw_mtcars,\n    expr = mtcars\n  ),\n  rxp_r(\n    name = clean_mtcars,\n    expr = dplyr::filter(raw_mtcars, am == 1)\n  ),\n  rxp_r(\n    name = selected_mtcars,\n    expr = dplyr::select(clean_mtcars, mpg, cyl, hp, wt)\n  )\n)\n\nThe rxp_pipeline() function takes:\n\nname: A descriptive name for this group of derivations\npath: Either a file path to an R script returning a list of derivations (recommended), or a list of derivation objects.\ncolor: Optional CSS color name or hex code for DAG visualisation\n\nThe second sub-pipeline:\n\n# Analysis Sub-Pipeline\n# pipelines/02_analysis.R\nlibrary(rixpress)\n\nlist(\n  rxp_r(\n    name = summary_stats,\n    expr = summary(selected_mtcars)\n  ),\n  rxp_r(\n    name = mpg_model,\n    expr = lm(mpg ~ hp + wt, data = selected_mtcars)\n  ),\n  rxp_r(\n    name = model_coefs,\n    expr = coef(mpg_model),\n    )\n)\n\nThe master script becomes very clean, as rxp_pipeline() handles sourcing the files:\n\n# gen-pipeline.R\nlibrary(rixpress)\n\n# Create named pipelines with colours by pointing to the files\npipe_data_prep &lt;- rxp_pipeline(\n  name = \"Data Preparation\",\n  path = \"pipelines/01_data_prep.R\",\n  color = \"#E69F00\"\n)\n\npipe_analysis &lt;- rxp_pipeline(\n  name = \"Statistical Analysis\",\n  path = \"pipelines/02_analysis.R\",\n  color = \"#56B4E9\"\n)\n\n# Build combined pipeline\nrxp_populate(\n  list(pipe_data_prep, pipe_analysis),\n  project_path = \".\",\n  build = TRUE)\n\n\n6.6.1 Visualising Sub-Pipelines\nWhen sub-pipelines are defined, visualisation tools use pipeline colours:\n\nInteractive Network (rxp_visnetwork()) and Static DAG (rxp_ggdag()) both use a dual-encoding approach:\n\nNode fill (interior): Derivation type colour (R = blue, Python = yellow, etc.)\nNode border (thick stroke): Pipeline group colour This allows you to see both what type of computation each node is and which pipeline it belongs to.\n\n\n\n\n\nSubpipelines are coloured.\n\n\n\nTrace: rxp_trace() output in the console is coloured by pipeline (using the cli package).\n\n\n\n\nIf your terminal supports it, derivation names are coloured according to the chosen sub-pipeline colour.\n\n\nIt is also possible to not highlight sub-pipelines by using the colour_by argument (allows you to switch between colouring the pipeline, or by derivation type):\n\nrxp_ggdag(colour_by = \"pipeline\")\n\nrxp_ggdag(colour_by = \"type\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#polyglot-pipelines",
    "href": "06-rixpress.html#polyglot-pipelines",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.7 Polyglot Pipelines",
    "text": "6.7 Polyglot Pipelines\nOne of {rixpress}’s strengths is seamlessly mixing languages. Here’s a pipeline that reads data with Python’s polars, processes it with R’s dplyr, and renders a Quarto report.\n\n\n\n\n\n\nPolyglot Development Is Now Cheap\n\n\n\nHistorically, using multiple languages in one project meant significant setup overhead: installing interpreters, managing conflicting dependencies, writing glue code. With Nix, that cost drops to near zero. You declare your R and Python dependencies in one file, and Nix handles the rest.\nLLMs lower the barrier further. Even if you are primarily an R programmer, you can ask an LLM to generate the Python code for a specific step, or vice versa. You don’t need to master both languages; you just need to know enough to recognise when each shines. Use R for statistics, Bayesian modelling, and visualisation with {ggplot2}. Use Python for deep learning, web scraping, or leveraging a library that only exists in the Python ecosystem. With Nix handling environments and LLMs helping with syntax, the “cost” of crossing language boundaries becomes negligible.\n\n\nOpen gen-env.R and define the tools your pipeline needs:\n\nlibrary(rix)\n\nrix(\n  date = \"2026-01-26\",\n  r_pkgs = c(\n    \"chronicler\",\n    \"dplyr\",\n    \"igraph\",\n    \"quarto\",\n    \"reticulate\",\n    \"rix\",\n    \"rixpress\"\n  ),\n  py_conf = list(\n    py_version = \"3.14\",\n    py_pkgs = c(\n      \"biocframe\",\n      \"numpy\",\n      \"phart\",\n      \"polars\",\n      \"pyarrow\",\n      \"rds2py\",\n      \"ryxpress\"\n    )\n  ),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nThe Python packages biocframe, phart, and rds2py are optional but highly recommended when working with ryxpress. biocframe enables direct transfer to polars or Bioconductor DataFrames, bypassing the need for pandas. rds2py (also from the BiocPy project) allows Python to read .rds files, R’s native binary format. Together, these packages ensure seamless interoperability between your R and Python steps. phart is a nice little package that allows you to visualise pipelines from a Python interpreter prompt: not as sexy as R’s {ggdag} but it gets the job done!\n\n6.7.1 Transferring data between Python and R\nThe rxp_py2r() and rxp_r2py() functions use {reticulate} to convert objects between languages:\n\nrxp_py2r(\n  name = mtcars_r,\n  expr = mtcars_py\n)\n\nHowever, {reticulate} conversions can sometimes be fragile or behave unexpectedly with complex objects. For robust production pipelines, I recommend using the encoder and decoder arguments to explicitly serialize data to intermediary formats like CSV, JSON, or Apache Arrow (Parquet/Feather).\nYou can define a custom encoder in Python and a decoder in R:\n\n# Python step: serialize to JSON\nrxp_py(\n  name = mtcars_json,\n  expr = \"mtcars_pl.filter(polars.col('am') == 1)\",\n  user_functions = \"functions.py\",\n  encoder = \"serialize_to_json\"\n),\n\n# R step: deserialize from JSON\nrxp_r(\n  name = mtcars_head,\n  expr = my_head(mtcars_json),\n  user_functions = \"functions.R\",\n  decoder = \"jsonlite::fromJSON\"\n)\n\nThe Python serialize_to_json function would be defined in functions.py:\n#| eval: false\ndef serialize_to_json(pl_df, path):\n    with open(path, 'w') as f:\n        f.write(pl_df.write_json())\nFor larger datasets, Apache Arrow is ideal because it allows zero-copy reads and is supported natively by polars, pandas, and R’s {arrow} package.\nHere is how you would transfer data from Python to R:\n\nrxp_py(\n  name = processed_data,\n  expr = \"prepare_features(raw_data)\",\n  user_functions = \"functions.py\",\n  encoder = \"save_arrow\"\n),\n\nrxp_r(\n  name = analysis_results,\n  expr = \"run_analysis(processed_data)\",\n  user_functions = \"functions.R\",\n  decoder = \"arrow::read_feather\"\n)\n\nThe Python save_arrow encoder function:\n#| eval: false\ndef save_arrow(df: pd.DataFrame, path: str):\n    \"\"\"Encoder function to save a pandas DataFrame to an Arrow file.\"\"\"\n    feather.write_feather(df, path)\nBoth encoder and decoder functions must accept the object as the first argument and the file path as the second argument. This pattern works for any serialization format supported by your languages of choice.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#advanced-topics",
    "href": "06-rixpress.html#advanced-topics",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.8 Advanced Topics",
    "text": "6.8 Advanced Topics\n\n6.8.1 Complete Polyglot Pipeline Walkthrough\nYou can find the complete code source to this example over here1.\nFor this polyglot pipeline, I use three programming languages: Julia to simulate synthetic data from a macroeconomic model, Python to train a machine learning model and R to visualize the results. Let me make it clear, though, that scientifically speaking, this is nonsensical: this is merely an example to showcase how to setup a complete end-to-end project with {rixpress}.\nThe model I use is the foundational Real Business Cycle (RBC) model. The theory and log-linearized equations used in the Julia script are taken directly from the excellent lecture slides by Bianca De Paoli.\n\n\n\n\n\n\nSource Attribution\n\n\n\nThe economic theory and equations for the RBC model are based on the following source. This document serves as a guide for our implementation.\nSource: De Paoli, B. (2009). Slides 1: The RBC Model, Analytical and Numerical solutions. https://personal.lse.ac.uk/depaoli/RBC_slides1.pdf\n\n\nFirst, we need the actual execution environment. Thanks to the {rix} package we can declaratively define a default.nix file. This file locks down the exact versions of R, Julia, Python, and all their respective packages, ensuring the pipeline runs identically today, tomorrow, or on any machine with Nix installed.\n#| eval: false\n#| code-summary: \"Environment Definition with {rix}\"\n# This script defines the polyglot environment our pipeline will run in.\nlibrary(rix)\n\n# Define the complete execution environment\nrix(\n  # Pin the environment to a specific date to ensure that all package\n  # versions are resolved as they were on this day.\n  date = \"2025-10-14\",\n\n  # 1. R Packages\n  # We need packages for plotting, data manipulation, and reading arrow files.\n  r_pkgs = c(\n    \"ggplot2\",\n    \"dplyr\",\n    \"arrow\",\n    \"rix\",\n    \"rixpress\"\n  ),\n\n  # 2. Julia Configuration\n  # We specify the Julia version and the list of packages needed\n  # for our manual RBC model simulation.\n  jl_conf = list(\n    jl_version = \"lts\",\n    jl_pkgs = c(\n      \"Distributions\", # For creating random shocks\n      \"DataFrames\", # For structuring the output\n      \"Arrow\", # For saving the data in a cross-language format\n      \"Random\"\n    )\n  ),\n\n  # 3. Python Configuration\n  # We specify the Python version and the packages needed for the\n  # machine learning step.\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\n      \"pandas\",\n      \"scikit-learn\",\n      \"xgboost\",\n      \"pyarrow\"\n    )\n  ),\n\n  # We set the IDE to 'none' for a minimal environment. You could change\n  # this to \"rstudio\" if you prefer to work interactively in RStudio.\n  ide = \"none\",\n\n  # Define the project path and allow overwriting the default.nix file.\n  project_path = \".\",\n  overwrite = TRUE\n)\nAssuming we are working on a system that has Nix installed, we can “drop” into a temporary Nix shell with R and {rix} available:\nnix-shell --expr \"$(curl -sl https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix)\"\nOnce the shell is ready, start R (by simply typing R) and run source(\"gen-env.R\") to generate the adequate default.nix file (or even easier, just Rscript gen-env.R). This is the environment that we can use to work interactively with the code while developing, and in which the pipeline will be executed.\nWe then need some dedicated scripts with our code. This Julia script contains a pure function that simulates the RBC model and returns a DataFrame. The code is a direct implementation of the state-space solution derived from the equations in the aforementioned lecture and is saved under functions/functions.jl. I don’t show the scripts contents here, as it is quite long, so look for it in the link from before.\nAt the end of the script, I create a wrapper around Arrow.write() called arrow_write() to serialize the generated data frame into an Arrow file.\nWhile developing, it is possible to start the Julia interpreter and test things and see if they work. Or you could even start by writing the first lines of gen-pipeline.R like so:\nlibrary(rixpress)\n\nlist(\n  # STEP 0: Define RBC Model Parameters as Derivations\n  # This makes the parameters an explicit part of the pipeline.\n  # Changing a parameter will cause downstream steps to rebuild.\n  rxp_jl(alpha, 0.3), # Capital's share of income\n  rxp_jl(beta, 1 / 1.01), # Discount factor\n  rxp_jl(delta, 0.025), # Depreciation rate\n  rxp_jl(rho, 0.95), # Technology shock persistence\n  rxp_jl(sigma, 1.0), # Risk aversion (log-utility)\n  rxp_jl(sigma_z, 0.01), # Technology shock standard deviation\n\n  # STEP 1: Julia - Simulate a Real Business Cycle (RBC) model.\n  # This derivation runs our Julia script to generate the source data.\n  rxp_jl(\n    name = simulated_rbc_data,\n    expr = \"simulate_rbc_model(alpha, beta, delta, rho, sigma, sigma_z)\",\n    user_functions = \"functions/functions.jl\", # The file containing the function\n    encoder = \"arrow_write\" # The function to use for saving the output\n  )\n) |&gt;\n  rxp_populate(\n    project_path = \".\",\n    build = TRUE,\n    verbose = 1\n  )\nStart an R session, and run source(\"gen-pipeline.R\"). This will cause the model to be simulated. Now, to inspect the data, you can use rxp_read(): by default, rxp_read() will try to find the adequate function to read the data and present it to you. This works for common R and Python objects. But in this case, the generated data is an Arrow data file, so rxp_read(), not knowing how to read it, will simply return the path to the file in the Nix store. We can pass this file to the adequate reader.\nrxp_read(\"simulated_rbc_data\") |&gt;\n  arrow::read_feather() |&gt;\n  head()\nNow that we are happy that our Julia code works, we can move on to the Python step, by writing a dedicated Python script, functions/functions.py.\nThis Python script defines a function to perform our machine learning task. It takes the DataFrame from the Julia step, trains an XGBoost model to predict next-quarter’s output, and returns a new DataFrame containing the predictions. Of course, we could have simply used the RBC model itself for the predictions, but again, this is a toy example.\nThe Python script contains several little functions to make the code as modular as possible. I don’t show the script here, as it is quite long, but just as for the Julia script, I write a wrapper around feather.write_feather() to serialize the data into an Arrow file and pass it finally to R. Just as before, it is possible to start a Python interpreter as defined in the default.nix file and use to test and debug, or to add derivations to gen-pipeline.R, rebuild the pipeline and check the outputs:\nlibrary(rixpress)\n\nlist(\n  # ... the julia steps from before ...,\n\n  # STEP 2.1: Python - Prepare features (lagging data)\n  rxp_py(\n    name = processed_data,\n    expr = \"prepare_features(simulated_rbc_data)\",\n    user_functions = \"functions/functions.py\",\n    # Decode the Arrow file from Julia into a pandas DataFrame\n    decoder = \"feather.read_feather\"\n    # Note: No encoder needed here. {rixpress} will use pickle by default\n    # to pass the DataFrame between Python steps.\n  ),\n\n  # STEP 2.2: Python - Split data into training and testing sets\n  rxp_py(\n    name = X_train,\n    expr = \"get_X_train(processed_data)\",\n    user_functions = \"functions/functions.py\"\n  ),\n\n  # ... more Python steps as needed ...\n) |&gt;\n  rxp_populate(\n    project_path = \".\",\n    build = TRUE,\n    verbose = 1\n  )\nArrow is used to pass data from Julia to Python, but then from Python to Python, pickle is used. Finally, we can continue by passing data further down to R. Once again, I recommend writing a dedicated script with your functions that you need and save it in functions/functions.R. The gen-pipeline.R script would look something like this:\nlibrary(rixpress)\n\nlist(\n  # ... the julia steps from before ...,\n\n  # ... Python steps ...,\n\n  # STEP 2.5: Python - Format final results for R\n  rxp_py(\n    name = predictions,\n    expr = \"format_results(y_test, model_predictions)\",\n    user_functions = \"functions/functions.py\",\n    # We need an encoder here to save the final DataFrame as an Arrow file\n    # so the R step can read it.\n    encoder = \"save_arrow\"\n  ),\n\n  # STEP 3: R - Visualize the predictions from the Python model.\n  # This final derivation depends on the output of the Python step.\n  rxp_r(\n    name = output_plot,\n    expr = plot_predictions(predictions), # The function to call from functions.R\n    user_functions = \"functions/functions.R\",\n    # Specify how to load the upstream data (from Python) into R.\n    decoder = arrow::read_feather\n  ),\n\n  # STEP 4: Quarto - Compile the final report.\n  rxp_qmd(\n    name = final_report,\n    additional_files = \"_rixpress\",\n    qmd_file = \"readme.qmd\"\n  )\n) |&gt;\n  rxp_populate(\n    project_path = \".\",\n    build = TRUE,\n    verbose = 1\n  )\nTo view the plot in an interactive session, you can use rxp_read() again:\nrxp_read(\"output_plot\")\nTo run the pipeline, start the environment by simply typing nix-shell in the terminal, which will use the environment as defined in the default.nix and run source(\"gen-pipeline.R\"). If there are issues, setting the verbose argument of rxp_populate() to 1 is helpful to see the full error logs.\n\n\n6.8.2 ryxpress: The Python Interface\nIf you prefer working in Python, ryxpress provides the same functionality. You still define your pipeline in R (since that’s where {rixpress} runs), but you can build and inspect artifacts from Python.\nTo set up an environment with ryxpress:\n\nrix(\n  date = \"2025-10-14\",\n  r_pkgs = c(\"rixpress\"),\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"ryxpress\", \"rds2py\", \"biocframe\", \"pandas\")\n  ),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nThen from Python:\nfrom ryxpress import rxp_make, rxp_inspect, rxp_load\n\n# Build the pipeline\nrxp_make()\n\n# Inspect artifacts\nrxp_inspect()\n\n# Load an artifact\nrxp_load(\"mtcars_head\")\nryxpress handles the conversion automatically:\n\nTries pickle.load first\nFalls back to rds2py for R objects\nReturns file paths for complex outputs\n\n\n\n6.8.3 More Examples\nThe rixpress_demos repository includes:\n\njl_example: Using Julia in pipelines\nr_qs: Using {qs} for faster serialization\npython_r_typst: Compiling to Typst documents\nr_multi_envs: Different Nix environments for different derivations\nyanai_lercher_2020: Reproducing a published paper’s analysis",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#summary",
    "href": "06-rixpress.html#summary",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "6.9 Summary",
    "text": "6.9 Summary\n{rixpress} unifies environment management and workflow orchestration:\n\nDefine pipelines with rxp_*() functions in familiar R syntax\nMix languages freely: R, Python, Julia, Quarto\nBuild with Nix for deterministic, cached execution\nInspect outputs with rxp_read(), rxp_load(), rxp_copy()\nDebug with timestamped build logs\nShare reproducible pipelines via Git\n\nThe Python port ryxpress provides the same experience for Python-first workflows.\nBy embracing structured, plain-text pipelines over notebooks for production work, your analysis becomes more reliable, more scalable, and fundamentally more reproducible.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "06-rixpress.html#footnotes",
    "href": "06-rixpress.html#footnotes",
    "title": "6  Building Reproducible Pipelines with rixpress",
    "section": "",
    "text": "https://github.com/b-rodrigues/rixpress_demos/tree/master/rbc↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Building Reproducible Pipelines with rixpress</span>"
    ]
  },
  {
    "objectID": "07-docker.html",
    "href": "07-docker.html",
    "title": "7  Containerisation With Docker",
    "section": "",
    "text": "7.1 Introduction\nUp until now, we’ve been using Nix as a powerful tool for creating reproducible development environments directly on our machines. Nix gives us fine-grained control over every package and dependency in our project, ensuring bit-for-bit reproducibility. However, when it comes to distributing a data product, another technology, Docker, is incredibly popular.\nA common misconception is that Nix and Docker are alternatives, that you must choose one or the other. This is wrong. They are conceptually different tools that solve different problems. Nix is a package manager and build system: it answers “what software do I need and how do I build it reproducibly?” Docker is a containerisation platform: it answers “how do I package and run an application in isolation?” Understanding this distinction is key to using them together effectively.\nWhile Nix manages dependencies for an application that runs on a host operating system, Docker takes a different approach: it packages an application along with a lightweight operating system and all its dependencies into a single, portable unit called a container. This container can then run on any machine that has Docker installed, regardless of its underlying OS. Being familiar with both tools makes you a more versatile data scientist: you can use Nix for precise, reproducible development environments and Docker for distributing your work to colleagues, servers, or CI pipelines that may not have Nix installed.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#introduction",
    "href": "07-docker.html#introduction",
    "title": "7  Containerisation With Docker",
    "section": "",
    "text": "7.1.1 Spatial vs Temporal Reproducibility\nTo understand when and why to use Docker, it helps to distinguish between two types of reproducibility:\n\nSpatial reproducibility: The ability to execute an analysis identically across different machines right now. Docker excels here: a container runs the same way on your laptop, a colleague’s workstation, and a cloud server.\nTemporal reproducibility: The ability to execute an analysis identically over time. Docker is weaker here. The imperative commands in a Dockerfile (like apt-get update) are non-deterministic. Rebuilding the same file at different times can yield different images.\n\nThis distinction is crucial. Docker’s true reproducibility promise is that a specific, pre-built image will always launch an identical container. It does not promise that building the same Dockerfile twice will yield an identical image.\nAs empirical research has shown, achieving deterministic builds requires systems designed for this purpose, like Nix’s functional model, where identical inputs always produce identical outputs. Studies rebuilding historical Nix packages found bit-for-bit reproducibility rates exceeding 90%.\n\n\n7.1.2 The Best of Both Worlds\nThis is why we combine Nix and Docker rather than choosing one or the other:\n\nNix provides strong temporal reproducibility: the guarantee that an environment built today will be identical when rebuilt years later.\nDocker provides strong spatial reproducibility and universal distribution: the guarantee that a container runs the same everywhere Docker runs.\n\nThe pattern we’ll use is simple: use Nix inside a Docker container. Start with a minimal base image that has Nix installed. Then, use Nix to declaratively build the precise environment within the image. Docker becomes a portable runtime for this Nix-managed environment, excellent for deployment to systems where Nix isn’t installed.\n\n\n7.1.3 Interactive Development vs Distribution\nThis approach also clarifies when to use each tool:\n\nUse Nix directly for interactive development. It integrates seamlessly with your IDE and filesystem. No volume mounts, no port forwarding, no graphical application headaches.\nUse Docker for distributing finished data products. Package your {rixpress} pipeline into an image that anyone can run with a single command.\n\nIf you’ve never heard of Docker before, this chapter will provide the basic knowledge required to get started. Let’s start by watching this very short video that introduces the core concepts.\nThe Rocker Project provides a large collection of ready-to-use Docker images for R users.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#docker-essentials",
    "href": "07-docker.html#docker-essentials",
    "title": "7  Containerisation With Docker",
    "section": "7.2 Docker Essentials",
    "text": "7.2 Docker Essentials\n\n7.2.1 Installing Docker\nThe first step is to install Docker. You’ll find the instructions for Ubuntu here, for Windows here (read the system requirements section as well!) and for macOS here (make sure to choose the right version for the architecture of your Mac, if you have an M1 Mac use Mac with Apple silicon).\nAfter installation, it might be a good idea to restart your computer, if the installation wizard does not invite you to do so. To check whether Docker was installed successfully, run the following command in a terminal:\ndocker run --rm hello-world\nThis should print the following message:\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\nIf you see this message, congratulations, you are ready to run Docker. If you see an error message about permissions, this means that something went wrong. If you’re running Linux, make sure that your user is in the Docker group by running:\ngroups $USER\nYou should see your username and a list of groups that your user belongs to. If a group called docker is not listed, then you should add yourself to the group by following these steps.\n\n\n7.2.2 The Rocker Project and Image Registries\nWhen running a command like:\ndocker run --rm hello-world\nwhat happens is that an image, in this case hello-world, gets pulled from a so-called registry. A registry is a storage and distribution system for Docker images. Think of it as a GitHub for Docker images, where you can push and pull images, much like you would with code repositories. The default public registry that Docker uses is called Docker Hub, but companies can also host their own private registries to store proprietary images.\nMany open source projects build and distribute Docker images through Docker Hub, for example the Rocker Project.\nThe Rocker Project is instrumental for R users that want to use Docker. The project provides a large list of images that are ready to run with a single command. As an illustration, open a terminal and paste the following line:\ndocker run --rm -e PASSWORD=yourpassword -p 8787:8787 rocker/rstudio\nOnce this stops running, go to http://localhost:8787/ and enter rstudio as the username and yourpassword as the password. You should login to a RStudio instance: this is the web interface of RStudio that allows you to work with R from a server. In this case, the server is the Docker container running the image. Yes, you’ve just pulled a Docker image containing Ubuntu with a fully working installation of RStudio web!\nLet’s open a new script and run the following lines:\n\ndata(mtcars)\nsummary(mtcars)\n\nYou can now stop the container (by pressing CTRL-C in the terminal). Let’s now rerun the container… you should realise that your script is gone! This is the first lesson: whatever you do inside a container will disappear once the container is stopped. This also means that if you install the R packages that you need while the container is running, you will need to reinstall them every time.\nThankfully, the Rocker Project provides a list of images with many packages already available. For example to run R with the {tidyverse} collection of packages already pre-installed, run:\ndocker run --rm -e PASSWORD=yourpassword -p 8787:8787 rocker/tidyverse\n\n\n7.2.3 Basic Docker Workflow\nYou already know about running containers using docker run. With the commands we ran before, your terminal will need to stay open, or else, the container will stop. Starting now, we will run Docker commands in the background using the -d flag (d as in detach):\ndocker run --rm -d -e PASSWORD=yourpassword -p 8787:8787 rocker/tidyverse\nYou can run several containers in the background simultaneously. List running containers with docker ps:\ndocker ps\nCONTAINER ID   IMAGE              COMMAND   CREATED         STATUS         PORTS                    NAMES\nc956fbeeebcb   rocker/tidyverse   \"/init\"   3 minutes ago   Up 3 minutes   0.0.0.0:8787-&gt;8787/tcp   elastic_morse\nStop the container using its ID:\ndocker stop c956fbeeebcb\nLet’s discuss the other flags:\n\n--rm: Removes the container once it’s stopped\n-e: Provides environment variables to the container (e.g., PASSWORD)\n-p: Sets the port mapping (host:container)\n--name: Gives the container a custom name\n\nRun a container with a name:\ndocker run -d --name my_r --rm -e PASSWORD=yourpassword -p 8787:8787 rocker/tidyverse\nYou can now interact with this container using its name:\ndocker exec -ti my_r bash\nYou are now inside a terminal session, inside the running container! This can be useful for debugging purposes.\nFinally, let’s solve the issue of our scripts disappearing. Create a folder somewhere on your computer, then run:\ndocker run -d --name my_r --rm -e PASSWORD=yourpassword -p 8787:8787 \\\n  -v /path/to/your/local/folder:/home/rstudio/scripts:rw rocker/tidyverse\nYou should now be able to save scripts inside the scripts/ folder from RStudio and they will appear in the folder you created on your host machine.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#making-our-own-images",
    "href": "07-docker.html#making-our-own-images",
    "title": "7  Containerisation With Docker",
    "section": "7.3 Making Our Own Images",
    "text": "7.3 Making Our Own Images\nTo create our own images, you can start from an image provided by an open source project like Rocker, or you can start from the base Ubuntu image. Since we are using Nix to set up the reproducible development environment, we can use ubuntu:latest. Our development environment will always be exactly the same, thanks to Nix.\n\n7.3.1 A Minimal Dockerfile with Nix\nFROM ubuntu:latest\n\nRUN apt update -y\nRUN apt install curl -y\n\n# Download the default.nix that comes with {rix}\nRUN curl -O https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix\n\n# Install Nix via Determinate Systems installer\nRUN curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install linux \\\n  --extra-conf \"sandbox = false\" \\\n  --init none \\\n  --no-confirm\n\n# Add Nix to the path\nENV PATH=\"${PATH}:/nix/var/nix/profiles/default/bin\"\nENV user=root\n\n# Configure rstats-on-nix cache\nRUN mkdir -p /root/.config/nix && \\\n    echo \"substituters = https://cache.nixos.org https://rstats-on-nix.cachix.org\" &gt; /root/.config/nix/nix.conf && \\\n    echo \"trusted-public-keys = cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY= rstats-on-nix.cachix.org-1:vdiiVgocg6WeJrODIqdprZRUrhi1JzhBnXv7aWI6+F0=\" &gt;&gt; /root/.config/nix/nix.conf\n\n# Copy script to generate environment\nCOPY gen-env.R .\n\n# Generate default.nix from gen-env.R\nRUN nix-shell --run \"Rscript gen-env.R\"\n\n# Build the environment\nRUN nix-build\n\n# Run nix-shell when container starts\nCMD nix-shell\nEvery Dockerfile starts with a FROM statement specifying the base image. Then, every command starts with RUN. We install and configure Nix, copy an R script to generate the environment, and then build it. Finally, we run nix-shell when executing a container (the CMD statement).\n\n\n7.3.2 Splitting Into a Reusable Base Image\nBecause the Nix installation step is generic, we can split this into two stages. First, create a base image with just Nix installed:\n# nix-base/Dockerfile\nFROM ubuntu:latest AS nix-base\n\nRUN apt update -y && apt install -y curl\n\n# Install Nix via Determinate Systems installer\nRUN curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install linux \\\n  --extra-conf \"sandbox = false\" \\\n  --init none \\\n  --no-confirm\n\nENV PATH=\"/nix/var/nix/profiles/default/bin:${PATH}\"\nENV user=root\n\n# Configure Nix binary cache\nRUN mkdir -p /root/.config/nix && \\\n    echo \"substituters = https://cache.nixos.org https://rstats-on-nix.cachix.org\" &gt; /root/.config/nix/nix.conf && \\\n    echo \"trusted-public-keys = cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY= rstats-on-nix.cachix.org-1:vdiiVgocg6WeJrODIqdprZRUrhi1JzhBnXv7aWI6+F0=\" &gt;&gt; /root/.config/nix/nix.conf\nBuild and tag this image:\ndocker build -t nix-base:latest .\nNow, for any project, simply reuse it:\nFROM nix-base:latest\n\nCOPY gen-env.R .\n\nRUN curl -O https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix\nRUN nix-shell --run \"Rscript gen-env.R\"\nRUN nix-build\n\nCMD [\"nix-shell\"]\nWith gen-env.R:\n\nlibrary(rix)\n\nrix(\n  date = \"2025-08-04\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\"),\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"polars\", \"great-tables\")\n  ),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nBuild and run:\ndocker build -t my-project .\ndocker run -it --rm --name my-project-container my-project\nThis drops you in an interactive Nix shell running inside Docker!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#publishing-images-on-docker-hub",
    "href": "07-docker.html#publishing-images-on-docker-hub",
    "title": "7  Containerisation With Docker",
    "section": "7.4 Publishing Images on Docker Hub",
    "text": "7.4 Publishing Images on Docker Hub\nIf you want to share Docker images through Docker Hub, you first need to create a free account. A free account gives you unlimited public repositories.\nList all images on your computer:\ndocker images\nLog in to Docker Hub:\ndocker login\nTag the image with your username:\ndocker tag IMAGE_ID your_username/nix-base:latest\nPush the image:\ndocker push your_username/nix-base:latest\nThis image can now be used as a stable base for other projects:\nFROM your_username/nix-base:latest\n\nRUN mkdir ...\n\n7.4.1 Sharing Without Docker Hub\nIf you can’t upload to Docker Hub, you can save the image to a file:\ndocker save nix-base | gzip &gt; nix-base.tgz\nLoad it on another machine:\ngzip -d nix-base.tgz\ndocker load &lt; nix-base.tar",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#what-if-you-dont-use-nix",
    "href": "07-docker.html#what-if-you-dont-use-nix",
    "title": "7  Containerisation With Docker",
    "section": "7.5 What If You Don’t Use Nix?",
    "text": "7.5 What If You Don’t Use Nix?\nUsing Nix inside of Docker makes it very easy to set up an environment, but what if you can’t use Nix for some reason? In this case, you would need to use other tools to install the right R or Python packages and it is likely going to be more difficult. The main issue you will face is missing development libraries.\nFor example, to install and use the R {stringr} package, you will need to first install libicu-dev:\nFROM rocker/r-ver:4.5.1\n\nRUN apt-get update && apt-get install -y \\\n    libglpk-dev \\\n    libxml2-dev \\\n    libcairo2-dev \\\n    libgit2-dev \\\n    libcurl4-openssl-dev \\\n    # ... many more\nAnother issue is that building the image is not a reproducible process, only running containers is. To mitigate this, use tagged images or better yet, a digest:\nFROM rocker/r-ver@sha256:1dbe7a6718b7bd8630addc45a32731624fb7b7ffa08c0b5b91959b0dbf7ba88e\nThis will always pull exactly the same layers. However, at some point, that version of Ubuntu will be outdated. Using Nix, you can stay on ubuntu:latest.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#building-docker-images-directly-with-nix",
    "href": "07-docker.html#building-docker-images-directly-with-nix",
    "title": "7  Containerisation With Docker",
    "section": "7.6 Building Docker Images Directly with Nix",
    "text": "7.6 Building Docker Images Directly with Nix\nSo far, we’ve used Nix inside Docker containers. But there’s an even more powerful approach: using Nix to build Docker images directly, bypassing Dockerfiles entirely.\nNix provides dockerTools, a set of functions that can create OCI-compliant container images. Because these images are built through Nix’s deterministic build system, they have stronger reproducibility guarantees than images built with docker build.\n\n7.6.1 Why Skip the Dockerfile?\nA Dockerfile is fundamentally imperative: it’s a script of commands executed in order. Commands like apt-get update are non-deterministic by nature. Even with careful pinning, you’re fighting the tool’s design.\nNix’s dockerTools.buildImage is declarative. You describe what should be in the image, and Nix figures out how to build it reproducibly. The resulting image is a pure function of its inputs.\n\n\n7.6.2 A Basic Example\nHere’s a Nix expression that builds a Docker image containing R and some packages:\n# docker-image.nix\nlet\n  pkgs = import (fetchTarball\n    \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-10-14.tar.gz\"\n  ) {};\n  \n  # Define the R environment\n  myR = pkgs.rWrapper.override {\n    packages = with pkgs.rPackages; [\n      dplyr\n      ggplot2\n      quarto\n    ];\n  };\n  \nin pkgs.dockerTools.buildImage {\n  name = \"my-r-env\";\n  tag = \"latest\";\n  \n  copyToRoot = pkgs.buildEnv {\n    name = \"image-root\";\n    paths = [ myR pkgs.quarto pkgs.coreutils pkgs.bash ];\n    pathsToLink = [ \"/bin\" ];\n  };\n  \n  config = {\n    Cmd = [ \"${pkgs.bash}/bin/bash\" ];\n    WorkingDir = \"/work\";\n  };\n}\nBuild and load it:\n# Build the image (outputs a .tar.gz file)\nnix-build docker-image.nix\n\n# Load into Docker\ndocker load &lt; result\n\n# Run it\ndocker run -it --rm my-r-env:latest\n\n\n7.6.3 Using rix with dockerTools\nThe previous example manually defines the R environment in Nix. If you prefer to use {rix} to generate your environment (as we’ve done throughout this book), you can import the generated default.nix into your Docker image definition.\nFirst, create your environment with {rix} as usual:\n\nlibrary(rix)\n\nrix(\n  date = \"2025-10-14\",\n  r_pkgs = c(\"dplyr\", \"ggplot2\", \"quarto\"),\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\n\nThis generates a default.nix. Now create a docker-image.nix that imports it:\n# docker-image.nix\nlet\n  pkgs = import (fetchTarball\n    \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-10-14.tar.gz\"\n  ) {};\n  \n  # Import the shell environment generated by rix\n  rixEnv = import ./default.nix;\n  \nin pkgs.dockerTools.buildImage {\n  name = \"my-rix-project\";\n  tag = \"latest\";\n  \n  copyToRoot = pkgs.buildEnv {\n    name = \"image-root\";\n    paths = rixEnv.buildInputs ++ [ pkgs.coreutils pkgs.bash ];\n    pathsToLink = [ \"/bin\" ];\n  };\n  \n  config = {\n    Cmd = [ \"${pkgs.bash}/bin/bash\" ];\n    WorkingDir = \"/work\";\n  };\n}\nThis approach lets you keep your familiar {rix} workflow for defining environments while gaining the reproducibility benefits of dockerTools for image creation. Any changes to your gen-env.R → default.nix will automatically propagate to the Docker image on the next build.\n\n\n7.6.4 Benefits Over Dockerfiles\n\n\n\n\n\n\n\n\nAspect\nDockerfile\nNix dockerTools\n\n\n\n\nReproducibility\nImperative, non-deterministic\nDeclarative, deterministic\n\n\nLayer caching\nBased on command order\nBased on content hashes\n\n\nImage size\nOften includes unnecessary packages\nMinimal: only what you specify\n\n\nComposition\nCopy-paste between files\nReuse Nix expressions\n\n\n\n\n\n7.6.5 Layered Images for Efficiency\nFor faster CI builds, you can create layered images where the base environment is cached:\npkgs.dockerTools.buildLayeredImage {\n  name = \"my-analysis\";\n  tag = \"latest\";\n  \n  contents = [ myR pkgs.quarto ];\n  \n  config = {\n    Cmd = [ \"${myR}/bin/R\" \"--vanilla\" ];\n  };\n}\nbuildLayeredImage creates separate layers for each package, so unchanged dependencies are cached between builds.\n\n\n7.6.6 When to Use This Approach\n\nCI/CD pipelines: When you need reproducible image builds as part of automated workflows\nMinimal images: When image size matters and you want only what you need\nComplex environments: When the environment is already defined in Nix and you want to deploy it as a container\n\nFor simpler use cases, the “Nix inside Docker” approach from earlier sections may be more accessible.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#dockerising-a-rixpress-pipeline",
    "href": "07-docker.html#dockerising-a-rixpress-pipeline",
    "title": "7  Containerisation With Docker",
    "section": "7.7 Dockerising a rixpress Pipeline",
    "text": "7.7 Dockerising a rixpress Pipeline\nWe can package our entire {rixpress} project into a single Docker image. This image can then be run by anyone with Docker installed, regardless of their host operating system or whether they have Nix.\nAssume your project directory has:\n.\n├── data/\n│   └── mtcars.csv\n├── gen-env.R\n├── gen-pipeline.R\n├── functions.R\n├── functions.py\n├── default.nix\n└── pipeline.nix\n\n7.7.1 Step 1: The Dockerfile\nFROM nix-base:latest\n# Or: FROM your-username/nix-base:latest\n\nWORKDIR /app\n\n# Copy all project files\nCOPY . .\n\n# Build the pipeline during image build\nRUN nix-build pipeline.nix\n\n# Export results when container runs\nCOPY export-results.R .\nCMD [\"nix-shell\", \"--run\", \"Rscript export-results.R\"]\n\n\n7.7.2 Step 2: The Export Script\n\n# export-results.R\nlibrary(rixpress)\nlibrary(jsonlite)\n\noutput_dir &lt;- \"/output\"\ndir.create(output_dir, showWarnings = FALSE)\n\nmessage(\"Reading target 'mtcars_head'...\")\nfinal_data &lt;- rxp_read(\"mtcars_head\")\n\noutput_path &lt;- file.path(output_dir, \"mtcars_analysis_result.json\")\nwrite_json(final_data, output_path, pretty = TRUE)\n\nmessage(paste(\"Successfully exported result to\", output_path))\n\n\n\n7.7.3 Step 3: Build and Run\nBuild:\ndocker build -t my-reproducible-pipeline .\nRun to get results:\nmkdir -p ./output\n\ndocker run --rm --name my_pipeline_run \\\n  -v \"$(pwd)/output\":/output \\\n  my-reproducible-pipeline\nCheck your local output directory. You’ll find the mtcars_analysis_result.json file containing the exact, reproducible result of your pipeline.\nYou have successfully packaged a complex, polyglot pipeline into a simple, portable Docker image. This workflow combines the best of both worlds: Nix’s power for creating reproducible builds and Docker’s universal standard for distributing and running applications.\n\n\n7.7.4 Alternative: Using dockerTools for rixpress\nYou can also build your rixpress pipeline image purely with Nix, avoiding Dockerfiles entirely. This gives you fully deterministic image builds.\nCreate a docker-pipeline.nix:\n# docker-pipeline.nix\nlet\n  pkgs = import (fetchTarball\n    \"https://github.com/rstats-on-nix/nixpkgs/archive/2025-10-14.tar.gz\"\n  ) {};\n  \n  # Import your rix-generated environment\n  rixEnv = import ./default.nix;\n  \n  # Build the pipeline as a derivation\n  pipelineResult = pkgs.runCommand \"pipeline-result\" {\n    buildInputs = rixEnv.buildInputs;\n  } ''\n    mkdir -p $out\n    cd ${./. }\n    Rscript -e \"source('gen-pipeline.R')\"\n    # Copy results to output\n    cp -r _rixpress $out/\n  '';\n  \nin pkgs.dockerTools.buildImage {\n  name = \"my-rixpress-pipeline\";\n  tag = \"latest\";\n  \n  copyToRoot = pkgs.buildEnv {\n    name = \"image-root\";\n    paths = [ \n      rixEnv.buildInputs \n      pipelineResult \n      pkgs.coreutils \n      pkgs.bash \n    ];\n    pathsToLink = [ \"/bin\" \"/\" ];\n  };\n  \n  config = {\n    Cmd = [ \"${pkgs.bash}/bin/bash\" \"-c\" \"cp -r /pipeline-result/* /output/\" ];\n    WorkingDir = \"/work\";\n  };\n}\nBuild and run:\nnix-build docker-pipeline.nix\ndocker load &lt; result\ndocker run --rm -v \"$(pwd)/output\":/output my-rixpress-pipeline:latest\nThis approach bakes the pipeline results directly into the image during the Nix build phase. The resulting image is minimal and contains only the outputs, not the full R/Python environment needed to produce them.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#summary",
    "href": "07-docker.html#summary",
    "title": "7  Containerisation With Docker",
    "section": "7.8 Summary",
    "text": "7.8 Summary\nDocker and Nix complement each other:\n\nDocker provides a universal runtime and distribution mechanism\nNix provides bit-for-bit reproducible environment management\nTogether they enable truly reproducible, portable data products\n\nKey Docker concepts:\n\nImages are templates; containers are running instances\nContainers are ephemeral; use volumes for persistence\nUse registries (Docker Hub) to share images\nThe Rocker Project provides ready-made R images\n\nFor reproducibility:\n\nUse ubuntu:latest + Nix rather than pinning specific OS versions\nBuild a reusable nix-base image to share across projects\nPackage {rixpress} pipelines for distribution",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "07-docker.html#further-reading",
    "href": "07-docker.html#further-reading",
    "title": "7  Containerisation With Docker",
    "section": "7.9 Further Reading",
    "text": "7.9 Further Reading\n\nRunning Your R Script in Docker\nDocker & R Reproducibility\nR Docker Tutorial",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Containerisation With Docker</span>"
    ]
  },
  {
    "objectID": "08-monads.html",
    "href": "08-monads.html",
    "title": "8  Robust Pipelines with Monads",
    "section": "",
    "text": "8.1 The Problem: Decorated Functions Don’t Compose\nIn the previous chapters, you learned functional programming fundamentals and how to build reproducible pipelines with {rixpress}. Now we’ll add another layer of robustness: monads.\nMonads might sound abstract, but they solve concrete problems involving Logging (tracing what each step does without cluttering your functions), Error handling (letting failures propagate gracefully instead of crashing), and Composition (keeping functions composable even when they need to do “extra” work).\nBy the end of this chapter, you’ll know how to integrate {chronicler} and talvez into your pipelines for more robust, observable data workflows.\nSuppose you want your functions to provide logs. You might rewrite sqrt() like this:\nmy_sqrt &lt;- function(x, log = \"\") {\n  list(\n    result = sqrt(x),\n    log = c(log, paste0(\"Running sqrt with input \", x))\n  )\n}\n\nmy_log &lt;- function(x, log = \"\") {\n  list(\n    result = log(x),\n    log = c(log, paste0(\"Running log with input \", x))\n  )\n}\nThese functions now return lists with both the result and a log. But there’s a problem: they don’t compose:\n# This works:\n10 |&gt; sqrt() |&gt; log()\n\n# This fails:\n10 |&gt; my_sqrt() |&gt; my_log()\n# Error: non-numeric argument to mathematical function\nmy_log() expects a number, but my_sqrt() returns a list. We’ve broken composition.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#the-solution-function-factories-and-bind",
    "href": "08-monads.html#the-solution-function-factories-and-bind",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.2 The Solution: Function Factories and Bind",
    "text": "8.2 The Solution: Function Factories and Bind\nA monad provides two things:\n\nA function factory that decorates functions so they can provide additional output without rewriting their core logic\nA bind() function that makes these decorated functions compose\n\nHere’s a simple function factory for logging:\n\nlog_it &lt;- function(.f) {\n  fstring &lt;- deparse(substitute(.f))\n  \n  function(..., .log = NULL) {\n    list(\n      result = .f(...),\n      log = c(.log, paste0(\"Running \", fstring, \" with argument \", ...))\n    )\n  }\n}\n\n# Create decorated functions\nl_sqrt &lt;- log_it(sqrt)\nl_log &lt;- log_it(log)\n\nl_sqrt(10)\n#&gt; $result\n#&gt; [1] 3.162278\n#&gt; $log\n#&gt; [1] \"Running sqrt with argument 10\"\n\nNow we need bind() to make them compose:\n\nbind &lt;- function(.l, .f) {\n  .f(.l$result, .log = .l$log)\n}\n\n# Now they compose!\n10 |&gt;\n  l_sqrt() |&gt;\n  bind(l_log)\n\n#&gt; $result\n#&gt; [1] 1.151293\n#&gt; $log\n#&gt; [1] \"Running sqrt with argument 10\"\n#&gt; [2] \"Running log with argument 3.16227766016838\"\n\nThis pattern of a function factory plus bind() is the essence of a monad.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#the-chronicler-package",
    "href": "08-monads.html#the-chronicler-package",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.3 The chronicler Package",
    "text": "8.3 The chronicler Package\nThe {chronicler} package implements this pattern properly for R. It provides:\n\nrecord(): A function factory that decorates functions\nbind_record(): The bind operation\nAutomatic logging of all operations\n\n\nlibrary(chronicler)\n\n# Decorate functions\nr_sqrt &lt;- record(sqrt)\nr_exp &lt;- record(exp)\nr_mean &lt;- record(mean)\n\n# Compose them\nresult &lt;- 1:10 |&gt;\n  r_sqrt() |&gt;\n  bind_record(r_exp) |&gt;\n  bind_record(r_mean)\n\n# View the result\nresult$value\n#&gt; [1] 5.187899\n\n# View the log\nread_log(result)\n#&gt; [1] \"Complete log:\"\n#&gt; [2] \"✔ sqrt ran successfully\"\n#&gt; [3] \"✔ exp ran successfully\"\n#&gt; [4] \"✔ mean ran successfully\"\n\n\n\n\n\n\n\nGetting LLM assistance with {chronicler}\n\n\n\nIf the functional programming patterns in {chronicler} feel unfamiliar, remember that you can use pkgctx to generate LLM-ready context. The {chronicler} repository includes a .pkgctx.yaml file you can feed to your LLM. You can also generate your own:\nnix run github:b-rodrigues/pkgctx -- r github:b-rodrigues/chronicler &gt; chronicler.pkgctx.yaml\nWith this context, your LLM can help you refactor your existing functions to be monadic using record() and bind_record().",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#the-maybe-monad-handling-errors",
    "href": "08-monads.html#the-maybe-monad-handling-errors",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.4 The Maybe Monad: Handling Errors",
    "text": "8.4 The Maybe Monad: Handling Errors\nAnother common monad is Maybe, which handles computations that might fail. Instead of crashing, functions return either:\n\nJust(value) if successful\nNothing if something went wrong\n\nThe {chronicler} package uses this under the hood:\n\nr_sqrt &lt;- record(sqrt)\n\n# This works\nr_sqrt(16)\n#&gt; ✔ Value: Just\n#&gt; [1] 4\n\n# This fails gracefully\nr_sqrt(\"not a number\")\n#&gt; ✖ Value: Nothing\n\nWhen Nothing is passed to a decorated function, it immediately returns Nothing, the error propagates through the pipeline without crashing.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#monads-in-python-talvez",
    "href": "08-monads.html#monads-in-python-talvez",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.5 Monads in Python: talvez",
    "text": "8.5 Monads in Python: talvez\nThe same concepts exist in Python. The talvez package provides a Maybe monad:\nfrom talvez import maybe, just\n\n@maybe()\ndef parse_int(s: str) -&gt; int:\n    return int(s)\n\n@maybe(ensure=lambda x: x != 0)\ndef reciprocal(n: int) -&gt; float:\n    return 1 / n\n\n# Successful computation\nresult = (\n    parse_int(\"25\")\n      .bind(reciprocal)\n      .fmap(lambda x: x * 100)\n)\nprint(result)            # Just(4.0)\nprint(result.get_or(-1)) # 4.0\n\n# Failed computation\nbad = (\n    parse_int(\"not a number\")\n      .bind(reciprocal)\n      .fmap(lambda x: x * 100)\n)\nprint(bad)  # Nothing\n\n\n\n\n\n\nGetting LLM assistance with talvez\n\n\n\nJust like with {chronicler}, you can use pkgctx to help your LLM understand talvez. The repository includes a .pkgctx.yaml file, or you can generate one:\nnix run github:b-rodrigues/pkgctx -- python github:b-rodrigues/talvez &gt; talvez.pkgctx.yaml\nThis is particularly useful for learning how to chain monadic operations in Python correctly.\n\n\nThe key operations are:\n\nfmap(fn): Apply a pure function to the value inside the monad\nbind(fn): Apply a function that itself returns a monad",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#building-robust-pipelines",
    "href": "08-monads.html#building-robust-pipelines",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.6 Building Robust Pipelines",
    "text": "8.6 Building Robust Pipelines\nThe real power of monads becomes apparent when you combine them with pipeline orchestration. Consider a typical data pipeline:\n\n# Standard pipeline - fragile\nraw_data |&gt;\n  basic_cleaning() |&gt;\n  recodings() |&gt;\n  filter_arrivals() |&gt;\n  make_monthly() |&gt;\n  make_plot()\n\nIf recodings() fails halfway through, the entire pipeline crashes. You get an error message, but no information about what succeeded before the failure.\nNow imagine wrapping each function with record():\n\nlibrary(chronicler)\n\n# Robust pipeline with logging\nr_basic_cleaning &lt;- record(basic_cleaning)\nr_recodings &lt;- record(recodings)\nr_filter_arrivals &lt;- record(filter_arrivals)\nr_make_monthly &lt;- record(make_monthly)\nr_make_plot &lt;- record(make_plot)\n\nresult &lt;- raw_data |&gt;\n  r_basic_cleaning() |&gt;\n  bind_record(r_recodings) |&gt;\n  bind_record(r_filter_arrivals) |&gt;\n  bind_record(r_make_monthly) |&gt;\n  bind_record(r_make_plot)\n\n# Now you get:\n# - The result (or Nothing if any step failed)\n# - A complete log of which steps ran\n# - Exactly where and why it failed\nread_log(result)\n\nThis pattern transforms a fragile script into a robust, observable pipeline.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#integrating-with-rixpress",
    "href": "08-monads.html#integrating-with-rixpress",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.7 Integrating with rixpress",
    "text": "8.7 Integrating with rixpress\nYou can combine the power of {chronicler} with {rixpress} for even more robust pipelines. The key insight is that your user_functions can use record() internally:\n\n# functions.R\nlibrary(chronicler)\n\n# Create recorded versions of your functions\nr_basic_cleaning &lt;- record(function(data) {\n  data |&gt;\n    select(contains(\"TIME\"), contains(\"20\")) |&gt;\n    pivot_longer(cols = contains(\"20\"),\n                 names_to = \"date\",\n                 values_to = \"passengers\")\n})\n\nr_recodings &lt;- record(function(data) {\n  data |&gt;\n    mutate(tra_meas = fct_recode(tra_meas, ...)) |&gt;\n    mutate(passengers = as.numeric(passengers))\n})\n\n# Export a pipeline function that uses bind_record\nprocess_aviation_data &lt;- function(raw_data) {\n  raw_data |&gt;\n    r_basic_cleaning() |&gt;\n    bind_record(r_recodings)\n}\n\nThen in your {rixpress} pipeline:\n\nlibrary(rixpress)\n\nlist(\n  rxp_r_file(\n    name = avia_raw,\n    path = \"data/avia.tsv\",\n    read_function = readr::read_tsv\n  ),\n  \n  rxp_r(\n    name = processed_data,\n    expr = process_aviation_data(avia_raw),\n    user_functions = \"functions.R\"\n  ),\n  \n  # ... more steps\n) |&gt;\n  rxp_populate()\n\nThe {rixpress} derivation caches the result while {chronicler} provides logging and error handling within the step.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#python-integration-with-talvez",
    "href": "08-monads.html#python-integration-with-talvez",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.8 Python Integration with talvez",
    "text": "8.8 Python Integration with talvez\nThe same pattern works in Python pipelines:\n# functions.py\nfrom talvez import maybe, chain, just\n\n@maybe()\ndef basic_cleaning(df):\n    return df.dropna().reset_index(drop=True)\n\n@maybe()\ndef recodings(df):\n    df['category'] = df['category'].map(category_mapping)\n    return df\n\n@maybe()\ndef filter_arrivals(df):\n    return df[df['tra_meas'] == 'Arrivals']\n\ndef process_data(raw_df):\n    \"\"\"Returns Just(result) or Nothing with error handling.\"\"\"\n    return chain(\n        just(raw_df),\n        basic_cleaning,\n        recodings,\n        filter_arrivals\n    )\nThen in your {rixpress} pipeline:\n\nrxp_py(\n  name = processed_data,\n  expr = \"process_data(raw_data)\",\n  user_functions = \"functions.py\"\n)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "08-monads.html#summary",
    "href": "08-monads.html#summary",
    "title": "8  Robust Pipelines with Monads",
    "section": "8.9 Summary",
    "text": "8.9 Summary\nMonads add a layer of robustness to your pipelines:\n\nchronicler (record(), bind_record()): Logging and error handling for R\ntalvez (@maybe(), bind(), chain()): Error handling for Python\nJust/Nothing: Graceful failure propagation without crashes\nComposability preserved: Even with extra capabilities, functions still compose\n\nCombined with {rixpress}:\n\nNix provides hermetic, cached execution\nMonads provide logging and error handling\nTogether they give you robust, reproducible, observable pipelines\n\nThis completes our toolkit for reproducible data science: environments (Nix), functional code (FP), pipelines (rixpress), distribution (Docker), and robustness (monads).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Robust Pipelines with Monads</span>"
    ]
  },
  {
    "objectID": "09-testing.html",
    "href": "09-testing.html",
    "title": "9  Professional Workflows: Testing and Git",
    "section": "",
    "text": "9.1 Introduction\nI hope you are starting to see the pieces of our reproducible workflow coming together. We now have reproducible environments and pipelines with Nix, reproducible logic with functional programming and in the previous chapter, we explored monads as a way to make our functions even more robust by handling logging and errors in a principled way.\nThis brings us to two final, crucial questions: how do we prove that our functions actually do what we claim they do? And how do we manage the complexity of collaborating with others (and with AI) without breaking everything?\nThis chapter addresses both questions by introducing unit testing and advanced Git workflows. You will learn what unit tests are and why they are essential for reliable data analysis, how to write and run them in both R (with {testthat}) and Python (with pytest), and how to use LLMs to accelerate test writing while embracing your role as a code reviewer. We will also cover professional Git techniques like Trunk-Based Development and interactive staging, which are especially valuable when managing code generated by AI assistants.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Professional Workflows: Testing and Git</span>"
    ]
  },
  {
    "objectID": "09-testing.html#part-1-unit-testing",
    "href": "09-testing.html#part-1-unit-testing",
    "title": "9  Professional Workflows: Testing and Git",
    "section": "9.2 Part 1: Unit Testing",
    "text": "9.2 Part 1: Unit Testing\nThe answer to “how do we prove it works?” is unit testing. A unit test is a piece of code whose sole job is to check that another piece of code, a “unit”, works correctly. In our functional world, the “unit” is almost always a single function. This is why we spent so much time on FP in the previous chapter. Small, pure functions are not just easy to reason about; they are incredibly easy to test.\nWriting tests is your contract with your collaborators and your future self. It’s a formal promise that your function, calculate_mean_mpg(), given a specific input, will always produce a specific, correct output. It’s the safety net that catches bugs before they make it into your final analysis and the tool that gives you the confidence to refactor and improve your code without breaking it.\n\n9.2.1 The Philosophy of a Good Unit Test\nSo, what should we test? Writing good tests is a skill, but it revolves around answering a few key questions about your function. For any function you write, you should have tests that cover:\n\nThe “Happy Path”: does the function return the expected, correct value for a typical, valid input?\nBad Inputs: does the function fail gracefully or throw an informative error when given garbage input (e.g., a string instead of a number, a data frame with the wrong columns)?\nEdge Cases: how does the function handle tricky but valid inputs? For example, what happens if it receives an empty data frame, a vector with NA values, or a vector where all the numbers are the same?\n\nWriting tests forces you to think through these scenarios, and in doing so, almost always leads you to write more robust and well-designed functions.\n\n\n9.2.2 Unit Testing in Practice\nLet’s imagine we’ve written a simple helper function to normalise a numeric vector (i.e., scale it to have a mean of 0 and a standard deviation of 1). We’ll save this in a file named utils.R or utils.py.\nR version (utils.R):\nnormalize_vector &lt;- function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\nPython version (utils.py):\nimport numpy as np\n\ndef normalize_vector(x):\n  return (x - np.nanmean(x)) / np.nanstd(x)\nNow, let’s write tests for it.\n\n9.2.2.1 Testing in R with {testthat}\nIn R, the standard for unit testing is the {testthat} package. The convention is to create a tests/testthat/ directory in your project, and for a script utils.R, you would create a test file named test-utils.R.\nInside test-utils.R, we use the test_that() function to group related expectations.\n# In file: tests/testthat/test-utils.R\n\n# First, we need to load the function we want to test\nsource(\"../../utils.R\")\n\nlibrary(testthat)\n\ntest_that(\"Normalization works on a simple vector (the happy path)\", {\n  # 1. Setup: Create input and expected output\n  input_vector &lt;- c(10, 20, 30)\n  expected_output &lt;- c(-1, 0, 1)\n\n  # 2. Action: Run the function\n  actual_output &lt;- normalize_vector(input_vector)\n\n  # 3. Expectation: Check if the actual output matches the expected output\n  expect_equal(actual_output, expected_output)\n})\n\ntest_that(\"Normalization handles NA values correctly\", {\n  input_with_na &lt;- c(10, 20, 30, NA)\n  expected_output &lt;- c(-1, 0, 1, NA)\n\n  actual_output &lt;- normalize_vector(input_with_na)\n\n  # We need to use expect_equal because it knows how to compare NAs\n  expect_equal(actual_output, expected_output)\n})\nThe expect_equal() function checks for near-exact equality. {testthat} has many other expect_*() functions, like expect_error() to check that a function fails correctly, or expect_warning() to check for warnings.\n\n\n9.2.2.2 Testing in Python with pytest\nIn Python, the de facto standard is pytest. It’s incredibly simple and powerful. The convention is to create a tests/ directory, and your test files should be named test_*.py. Inside, you just write functions whose names start with test_ and use Python’s standard assert keyword.\n# In file: tests/test_utils.py\n\nimport numpy as np\nfrom utils import normalize_vector # Import our function\n\ndef test_normalize_vector_happy_path():\n    # 1. Setup\n    input_vector = np.array([10, 20, 30])\n    expected_output = np.array([-1.0, 0.0, 1.0])\n\n    # 2. Action\n    actual_output = normalize_vector(input_vector)\n\n    # 3. Expectation\n    # For floating point numbers, it's better to check for \"close enough\"\n    assert np.allclose(actual_output, expected_output)\n\ndef test_normalize_vector_with_nas():\n    input_with_na = np.array([10, 20, 30, np.nan])\n    expected_output = np.array([-1.0, 0.0, 1.0, np.nan])\n\n    actual_output = normalize_vector(input_with_na)\n\n    # `np.allclose` doesn't handle NaNs, but `np.testing.assert_allclose` does!\n    np.testing.assert_allclose(actual_output, expected_output)\nBecause this isn’t a package though (yet), but a simple project with scripts, you also need to create another file called pytest.ini, which will tell pytest where to find the tests:\n[pytest]\n# Discover tests in the tests/ directory\ntestpaths = tests/\n\n# Include default patterns and your hyphenated pattern\npython_files = test_*.py *_test.py test-*.py\n\n# Adds the root directory to the pythonpath, without this\n# it'll be impossible to import normalize_vector() from\n# utils.py\npythonpath = .\nTo run your tests, you simply navigate to your project’s root directory in the terminal and run the command pytest. It will automatically discover and run all your tests for you. That being said, you will get an error:\n&gt;       assert np.allclose(actual_output, expected_output)\nE       assert False\nE        +  where False = &lt;function allclose at 0x7f0e81c959f0&gt;(array([-1.22474487,  0.        ,  1.22474487]), array([-1.,  0.,  1.])    def test_normalize_vector_with_nas():\n        input_with_na = np.array([10, 20, 30, np.nan])\n        expected_output = np.array([-1.0, 0.0, 1.0, np.nan])\n\n        actual_output = normalize_vector(input_with_na)\n\n        # `np.allclose` doesn't handle NaNs, but `np.testing.assert_allclose` does!\n&gt;       np.testing.assert_allclose(actual_output, expected_output)\nE       AssertionError:\nE       Not equal to tolerance rtol=1e-07, atol=0\nE\nE       Mismatched elements: 2 / 4 (50%)\nE       Max absolute difference among violations: 0.22474487\nE       Max relative difference among violations: 0.22474487\nE        ACTUAL: array([-1.224745,  0.      ,  1.224745,       nan])\nE        DESIRED: array([-1.,  0.,  1., nan])\n\ntests/test_utils.py:25: AssertionError\n====================================================== short test summary info =======================================================\nFAILED tests/test_utils.py::test_normalize_vector_happy_path - assert False\nFAILED tests/test_utils.py::test_normalize_vector_with_nas - AssertionError:\n========================================================= 2 failed in 0.15s ==========================================================\nYou don’t encounter this issue in R and understanding why reveals how valuable unit tests can be. (Hint: how does the implementation of the sd function, which computes the standard deviation, differ between R and NumPy?)\n\n\n\n9.2.3 Testing as a Design Tool\nTesting can also help you with programming, by thinking about edge cases. For example, what happens if we try to normalise a vector where all the elements are the same?\nLet’s write a test for this edge case first.\npytest version:\n# tests/test_utils.py\ndef test_normalize_vector_with_zero_std():\n    input_vector = np.array([5, 5, 5, 5])\n    actual_output = normalize_vector(input_vector)\n    # The current function will return `[nan, nan, nan, nan]`\n    # Let's assert that we expect a vector of zeros instead.\n    assert np.allclose(actual_output, np.array([0, 0, 0, 0]))\nIf we run pytest now, this test will fail. Our test has just revealed a flaw in our function’s design. This process is a core part of Test-Driven Development (TDD): write a failing, but correct, test, then write the code to make it pass.\nLet’s improve our function:\n\nimport numpy as np\n\ndef normalize_vector(x):\n  std_dev = np.nanstd(x)\n  if std_dev == 0:\n    # If std is 0, all elements are the mean. Return a vector of zeros.\n    return np.zeros_like(x, dtype=float)\n  return (x - np.nanmean(x)) / std_dev\nNow, if we run pytest again, our new test will pass. We used testing not just to verify our code, but to actively make it more robust and thoughtful.\n\n\n9.2.4 The Modern Data Scientist’s Role: Reviewer and AI Collaborator\nIn the past, writing tests was often seen as a chore. Today, LLMs make this process very easy.\n\n9.2.4.1 Using LLMs to Write Tests\nLLMs are fantastic at writing unit tests. They are good at handling boilerplate code and thinking of edge cases. You can provide your function to an LLM and give it a prompt like this:\n\n“Here is my Python function normalize_vector. Please write three pytest unit tests for it. Include a test for the happy path with a simple array, a test for an array containing np.nan, and a test for the edge case where all elements in the array are identical.”\n\nThe LLM will likely generate high-quality test code that is very similar to what we wrote above.\n\n\n\n\n\n\nAll you need is context\n\n\n\nWhen using an LLM to generate tests, context is everything. If you are writing tests for functions that heavily use specific packages (like {dplyr} or pandas), providing the pkgctx output for those packages ensures the LLM writes idiomatic and correct tests.\nFor example, if you are testing a function that uses {rix}, feed the rix.pkgctx.yaml to the LLM so it knows exactly how to mock or assert the outputs of rix() functions. This is very useful particularly for packages that are not very well-known (or internal packages to your company that aren’t even public) or packages that have been updated very recently, as the LLMs training data cutoff might not include the latest versions of the package.\n\n\nThis is a massive productivity boost. However, this introduces a new, critical role for the data scientist: you are the reviewer.\nAn LLM does not write your tests; it generates a draft. It is your professional responsibility to:\n\nRead and understand every line of the test code.\nVerify that the expected_output is actually correct.\nConfirm that the tests cover the cases you care about.\nCommit that code under your name, taking full ownership of it.\n\n“A COMPUTER CAN NEVER BE HELD ACCOUNTABLE THEREFORE A COMPUTER MUST NEVER MAKE A MANAGEMENT DECISION” - IBM Training Manual, 1979.\nThis principle is, in my opinion, even more important for tests than for production code. Even before LLMs, we relied on code we didn’t write ourselves. At my first job, my boss insisted that I avoid external packages entirely and rewrite everything from scratch. I ignored her and continued using the packages I trusted. At some point, you have to trust strangers. That was true then, and it is true now, except that “strangers” now includes LLMs and developers who themselves rely heavily on LLMs.\nBut here is the key difference: tests are small. Unlike a sprawling codebase, a unit test is short enough to read and understand completely. When you review an LLM-generated test, you can verify that the expected output is correct and that the test covers the right cases. This makes tests uniquely suited to the “trust but verify” workflow that AI-assisted development demands.\nIn the next chapter, we will learn the very basics of packaging, which makes testing even easier.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Professional Workflows: Testing and Git</span>"
    ]
  },
  {
    "objectID": "09-testing.html#part-2-advanced-version-control",
    "href": "09-testing.html#part-2-advanced-version-control",
    "title": "9  Professional Workflows: Testing and Git",
    "section": "9.3 Part 2: Advanced Version Control",
    "text": "9.3 Part 2: Advanced Version Control\nI assume you already know the basics of Git: clone, add, commit, and push. If you don’t, there are countless tutorials available, and I highly recommend you check one out and then come back to this.\nIn this section, we will focus on the techniques that separate the “I submit code via Dropbox” novice from the professional data scientist: Trunk-Based Development and managing AI-generated code.\n\n9.3.1 A Better Way to Collaborate: Trunk-Based Development\nA common mistake for new teams is to use branches to add new features, and then let these feature branches live for a very long time. A data scientist might create a branch called feature-big-analysis, work on it for three weeks, and then try to merge it back into main. The result is often what’s called “merge hell”: main has changed so much in three weeks that merging the branch back in creates dozens of conflicts and is a painful, stressful process.\nTo avoid this, many professional teams use a workflow called Trunk-Based Development (TBD). The philosophy is simple but powerful:\n\nAll developers integrate their work back into the main branch (the “trunk”) as frequently as possible, at least once a day.\n\nThis means that feature branches are incredibly short-lived. Instead of a single, massive feature branch that takes weeks, you create many tiny branches that each take a few hours or a day at most.\n\n9.3.1.1 How to Work with Short-Lived Branches\nBut how can you merge something back into main if the feature isn’t finished? The main branch must always be stable and runnable. You can’t merge broken code.\nThe first way to solve this issue is to use feature flags.\nA feature flag is just a simple variable (like a TRUE/FALSE switch) that lets you turn a new, unfinished part of the code on or off. This allows you to merge the code into main while keeping it “off” until it’s ready.\nImagine you are adding a new, complex plot to analysis.R, but it will take a few days to get right.\n# At the top of your analysis.R script\n# --- Configuration ---\nuse_new_scatterplot &lt;- FALSE # Set to FALSE while in development\n\n# ... lots of existing, working code ...\n\n# --- New Feature Code ---\nif (use_new_scatterplot) {\n  # All your new, unfinished, possibly-buggy plotting code goes here.\n  # It won't run as long as the flag is FALSE.\n  library(scatterplot3d)\n  scatterplot3d(mtcars$mpg, mtcars$hp, mtcars$wt)\n}\nWith this if block, you can safely merge your changes into main. The new code is there, but it won’t execute and won’t break the existing analysis. Other developers can pull your changes and won’t even notice. Once you’ve finished the feature in subsequent small commits, the final change is just to flip the switch: use_new_scatterplot &lt;- TRUE.\n\n\n\n9.3.2 Collaborative Hygiene: Rebase vs. Merge\nWhen you and a colleague both work on main, your histories can diverge. When you git pull, Git behaves in two ways:\n\nMerge (Default): creates a “merge commit” that ties the histories together. This results in a messy, non-linear history graph if done frequently.\nRebase (Recommended): temporarily lifts your commits, pulls your colleague’s changes, and then replays your commits on top.\n\nIn data science projects, where identifying when a plot or number changed is critical, a linear history is valuable. I strongly recommend configuring Git to use rebase by default:\ngit config --global pull.rebase true\nNow, when you git pull, your local changes will stay at the “tip” of the history, keeping the log clean.\n\n\n9.3.3 Working with LLMs and Git: Managing AI-Generated Changes\nWhen working with LLMs like GitHub Copilot, it’s crucial to review changes carefully before committing them. Git provides a powerful tool for this: interactive staging.\n\n9.3.3.1 Interactive Staging: Accepting changes chunk by chunk\nGit’s interactive staging feature (git add -p) is perfect for reviewing LLM changes. Instead of blindly adding all changes with git add ., git add -p lets you review each “hunk” (chunk of changes) individually.\nSuppose an LLM refactored your Python script. You run:\ngit add -p analysis.py\nGit will show you a chunk of changes and prompt you:\n@@ -1,2 +1,4 @@\n # Load required libraries\n import pandas as pd\n+import matplotlib.pyplot as plt\n+import seaborn as sns\nStage this hunk [y,n,q,a,d,s,e,?]?\nYou can reply:\n\ny: Yes, stage this (I approve).\nn: No, do not stage this (I reject or want to edit later).\ns: Split this hunk into smaller pieces (if the LLM did two unrelated things in one block).\ne: Edit the hunk manually before staging.\n\nThis forces you to be the reviewer. It prevents “accidental” AI hallucinations (like deleting a critical import) from slipping into your repository.\n\n\n9.3.3.2 Example LLM Workflow\n\nSave state: git commit -m \"Working state before LLM\"\nPrompt LLM: “Please refactor this function to be more efficient.”\nReview: Run git diff to see what it did.\nSelect: Run git add -p and say y only to the parts that look correct.\nClean: Run git checkout . to discard the parts you rejected.\nVerify: Run your unit tests!\n\nThis workflow ensures you maintain full control over your codebase while benefiting from LLM assistance.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Professional Workflows: Testing and Git</span>"
    ]
  },
  {
    "objectID": "09-testing.html#summary",
    "href": "09-testing.html#summary",
    "title": "9  Professional Workflows: Testing and Git",
    "section": "9.4 Summary",
    "text": "9.4 Summary\nIn this chapter, we covered the safety protocols of professional software engineering:\n\nUnit tests prove your code works and protect you from regressions.\nGit workflows (Trunk-Based Development, Rebasing) keep your collaboration clean and history linear.\nCode review (via Interactive Staging) is your primary defence against AI-generated bugs.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Professional Workflows: Testing and Git</span>"
    ]
  },
  {
    "objectID": "10-packaging.html",
    "href": "10-packaging.html",
    "title": "10  A Short Intro to Packaging Your Code in R and Python",
    "section": "",
    "text": "10.1 Introduction\nIn the previous chapter, we learned how to prove that our code works through unit testing and how to manage our collaboration with Git. We now have functions, tests, and version control. But there is one more step we can take to truly professionalise our workflow: packaging.\nYou might think, “I’m a data scientist, not a software engineer. Isn’t this overkill?” The answer is a definitive no. Packaging your code, even for an internal analysis project, provides enormous benefits. Instead of copying and pasting your clean_data() function from project to project, you can simply import mypackage or library(mypackage) and use a single, trusted version. Sharing your work with a colleague becomes as simple as sending them a single command rather than emailing a zip file of scripts. Packaging also forces you into a standardised way of documenting your functions, provides a formal framework for running unit tests, and explicitly declares all of your dependencies.\nThis chapter will walk you through the process of creating a simple package in both R and Python. You will learn how to create, document, and test a basic R package using {devtools} and {usethis}, how to do the same for a modern Python package using uv and pytest, and how to install your own packages directly from GitHub so you can share your tools with colleagues and your future self. The goal is not to become an expert package developer, but to understand the structure and benefits so you can apply this powerful “packaging mindset” to all your future projects.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>A Short Intro to Packaging Your Code in R and Python</span>"
    ]
  },
  {
    "objectID": "10-packaging.html#part-1-creating-an-r-package-with-usethis-and-devtools",
    "href": "10-packaging.html#part-1-creating-an-r-package-with-usethis-and-devtools",
    "title": "10  A Short Intro to Packaging Your Code in R and Python",
    "section": "10.2 Part 1: Creating an R Package with {usethis} and {devtools}",
    "text": "10.2 Part 1: Creating an R Package with {usethis} and {devtools}\nThe R community has developed an outstanding set of tools that make package development incredibly streamlined. The two essential packages are:\n\n{devtools}: provides core development tools like install(), test(), and check().\n{usethis}: a workflow package that automates all the boilerplate. It creates files, sets up infrastructure, and guides you through the process.\n\nLet’s build a package called cleanR, which will contain a function to standardise column names. Create a folder called cleanR and cd into it.\n\n10.2.1 Step 1: Project Setup\nFirst, make sure you have the necessary tools installed. Create a Nix environment that contains the following R packages: {devtools}, {usethis} and {roxygen2}.\nDrop into this Nix shell, and let {usethis} create the package structure for you. From your R console, run:\nusethis::create_package(\"~/Documents/projects/cleanR\")\nThis will create a new cleanR directory with all the necessary files and subdirectories. It will also open a new RStudio session for that project. The key components are:\n\nR/: this is where your R source code files will live.\nDESCRIPTION: a metadata file describing your package, its author, licence, and dependencies.\nNAMESPACE: a file that declares which functions your package exports for users and which functions it imports from other packages. You should never edit this file by hand. {roxygen2} will manage it for you.\n\n\n\n10.2.2 Step 2: Write and Document a Function\nLet’s create our function. {usethis} helps with this too:\nusethis::use_r(\"clean_names\")\nThis creates a new file R/clean_names.R and opens it for editing. Let’s add our function, including special comments for documentation. These #' comments are used by the {roxygen2} package to automatically generate the official documentation.\n# In R/clean_names.R\n\n#' Clean and Standardize Column Names\n#'\n#' This function takes a data frame and returns a new data frame with\n#' cleaned-up column names (lowercase, with underscores instead of spaces\n#' or periods).\n#'\n#' @param df A data frame.\n#' @return A data frame with standardized column names.\n#' @export\n#' @examples\n#' messy_df &lt;- data.frame(\"First Name\" = c(\"Ada\", \"Bob\"), \"Last.Name\" = c(\"Lovelace\", \"Ross\"))\n#' clean_names(messy_df)\nclean_names &lt;- function(df) {\n  old_names &lt;- names(df)\n  new_names &lt;- tolower(old_names)\n  new_names &lt;- gsub(\"[ .]\", \"_\", new_names)\n  names(df) &lt;- new_names\n  return(df)\n}\nThe key tags here are:\n\n@param: describes a function argument.\n@return: describes what the function returns.\n@export: this is crucial. It tells R that you want this function to be available to users when they load your package with library(cleanR).\n@examples: provides runnable examples that will appear in the help file.\n\nNow, run the magic command to process these comments:\ndevtools::document()\nThis updates the NAMESPACE file and creates the help file (man/clean_names.Rd). You can now see your function’s help page with ?clean_names.\n\n\n10.2.3 Step 3: Add Unit Tests\nA package without tests is a package waiting to break. {usethis} makes setting up tests trivial.\nusethis::use_testthat() # Sets up the tests/testthat/ directory\nusethis::use_test(\"clean_names\") # Creates tests/testthat/test-clean_names.R\nNow, edit the test file to add your expectations.\n# In tests/testthat/test-clean_names.R\ntest_that(\"clean_names works with spaces and periods\", {\n  messy_df &lt;- data.frame(\"First Name\" = c(\"A\"), \"Last.Name\" = c(\"B\"))\n  cleaned_df &lt;- clean_names(messy_df)\n\n  expected_names &lt;- c(\"first_name\", \"last_name\")\n\n  expect_equal(names(cleaned_df), expected_names)\n})\n\ntest_that(\"clean_names handles already clean names\", {\n  clean_df &lt;- data.frame(a = 1, b = 2)\n  # The function should not change anything\n  expect_equal(names(clean_names(clean_df)), c(\"a\", \"b\"))\n})\nTo run all the tests for your package, use:\ndevtools::test()\n\n\n10.2.4 Step 4: Check and Install\nThe final step before sharing is to run the official R CMD check, the gold standard for package quality. This command runs all tests, checks documentation, and looks for common problems.\ndevtools::check()\nIf your package passes with 0 errors, 0 warnings, and 0 notes, you are in great shape. If your package raises NOTEs or WARNINGs during the check phase, you can most of the time safely ignore these, especially if the package is only intended for internal usage. However, I would recommend that you still take care of the WARNINGs at the very least.\nAs a next step, you could edit the DESCRIPTION file. This is where you will list yourself as the package author, list the dependencies of the package and so on. I won’t got into detail here, but learning how to edit the DESCRIPTION file is important for actual package development (especially listing dependencies is key).\nTo use your package within a project, the simplest way is to host it on GitHub or build a .tar.gz file and install it locally.\n\n\n10.2.5 Step 5: Install from GitHub\nIf you can publicly host your package, hosting it on GitHub is a good way to easily share your code, and install the package in your projects without needing to publish it on CRAN.\n\nCreate a new, empty repository on GitHub (e.g., cleanR).\nIn your local project, follow the instructions GitHub provides to link your local repository and push your code. This usually involves commands like:\ngit remote add origin git@github.com:yourusername/cleanR.git\ngit branch -M main\ngit push -u origin main\nNow, anyone (including you on a different machine) can install your package with a single command (if you don’t use Nix):\n# You might need to install {remotes} first\n# install.packages(\"remotes\")\nremotes::install_github(\"yourusername/cleanR\")\n\nIf you want to create an environment using Nix that includes this package, use the git_pkgs argument of rix::rix() to generate the right default.nix file.\nCongratulations, you have created and shared a fully functional R package!\n\n\n10.2.6 Step 5bis: Install it locally\nIf you can’t share your package on GitHub, the alternative is to build it locally using:\ndevtools::build()\nwhich will create a .tar.gz package. You can then install it using either devtools::install_local() if you don’t use Nix or the local_r_pkgs argument of rix::rix().",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>A Short Intro to Packaging Your Code in R and Python</span>"
    ]
  },
  {
    "objectID": "10-packaging.html#part-2-creating-a-minimal-python-package-with-uv",
    "href": "10-packaging.html#part-2-creating-a-minimal-python-package-with-uv",
    "title": "10  A Short Intro to Packaging Your Code in R and Python",
    "section": "10.3 Part 2: Creating a Minimal Python Package with uv",
    "text": "10.3 Part 2: Creating a Minimal Python Package with uv\nThere are many different ways to build packages in Python, and what I propose here is just one way to do it. While we will use Nix to manage our overall environment, we still need to define the metadata and structure for our Python package. We will use uv, an extremely fast and modern tool, for one specific purpose: initialising our project’s configuration file. We will not use uv to manage a virtual environment, as Nix already handles that for us (unless you absolutely want to: however, you should then make sure that uv itself is being managed by Nix to ensure reproducibility).\nLet’s build a Python package called pyclean, the equivalent of our R package.\n\n10.3.1 Step 1: Project Setup with uv\nFirst, ensure uv is installed in your Nix environment:\nlibrary(rix)\n\nrix(\n  date = \"2025-10-07\",\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"pytest\", \"pandas\")\n  ),\n  system_pkgs = \"uv\",\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\nThen, create a directory for your new package and initialise it:\nmkdir pyclean\ncd pyclean\nuv init --bare\nThe --bare flag is perfect for our Nix workflow. It creates only the essential pyproject.toml file without creating a virtual environment or extra directories. This leaves us with a clean slate.\nNow, we must create the source and test directories manually. We’ll use the standard src layout:\nmkdir -p src/pyclean\nmkdir tests\ntouch src/pyclean/__init__.py\nYour project structure should now look like this (check it using the tree command):\npyclean/\n├── pyproject.toml\n├── src/\n│   └── pyclean/\n│       └── __init__.py\n└── tests/\n\n\n10.3.2 Step 2: Write a Function and Declare Dependencies\nLet’s create our clean_names function inside a new file, src/pyclean/formatters.py.\n# In src/pyclean/formatters.py\nimport pandas as pd\n\ndef clean_names(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Clean and standardize column names of a DataFrame.\n\n    Args:\n        df: The input pandas DataFrame.\n\n    Returns:\n        A pandas DataFrame with standardized column names.\n    \"\"\"\n    new_df = df.copy()\n    new_cols = {col: col.lower().replace(\" \", \"_\").replace(\".\", \"_\") for col in new_df.columns}\n    new_df = new_df.rename(columns=new_cols)\n    return new_df\nTo make this function easily importable, we expose it in src/pyclean/__init__.py:\n# In src/pyclean/__init__.py\nfrom .formatters import clean_names\n\n__all__ = [\"clean_names\"]\nNext, we must declare our dependencies by manually editing pyproject.toml. We need pandas for our function and pytest for our tests.\n# In pyproject.toml\n[project]\nname = \"pyclean\"\nversion = \"0.1.0\"\ndescription = \"A simple package to clean data.\"\ndependencies = [\n    \"pandas&gt;=2.0.0\",\n]\n\n[project.optional-dependencies]\ntest = [\n    \"pytest\",\n]\n\n[tool.pytest.ini_options]\npythonpath = [\n  \"src\"\n]\nThe pythonpath = [\"src\"] line is very important. Without it, you’d first need to install your pyclean library in editable mode using pip before running the tests. By adding this block, simply running pytest from the command line will work.\n\n\n10.3.3 Step 3: Add Unit Tests\nCreate a new test file, tests/test_formatters.py, and add your tests.\n# In tests/test_formatters.py\nimport pandas as pd\nfrom pyclean import clean_names\n\ndef test_clean_names_happy_path():\n    messy_df = pd.DataFrame({\"First Name\": [\"Ada\"], \"Last.Name\": [\"Lovelace\"]})\n    cleaned_df = clean_names(messy_df)\n    expected_cols = [\"first_name\", \"last_name\"]\n    assert list(cleaned_df.columns) == expected_cols\n\ndef test_clean_names_is_idempotent():\n    clean_df = pd.DataFrame({\"first_name\": [\"a\"], \"last_name\": [\"b\"]})\n    still_clean_df = clean_names(clean_df)\n    assert list(still_clean_df.columns) == list(clean_df.columns)\nSince your Nix environment provides all the tools, you can run tests directly from your terminal:\npytest\n\n\n10.3.4 Step 4: Build and Install\nTo package your code, you need a build tool. It turns out that uv bundles a build tool with it, so we only need to call uv build:\n# In your terminal, from the root of the 'pyclean' project\nuv build\nThis creates a dist/ directory containing a source distribution (.tar.gz) and a compiled wheel (.whl). The wheel is the modern standard for distribution.\nOutside of a Nix shell, to use your package during development, you can install it in “editable” mode. This creates a link to your source code, so any changes you make are immediately reflected without needing to reinstall.\n# Install the package and its test dependencies\npip install -e .[test]\nBut we are working from a Nix shell. Instead, we will simply edit our default.nix to update the PYTHONPATH environment variable, so our package can easily be found:\nshellHook = ''\n  export PYTHONPATH=$PWD/src:$PYTHONPATH\n'';\nTo generate this with {rix}, use the shell_hook argument:\nlibrary(rix)\n\nrix(\n  date = \"2025-10-07\",\n  py_conf = list(\n    py_version = \"3.13\",\n    py_pkgs = c(\"pytest\", \"pandas\")\n  ),\n  system_pkgs = \"uv\",\n  shell_hook = \"export PYTHONPATH=$PWD/src:$PYTHONPATH\",\n  ide = \"none\",\n  project_path = \".\",\n  overwrite = TRUE\n)\nWith this, dropping into the Nix shell, starting the Python interpreter and then typing import pyclean will work without any issues. You may need to adapt the path depending on where you’re developing the package.\n\n\n10.3.5 Step 5: Install from GitHub\nSharing via GitHub is the most common way to distribute packages that aren’t on the official Python Package Index (PyPI):\n\nCreate a new, empty repository on GitHub.\nPush your local project to the remote repository.\nNow, anyone can install your package directly from GitHub using pip, which is smart enough to find and process your pyproject.toml file: bash     pip install git+https://github.com/yourusername/pyclean.git\n\nFor Nix environments, add this to your default.nix:\npyclean = pkgs.python313Packages.buildPythonPackage rec {\n  pname = \"pyclean\";\n  version = \"0.1.0\";\n  src = pkgs.fetchgit {\n    url = \"https://github.com/b-rodrigues/pyclean\";\n    rev = \"174d4d482d400536bb0d987a3e25ae80cd81ef3c\";\n    sha256 = \"sha256-xTYydkuduPpZsCXE2fv5qZCnYYCRoNFpV7lQBM3LMSg=\";\n  };\n  pyproject = true;\n  propagatedBuildInputs = [ pkgs.python313Packages.pandas pkgs.python313Packages.setuptools ];\n  # Add more dependencies to propagatedBuildInputs as needed\n};\nYou need to add the rev, which corresponds to the commit that want, and the sha256. To find the right sha256, start with an empty one (sha256 = \"\";) and try to build the package. The error message will give you the right sha256. Also not that this isn’t the the most idiomatic way to build a Python package for Nix, but it’s good enough for our purposes.\nFinally, add pyclean to the buildInputs of the shell:\n  buildInputs = [ rpkgs pyconf pyclean tex system_packages github_pkgs ];\nThis process is naturally more involved than simply calling pip install, but it has the advantage of being entirely reproducible.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>A Short Intro to Packaging Your Code in R and Python</span>"
    ]
  },
  {
    "objectID": "10-packaging.html#conclusion-the-packaging-mindset-in-the-age-of-ai",
    "href": "10-packaging.html#conclusion-the-packaging-mindset-in-the-age-of-ai",
    "title": "10  A Short Intro to Packaging Your Code in R and Python",
    "section": "10.4 Conclusion: The Packaging Mindset in the Age of AI",
    "text": "10.4 Conclusion: The Packaging Mindset in the Age of AI\nYou have now successfully created, tested, documented, and shared a basic package in both R and Python. While there is much more to learn about advanced package development, you have already mastered the most important part: the packaging mindset.\nFrom now on, when you start a new analysis project, think of it as a small, internal package.\n\nPut your reusable logic into functions.\nPlace those functions in the R/ or mypackage/ source directory.\nDocument them.\nWrite a few simple tests to prove they work.\nManage dependencies formally in DESCRIPTION or pyproject.toml.\n\nAdopting this structure will make your work more robust, easier to share, and fundamentally more reproducible. It is the bridge between writing one-off scripts and building reliable, professional data science tools.\nThis packaging mindset becomes even more powerful when you introduce a modern collaborator: the LLM. The structured, component-based nature of a package is the perfect way to interact with AI assistants.\nA package provides a clear contract and a well-defined structure that LLMs thrive on. Instead of a vague prompt like, “Refactor my messy analysis script,” you can now make precise, targeted requests:\n\n“Here is my function clean_names. Please write three pytest unit tests for it, including one for the happy path, one for an empty DataFrame, and one for names that are already clean.”\n“Generate the roxygen2 documentation skeleton for this R function, including @param, @return, and @examples tags.”\n“I need a function in my pyclean/utils.py module that calculates the Z-score for a pandas Series. Please generate the function and its docstring.”\n\nThis synergy is a two-way street. Not only does the structure help you write better prompts, but LLMs excel at generating the very boilerplate that makes packaging robust. Tedious tasks like writing standard documentation headers, creating skeleton unit test files, or even generating a first draft of a function based on a clear description become near-instantaneous.\nThis elevates your role from a writer of code to an architect and a reviewer. Your job is to design the components (the functions), prompt the LLM to generate the implementation, and then, most critically, use the testing framework you just built to rigorously verify that the AI-generated code is correct, efficient, and robust. You are the final authority, and the package structure gives you the tools to enforce quality control.\nBy combining the discipline of packaging with the power of LLMs, you lower the barrier to adopting best practices like comprehensive testing and documentation. This combination doesn’t just make you faster; it makes you a more reliable and professional data scientist, capable of producing tools that are truly reproducible and built to last.\nWhile a full guide to package development is beyond the scope of this course, it is the natural next step in your journey as a data scientist who produces reliable tools. When you are ready to take that step, here are the definitive resources to guide you:\n\nFor R: the “R Packages” (2e) book by Hadley Wickham and Jennifer Bryan is the essential, comprehensive guide. It covers everything from initial setup with {usethis} to testing, documentation, and submission to CRAN. Read it online here.\nFor Python: the official Python Packaging User Guide is the place to start. For a more modern and streamlined approach that handles dependency management and publishing, many developers use tools like Poetry or Hatch.\n\nTreating your data analysis project like a small, internal software package, complete with functions and tests, is a powerful mindset that will elevate the quality and reliability of your work.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>A Short Intro to Packaging Your Code in R and Python</span>"
    ]
  },
  {
    "objectID": "11-ci.html",
    "href": "11-ci.html",
    "title": "11  Continuous Integration with GitHub Actions",
    "section": "",
    "text": "11.1 Introduction\nWe are almost at the end of our journey. In the previous chapters, we built reproducible environments with Nix, organised our code into pure functions, added robustness with monads, proved correctness with unit tests, managed our collaboration with Git, and bundled everything into shareable packages. We can now run our pipelines in a 100% reproducible way.\nHowever, all of this still requires manual steps. And maybe that’s not a problem; if your environment is set up and users only need to drop into a Nix shell and run the pipeline, that’s already a huge improvement. But you should keep in mind that manual steps don’t scale. Imagine you are part of a team that needs to quickly ship products to clients. Several people contribute to the product, and you might need to work on multiple projects in the same day. You and your teammates should be focusing on writing code, not on repetitive tasks like building images or running tests. Ideally, we would want to automate these steps. That is what we are going to learn in this chapter.\nThis chapter will introduce you to Continuous Integration (CI) with GitHub Actions. You will learn how to set up workflows that automatically run your tests when you push code, how to build Docker images and recover artifacts, and how to run your pipelines directly from GitHub’s servers. Because we’re using Git to trigger all the events and automate the whole pipeline, this approach is sometimes called GitOps.\nYou may have heard the term “CI/CD,” where CD stands for Continuous Deployment or Continuous Delivery. We will focus on CI in this chapter. Continuous Deployment (automatically pushing results to a database, dashboard, or API) is highly specific to your organisation and infrastructure. What we cover here, however, gives you the foundation: once your pipeline runs reliably on CI, the “deployment” step is just one more workflow job pointing to wherever your results need to go.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Integration with GitHub Actions</span>"
    ]
  },
  {
    "objectID": "11-ci.html#getting-your-repo-ready-for-github-actions",
    "href": "11-ci.html#getting-your-repo-ready-for-github-actions",
    "title": "11  Continuous Integration with GitHub Actions",
    "section": "11.2 Getting your repo ready for Github Actions",
    "text": "11.2 Getting your repo ready for Github Actions\nObviously, you should use a project that is versioned on GitHub. Use the package we’ve developed previously. If you go on its GitHub page, you should see an “Actions” tab on top:\n\n\n\n\n\n\n\n\n\n\n\nThis will open a new view where you can select a lot of available, ready to use actions. “Actions” are premade scripts that execute some commands you might need: such as setting up R, Python, running tests, etc. Since we’re using Nix, we don’t really need to look for any actions to set up our environments. However, we might want to use some pre-made actions to upload artifacts for instance.\nTo actually configure our repository to run actions, we need to edit a file in our project under the .github/workflows directory (create them if needed). In it, write a yaml file called hello.yaml and write the following in it:\nname: Hello world\non: [push]\njobs:\n  say-hello:\n    runs-on: ubuntu-latest\n    steps:\n      - run: echo \"Hello from Github Actions!\"\n      - run: echo \"This command is running from an Ubuntu VM each time you push.\"\nLet’s study this workflow definition line by line:\nname: Hello world\nSimply gives a name to the workflow.\non: [push]\nWhen should this workflow be triggered? Here, whenever something gets pushed.\njobs:\nWhat is the actual things that should happen? This defines a list of actions.\n  say-hello:\nThis defines the say-hello job.\n    runs-on: ubuntu-latest\nThis job should run on an Ubuntu VM. You can also run jobs on Windows or macOS VMs, but this uses more compute minutes than a Linux VM (which doesn’t matter for public projects. For private projects, the amount of compute minutes is limited).\n    steps:\nWhat are the different steps of the job?\n      - run: echo \"Hello from Github Actions!\"\nFirst, run the command echo \"Hello from Github Actions!\". This commands runs inside the VM. Then, run this next command:\n      - run: echo \"This command is running from an Ubuntu VM each time you push.\"\nIf we take a look at the commit we just pushed, on GitHub, we see this yellow dot next to the commit name. This means that an action is running. We can then take a look at the output of the job, and see that our commands, defined with the run statements in the workflow file, succeeded and echoed what we asked them.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Integration with GitHub Actions</span>"
    ]
  },
  {
    "objectID": "11-ci.html#nix-and-github-actions",
    "href": "11-ci.html#nix-and-github-actions",
    "title": "11  Continuous Integration with GitHub Actions",
    "section": "11.3 Nix and GitHub Actions",
    "text": "11.3 Nix and GitHub Actions\nTo set up Nix on GitHub Actions you can use several steps (create a new file called run-tests.yaml):\n- name: Install Nix\n  uses: cachix/install-nix-action@v31\n  with:\n    nix_path: nixpkgs=https://github.com/rstats-on-nix/nixpkgs/archive/r-daily.tar.gz\n\n- name: Setup Cachix\n  uses: cachix/cachix-action@v15\n  with:\n    name: rstats-on-nix\nIf you’re repository contains a default.nix file, the same environment you’ve been using locally can be used on GitHub Actions just as easily. You can also instead generate the default.nix from the gen-env.R script:\n- name: Build dev env\n  run: |\n    nix-shell --expr \"$(curl -sl https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix)\" --run \"Rscript -e 'source(\\\"gen-env.R\\\")'\"\nYou can then use the shell to run whatever you need. For example, if you’re developing a package, you could run unit tests on each push:\n- name: devtools::test() via nix-shell\n  run: nix-shell --run \"Rscript -e \\\"devtools::test(stop_on_failure = TRUE)\\\"\"\nstop_on_failure = TRUE is needed to make the step fail if there’s an error, otherwise, the step would run successfully, even with failing tests.\nOf course, if you’re developing a Python package, use nix-shell --run \"pytest\" instead to run the tests.\nI highly recommend you run tests when pull requests get opened:\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\nThis will ensure that if someone contributes to your project, you know immediately if what they did breaks tests or not. If it does, ask them to fix the code until tests pass.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Integration with GitHub Actions</span>"
    ]
  },
  {
    "objectID": "11-ci.html#running-a-dockerized-workflow",
    "href": "11-ci.html#running-a-dockerized-workflow",
    "title": "11  Continuous Integration with GitHub Actions",
    "section": "11.4 Running a dockerized workflow",
    "text": "11.4 Running a dockerized workflow\nThis next example can be found in this repository. This example doesn’t use Nix, {rix} nor {rixpress}, but the point here is to show how a Docker container can be executed on GitHub Actions, and artifacts can be recovered. The process is always the same, regardless is inside the Docker image. If you want to follow along, fork this repository.\nThis is what our workflow file looks like:\nname: Reproducible pipeline\n\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n\njobs:\n\n  build:\n\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v5\n    - name: Build the Docker image\n      run: docker build -t my-image-name .\n    - name: Docker Run Action\n      run: docker run --rm --name my_pipeline_container -v /github/workspace/fig/:/home/graphs/:rw my-image-name\n    - uses: actions/upload-artifact@v4\n      with:\n        name: my-figures\n        path: /github/workspace/fig/\nFor now, let’s focus on the run statements, because these should be familiar:\nrun: docker build -t my-image-name .\nand:\nrun: docker run --rm --name my_pipeline_container -v /github/workspace/fig/:/home/graphs/:rw my-image-name\nThe only new thing here, is that the path has been changed to /github/workspace/. This is the home directory of your repository, so to speak. Now there’s the uses keyword that’s new:\nuses: actions/checkout@v5\nThis action checkouts your repository inside the VM, so the files in the repo are available inside the VM. Then, there’s this action here:\n- uses: actions/upload-artifact@v4\n  with:\n    name: my-figures\n    path: /github/workspace/fig/\nThis action takes what’s inside /github/workspace/fig/ (which will be the output of our pipeline) and makes the contents available as so-called “artifacts”. Artifacts are the outputs of your workflow. In our case, as stated, the output of the pipeline. So let’s run this by pushing a change, and let’s take a look at these artifacts!\nAfter the action done running, you will be able to download a zip file containing the plots. It is thus possible to rerun our workflow in the cloud. This has the advantage that we can now focus on simply changing the code, and not have to bother with boring manual steps. For example, let’s change this target in the _targets.R file:\ntar_target(\n  commune_data,\n  clean_unemp(\n    unemp_data,\n    place_name_of_interest = c(\n      \"Luxembourg\", \"Dippach\",\n      \"Wiltz\", \"Esch/Alzette\",\n      \"Mersch\", \"Dudelange\"),\n    col_of_interest = active_population)\n)\nI’ve added “Dudelange” to the list of communes to plot. Pushing this change to GitHub triggers the action we’ve defined before. The plots (artifacts) get refreshed, and we can download them. Take a look and see that Dudelange was added in the communes.png plot!\nIt is also possible to “deploy” the plots directly to another branch, and do much, much more. I just wanted to give you a little taste of Github Actions (and more generally GitOps). The possibilities are virtually limitless, and I still can’t get over the fact that Github Actions is free for public repositories.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Integration with GitHub Actions</span>"
    ]
  },
  {
    "objectID": "11-ci.html#building-a-docker-image-and-pushing-it-to-a-registry",
    "href": "11-ci.html#building-a-docker-image-and-pushing-it-to-a-registry",
    "title": "11  Continuous Integration with GitHub Actions",
    "section": "11.5 Building a Docker image and pushing it to a registry",
    "text": "11.5 Building a Docker image and pushing it to a registry\nIt is also possible to build a Docker image and have it made available on an image registry. You can see how this works on this repository. This images can then be used as a base for other reproducible pipelines, as in this repository. Why do this? Well because of “separation of concerns”. You could have a repository which builds in image containing your development environment: this could be an image with a specific version of R and R packages built with Nix. And then have as many repositories as projects that run pipelines using that development environment image as a basis. Simply add the project-specific packages that you need for each project.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Integration with GitHub Actions</span>"
    ]
  },
  {
    "objectID": "11-ci.html#running-a-rixpress-pipeline-from-github-actions",
    "href": "11-ci.html#running-a-rixpress-pipeline-from-github-actions",
    "title": "11  Continuous Integration with GitHub Actions",
    "section": "11.6 Running a rixpress Pipeline from GitHub Actions",
    "text": "11.6 Running a rixpress Pipeline from GitHub Actions\nWith Nix and {rixpress}, running your pipeline directly on GitHub Actions is straightforward. Because Nix handles all dependencies reproducibly, you don’t need Docker as an intermediary. The rixpress_demos repository contains several complete examples; here we will walk through the key steps.\nThe workflow triggers on pushes and pull requests to main:\non:\n  pull_request:\n    branches: [main, master]\n  push:\n    branches: [main, master]\nAfter checking out the repository and installing Nix (with Cachix for faster builds), the first step generates or regenerates the development environment from the gen-env.R script:\n- name: Build dev env\n  run: |\n    nix-shell -p R \"rPackages.rix\" \"rPackages.rixpress\" --run \"Rscript gen-env.R\"\nNext, the pipeline definition is generated from gen-pipeline.R:\n- name: Generate pipeline\n  run: |\n    nix-shell --quiet --run \"Rscript gen-pipeline.R\"\nYou can optionally visualise the DAG to verify the pipeline structure:\n- name: Check DAG\n  run: |\n    nix-shell --quiet -p haskellPackages.stacked-dag --run \"stacked-dag dot _rixpress/dag.dot\"\nFinally, build and inspect the pipeline:\n- name: Build pipeline\n  run: |\n    nix-shell --quiet --run \"Rscript -e 'rixpress::rxp_make()'\"\n\n- name: Inspect built derivations\n  run: |\n    nix-shell --quiet --run \"Rscript -e 'rixpress::rxp_inspect()'\"\n\n- name: Show result\n  run: |\n    nix-shell --quiet --run \"Rscript -e 'rixpress::rxp_read(\\\"confusion_matrix\\\")'\"\n\n11.6.1 Caching Pipeline Outputs Between Runs\nWhile Nix caches derivations, CI runners are ephemeral: each run starts fresh. To avoid rebuilding the entire pipeline every time, {rixpress} provides rxp_export_artifacts() and rxp_import_artifacts() to persist outputs between runs.\nBefore building, check if cached outputs exist and import them:\n- name: Import cached outputs if available\n  run: |\n    if [ -f \"../outputs/my_pipeline/pipeline_outputs.nar\" ]; then\n      nix-shell --quiet --run \"Rscript -e 'rixpress::rxp_import_artifacts(archive_file = \\\"../outputs/my_pipeline/pipeline_outputs.nar\\\")'\"\n    else\n      echo \"No cached outputs found, will build from scratch\"\n    fi\nAfter building, export the outputs so they can be reused:\n- name: Export outputs to avoid rebuild\n  run: |\n    mkdir -p ../outputs/my_pipeline\n    nix-shell --quiet --run \"Rscript -e 'rixpress::rxp_export_artifacts(archive_file = \\\"../outputs/my_pipeline/pipeline_outputs.nar\\\")'\"\nFinally, commit the cached outputs back to the repository:\n- name: Push cached outputs\n  run: |\n    cd ..\n    git config --global user.name \"GitHub Actions\"\n    git config --global user.email \"actions@github.com\"\n    git pull --rebase --autostash origin main\n    git add outputs/my_pipeline/pipeline_outputs.nar\n    if git diff --cached --quiet; then\n      echo \"No changes to commit.\"\n    else\n      git commit -m \"Update cached pipeline outputs\"\n      git push origin main\n    fi\nThis pattern ensures that only changed derivations are rebuilt on subsequent runs. The .nar file format is Nix’s archive format and contains all the built outputs.\n\n\n11.6.2 The Easy Way: rxp_ga()\nIf the above seems like a lot of boilerplate, {rixpress} provides a helper function that generates a complete GitHub Actions workflow for you:\nrixpress::rxp_ga()\nThis creates a .github/workflows/run-rxp-pipeline.yaml file that handles everything: installing Nix, setting up Cachix, generating the environment and pipeline, importing and exporting artifacts, and storing them in a dedicated rixpress-runs orphan branch. Using an orphan branch keeps your main branch clean while persisting the cached outputs between runs.\nFor most projects, running rxp_ga() once and committing the generated workflow file is all you need to get your pipeline running on CI.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Integration with GitHub Actions</span>"
    ]
  },
  {
    "objectID": "11-ci.html#github-actions-without-nix",
    "href": "11-ci.html#github-actions-without-nix",
    "title": "11  Continuous Integration with GitHub Actions",
    "section": "11.7 GitHub Actions without Nix",
    "text": "11.7 GitHub Actions without Nix\nIf you’re not using Nix, you’ll have to set up GitHub Actions manually. Suppose you have a package project and want to run unit tests on each push. See for example the {myPackage} package, in particular this file. This action runs on each push and pull request on Windows, Ubuntu and macOS:\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n\njobs:\n  rcmdcheck:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\nSeveral steps are executed, all using pre-defined actions from the r-lib project:\n    steps:\n    - uses: actions/checkout@v4\n    - uses: r-lib/actions/setup-r@v2\n    - uses: r-lib/actions/setup-r-dependencies@v2\n      with:\n        extra-packages: any::rcmdcheck\n        needs: check\n    - uses: r-lib/actions/check-r-package@v2\nAn action such as r-lib/actions/setup-r@v2 will install R on any of the supported operating systems without requiring any configuration from you. If you didn’t use such an action, you would need to define three separate actions: one that would be executed on Windows, on Ubuntu and on macOS. Each of these operating-specific actions would install R in their operating-specific way.\nCheck out the workflow results to see how the package could be improved here.\nHere again, using Nix simplifies this process immensely. Look at this workflow file from {rix}’s repository here. Setting up the environment is much easier, as is running the actual test suite.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Integration with GitHub Actions</span>"
    ]
  },
  {
    "objectID": "11-ci.html#advanced-patterns",
    "href": "11-ci.html#advanced-patterns",
    "title": "11  Continuous Integration with GitHub Actions",
    "section": "11.8 Advanced patterns",
    "text": "11.8 Advanced patterns\nNow that you understand the basics, let’s look at some more advanced patterns that will make your CI workflows more efficient and informative.\n\n11.8.1 Caching with Cachix\nBuilding Nix environments from scratch on every CI run can be slow. Cachix solves this by providing a binary cache for your Nix derivations. Once you build something, subsequent runs can download the pre-built binaries instead of rebuilding from source.\nTo use Cachix, you first need to create a free account at cachix.org and create a cache. Then, generate an auth token and add it as a secret in your GitHub repository settings (under Settings → Secrets and variables → Actions). Call it something like CACHIX_AUTH.\nHere is a workflow that builds your development environment and pushes the results to your Cachix cache:\nname: Update Cachix cache\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build-and-cache:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install Nix\n        uses: DeterminateSystems/nix-installer-action@main\n\n      - uses: cachix/cachix-action@v15\n        with:\n          name: your-cache-name\n          authToken: '${{ secrets.CACHIX_AUTH }}'\n\n      - name: Build and push to cache\n        run: |\n          nix-build\n          nix-store -qR --include-outputs $(nix-instantiate default.nix) | cachix push your-cache-name\nThe key line here is the nix-store command at the end. It queries all the dependencies of your build and pushes them to Cachix. The next time you or anyone else runs this workflow, the cachix/cachix-action will automatically pull from your cache, dramatically speeding up the build.\nIf you want to build on both Linux and macOS (since Nix binaries are platform-specific), you can use a matrix:\njobs:\n  build-and-cache:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, macos-latest]\n\n\n11.8.2 Storing outputs in orphan branches\nWhen running a pipeline on CI, you often want to keep the outputs (plots, data, reports) without committing them to your main branch. A clean solution is to store them in an orphan branch. An orphan branch has no commit history and is completely separate from your main code.\nHere is the pattern:\n- name: Check if outputs branch exists\n  id: branch-exists\n  run: git ls-remote --exit-code --heads origin pipeline-outputs\n  continue-on-error: true\n\n- name: Create orphan branch if needed\n  if: steps.branch-exists.outcome != 'success'\n  run: |\n    git checkout --orphan pipeline-outputs\n    git rm -rf .\n    echo \"Pipeline outputs\" &gt; README.md\n    git add README.md\n    git commit -m \"Initial commit\"\n    git push origin pipeline-outputs\n    git checkout -\n\n- name: Push outputs to branch\n  run: |\n    git config --local user.name \"GitHub Actions\"\n    git config --local user.email \"actions@github.com\"\n    git fetch origin pipeline-outputs\n    git worktree add ./outputs pipeline-outputs\n    cp -r _outputs/* ./outputs/\n    cd outputs\n    git add .\n    git commit -m \"Update outputs\" || echo \"No changes\"\n    git push origin pipeline-outputs\nThis pattern first checks if the branch exists using git ls-remote. If not, it creates an orphan branch. Then it uses git worktree to work with both branches simultaneously, copies the outputs, and pushes them. The {rix} and {rixpress} packages use this pattern to store pipeline outputs between runs.\n\n\n11.8.3 Creating workflow summaries\nGitHub Actions has a built-in feature for creating rich summaries that appear directly on the workflow run page. You write Markdown to a special file path stored in the GITHUB_STEP_SUMMARY environment variable.\n- name: Create summary\n  run: |\n    echo \"## Pipeline Results 🎉\" &gt;&gt; $GITHUB_STEP_SUMMARY\n    echo \"\" &gt;&gt; $GITHUB_STEP_SUMMARY\n    echo \"| Metric | Value |\" &gt;&gt; $GITHUB_STEP_SUMMARY\n    echo \"|--------|-------|\" &gt;&gt; $GITHUB_STEP_SUMMARY\n    echo \"| Tests passed | 42 |\" &gt;&gt; $GITHUB_STEP_SUMMARY\n    echo \"| Coverage | 87% |\" &gt;&gt; $GITHUB_STEP_SUMMARY\nYou can also generate the summary dynamically from your R or Python code:\n- name: Generate summary from R\n  run: |\n    nix-shell --run \"Rscript -e '\n      results &lt;- readRDS(\\\"results.rds\\\")\n      cat(\\\"## Analysis Complete\\n\\n\\\", file = Sys.getenv(\\\"GITHUB_STEP_SUMMARY\\\"), append = TRUE)\n      cat(paste(\\\"Processed\\\", nrow(results), \\\"observations\\n\\\"), file = Sys.getenv(\\\"GITHUB_STEP_SUMMARY\\\"), append = TRUE)\n    '\"\nThis is particularly useful for:\n\nShowing test results at a glance\nDisplaying key metrics from your analysis\nProviding download links to artifacts\nReporting any warnings or issues\n\nThe summary appears right on the Actions tab, making it easy for collaborators to see what happened without digging through logs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Integration with GitHub Actions</span>"
    ]
  },
  {
    "objectID": "11-ci.html#conclusion",
    "href": "11-ci.html#conclusion",
    "title": "11  Continuous Integration with GitHub Actions",
    "section": "11.9 Conclusion",
    "text": "11.9 Conclusion\nThis chapter introduced Continuous Integration with GitHub Actions, the final piece of our reproducible workflow.\nKey takeaways:\n\nAutomation removes manual steps: Every push triggers tests, builds, and deployments without human intervention\nNix simplifies CI setup: The same default.nix you use locally works on GitHub Actions, eliminating “works on my machine” problems\nCachix speeds up builds: By caching Nix derivations, subsequent runs avoid rebuilding unchanged dependencies\nrxp_ga() handles the boilerplate: One function call generates a complete workflow for running {rixpress} pipelines on CI\nArtifact caching persists outputs: Using rxp_export_artifacts() and an orphan branch, pipeline outputs survive between ephemeral CI runs\n\nWith continuous integration in place, your reproducible analytical pipeline is truly automated. Push your code, and GitHub takes care of the rest: running tests, building your environment, executing your pipeline, and storing the results. This frees you to focus on what matters: the analysis itself.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Continuous Integration with GitHub Actions</span>"
    ]
  },
  {
    "objectID": "12-conclusion.html",
    "href": "12-conclusion.html",
    "title": "12  Conclusion: Adopting Reproducibility in the Real World",
    "section": "",
    "text": "12.1 Introduction\nIf you have made it this far, you now have a powerful toolkit at your disposal. You understand how Nix provides truly reproducible environments, how functional programming leads to testable and predictable code, how packages bundle your work for sharing, and how CI/CD automates the boring parts. But knowing these tools is only half the battle. The harder question is: how do you actually start using them?\nIf you work alone, the answer is simple: just start. Pick one project and commit to doing it the “right” way. You will make mistakes, you will get frustrated, and you will learn. But if you work in a team, the challenge is different. You cannot simply mandate that everyone learns Nix by next Tuesday. Change management is a skill in itself, and this final chapter offers some practical strategies for introducing reproducible practices to yourself, your team, and your organisation.\nThe philosophy here is simple: show, don’t tell. Nobody was ever convinced to adopt a new tool by a slideshow. They are convinced when they see it solve a real problem.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Conclusion: Adopting Reproducibility in the Real World</span>"
    ]
  },
  {
    "objectID": "12-conclusion.html#start-with-yourself",
    "href": "12-conclusion.html#start-with-yourself",
    "title": "12  Conclusion: Adopting Reproducibility in the Real World",
    "section": "12.2 Start with yourself",
    "text": "12.2 Start with yourself\nBefore you try to convince anyone else, you need to become proficient yourself. Pick a small, low-stakes project and use it as your learning ground. A perfect candidate is a personal side project or an internal analysis that nobody else depends on.\nYour goal in this phase is to build muscle memory. You want to reach the point where creating a default.nix file feels as natural as opening RStudio. You want to be able to troubleshoot common Nix errors without panic. You want to have a working CI pipeline that you understand inside and out.\nThis investment pays dividends later. When a colleague asks “Why is this so complicated?”, you will have a ready answer because you asked yourself the same question two months ago.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Conclusion: Adopting Reproducibility in the Real World</span>"
    ]
  },
  {
    "objectID": "12-conclusion.html#the-two-minute-demo",
    "href": "12-conclusion.html#the-two-minute-demo",
    "title": "12  Conclusion: Adopting Reproducibility in the Real World",
    "section": "12.3 The two-minute demo",
    "text": "12.3 The two-minute demo\nOnce you are comfortable, look for opportunities to demonstrate value. The best opportunities are problems that everyone recognises but nobody has solved.\n“It works on my machine” is the classic. The next time a colleague struggles to run your code, don’t just send them a fixed script. Send them a default.nix file and a one-liner: nix-shell --run \"Rscript analysis.R\". When it works on the first try, you will have their attention.\nAnother opportunity is the dreaded “update everything and pray” scenario. When a project breaks after a package update, you can show how Nix lets you pin exact versions. “This analysis will run the same way in five years” is a powerful statement.\nThe key is to solve real problems, not hypothetical ones. Do not lecture your colleagues about reproducibility in the abstract. Show them how it saved you two hours of debugging on Tuesday.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Conclusion: Adopting Reproducibility in the Real World</span>"
    ]
  },
  {
    "objectID": "12-conclusion.html#make-it-easy-for-others",
    "href": "12-conclusion.html#make-it-easy-for-others",
    "title": "12  Conclusion: Adopting Reproducibility in the Real World",
    "section": "12.4 Make it easy for others",
    "text": "12.4 Make it easy for others\nIf you want your team to adopt these practices, you need to lower the barrier to entry as much as possible. This means:\n\nWrite excellent documentation. Not for yourself, but for the colleague who has never heard of Nix. Assume they will spend five minutes reading before giving up.\nProvide templates. Create a repository with a working default.nix, gen-env.R, CI workflow, and a simple example analysis. When someone starts a new project, they can clone this template and be productive immediately.\nAutomate the setup. If your organisation allows it, provide a script that installs Nix with a single command. The fewer steps, the fewer places where someone can get stuck.\nBe available. When someone tries your approach and hits a wall, they need help within minutes, not days. If you are not responsive, they will abandon the effort and go back to their old ways.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Conclusion: Adopting Reproducibility in the Real World</span>"
    ]
  },
  {
    "objectID": "12-conclusion.html#pick-your-battles",
    "href": "12-conclusion.html#pick-your-battles",
    "title": "12  Conclusion: Adopting Reproducibility in the Real World",
    "section": "12.5 Pick your battles",
    "text": "12.5 Pick your battles\nYou cannot change everything at once. Be strategic about where you focus your energy.\nSome projects are not worth the effort to convert. If an analysis was run once three years ago and will never be touched again, leave it alone. Focus on new projects and on projects that are actively maintained.\nSome colleagues will never be convinced. That is fine. You do not need everyone on board. You need a few early adopters who will become advocates themselves. Focus on the curious ones, the ones who ask “how did you do that?” when they see your workflow.\nSome organisations have constraints you cannot change. Maybe you cannot install Nix on the production server. Maybe you are stuck with Windows machines. Work within these constraints rather than fighting them. Docker can bridge many gaps. WSL2 makes Nix possible on Windows. Find the path of least resistance.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Conclusion: Adopting Reproducibility in the Real World</span>"
    ]
  },
  {
    "objectID": "12-conclusion.html#the-gradual-adoption-path",
    "href": "12-conclusion.html#the-gradual-adoption-path",
    "title": "12  Conclusion: Adopting Reproducibility in the Real World",
    "section": "12.6 The gradual adoption path",
    "text": "12.6 The gradual adoption path\nHere is a realistic roadmap for introducing reproducibility to a team:\nMonth 1-2: Personal mastery. Use Nix and the tools from this book on your own projects. Get comfortable. Build your own templates and workflows.\nMonth 3-4: Show and tell. Start sharing your work. When you give a code review, mention that the project runs in a Nix shell. When someone asks how to install a package, show them rix(). Let people discover the value organically.\nMonth 5-6: Pilot project. Propose using the full stack (Nix, tests, CI) for one team project. Pick something visible but not critical. Document everything. Make it a success.\nMonth 7-12: Expand. Use the pilot project as a template for new work. Gradually, “the new way” becomes “the way we do things here.” Update onboarding documentation to include Nix setup.\nThis is slow. It takes patience. But lasting change always does.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Conclusion: Adopting Reproducibility in the Real World</span>"
    ]
  },
  {
    "objectID": "12-conclusion.html#common-objections-and-how-to-address-them",
    "href": "12-conclusion.html#common-objections-and-how-to-address-them",
    "title": "12  Conclusion: Adopting Reproducibility in the Real World",
    "section": "12.7 Common objections and how to address them",
    "text": "12.7 Common objections and how to address them\n“This is too complicated.”\nIt is more complicated than doing nothing. But it is simpler than debugging environment issues at 3am before a deadline. Complexity is a trade-off. You are trading upfront learning for long-term reliability. Acknowledge the learning curve, but emphasise that you are there to help.\n“We don’t have time for this.”\nYou do not have time not to do this. Every hour spent on environment issues is an hour not spent on analysis. Every “it works on my machine” is a risk. Frame reproducibility as a time investment, not a time cost.\n“My code works fine without it.”\nIt works today. Will it work in six months? Will it work when you hand it to a colleague? Will it work when the package maintainer pushes a breaking change? Reproducibility is insurance. You hope you never need it, but you are grateful when you do.\n“I’ll learn it later.”\nLater never comes. Start with one small project. Commit to using one new tool. You can learn incrementally. You do not need to master everything before you begin.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Conclusion: Adopting Reproducibility in the Real World</span>"
    ]
  },
  {
    "objectID": "12-conclusion.html#llms-as-adoption-accelerators",
    "href": "12-conclusion.html#llms-as-adoption-accelerators",
    "title": "12  Conclusion: Adopting Reproducibility in the Real World",
    "section": "12.8 LLMs as adoption accelerators",
    "text": "12.8 LLMs as adoption accelerators\nThroughout this book, we have seen how LLMs can assist with writing tests, generating documentation, and even drafting functions. But there is a bigger point to make here: LLMs dramatically lower the barrier to adopting these practices.\nThe old objection to tools like Nix was that the learning curve was too steep. You had to understand a new language, a new paradigm, and a lot of unfamiliar syntax. This is still true to some extent. But with an LLM at your side, you no longer need to memorise everything. You can ask.\n“How do I add a Python package to my rix() call?” “What does this Nix error message mean?” “Generate a GitHub Actions workflow that runs my tests in a Nix shell.” These are questions that would have required hours of documentation diving or forum searching. Now they take seconds.\nThis changes the economics of learning. The boilerplate that used to be a barrier is now generated for you. The syntax you kept forgetting is a prompt away. The edge cases you never encountered are handled by an assistant that has seen them all.\nMore importantly, LLMs make it easier to help your colleagues. When someone on your team struggles with a Nix expression, you do not need to be available at that exact moment. They can ask an LLM for help. They can generate a first draft and ask you to review it. The bottleneck shifts from “I need to learn everything” to “I need to understand enough to verify the output.”\nThis is why the pkgctx tool exists. By feeding package documentation to an LLM, you give it the context it needs to generate idiomatic code for libraries like {rix}, {rixpress}, or {chronicler}. You are not replacing your expertise; you are amplifying it.\nThe combination of reproducibility tools and LLMs is powerful. The tools provide structure. The LLMs provide accessibility. Together, they make best practices achievable for teams that would never have adopted them otherwise.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Conclusion: Adopting Reproducibility in the Real World</span>"
    ]
  },
  {
    "objectID": "12-conclusion.html#a-note-on-literate-programming",
    "href": "12-conclusion.html#a-note-on-literate-programming",
    "title": "12  Conclusion: Adopting Reproducibility in the Real World",
    "section": "12.9 A note on literate programming",
    "text": "12.9 A note on literate programming\nThis book has focused on the plumbing of reproducibility: environments, packages, pipelines, and automation. But there is another dimension we have not covered in depth: literate programming.\nLiterate programming is the practice of combining code, prose, and outputs into a single document. Tools like Quarto, R Markdown, and Jupyter notebooks make this possible. The idea is that the analysis and its documentation are the same thing. You cannot have one without the other.\nThis book itself was written with Quarto. Every code example runs. Every output is generated fresh on each build. This is literate programming in action.\nIf you are interested in this approach, there are excellent resources available. The Quarto documentation at quarto.org is a great starting point. For R Markdown, “R Markdown: The Definitive Guide” by Xie, Allaire, and Grolemund is comprehensive. The principles you have learned in this book apply directly: pin your environment with Nix, run your document rendering in CI, and your reports become just as reproducible as your analyses.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Conclusion: Adopting Reproducibility in the Real World</span>"
    ]
  },
  {
    "objectID": "12-conclusion.html#final-thoughts",
    "href": "12-conclusion.html#final-thoughts",
    "title": "12  Conclusion: Adopting Reproducibility in the Real World",
    "section": "12.10 Final thoughts",
    "text": "12.10 Final thoughts\nReproducibility is not a destination. It is a practice. You will never have a perfectly reproducible workflow because requirements change, tools evolve, and you learn new things. The goal is not perfection but continuous improvement.\nStart where you are. Use what you have. Do what you can.\nIf this book has given you one new tool or one new idea, it has done its job. If it has changed how you think about your work, even better. And if you go on to share these practices with your colleagues and build a culture of reproducibility in your team, then you have not only learned but taught. That is the highest compliment.\nGood luck. Stay curious. Build things that last.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Conclusion: Adopting Reproducibility in the Real World</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Peng, Roger D. 2011. “Reproducible Research in Computational\nScience.” Science 334 (6060): 1226–27.",
    "crumbs": [
      "References"
    ]
  }
]