# Building Reproducible Pipelines with rixpress

## Introduction: From Scripts and Notebooks to Pipelines

So far, we have learned about reproducible development environments with Nix and
`{rix}`. We can now create project-specific environments with precise versions
of R, Python, and all dependencies. But there's one more piece to the puzzle:
**orchestration**.

How do we take our collection of functions and data files and run them in the
correct order to produce our final data product? This problem of managing
computational workflows is not new, and a whole category of **build automation
tools** has been created to solve it.

### The Evolution of Build Automation

The original solution, dating back to the 1970s, is **`make`**. Created by
Stuart Feldman at Bell Labs in 1976, `make` reads a `Makefile` that describes
the dependency graph of a project. If you change the code that generates
`plot.png`, `make` is smart enough to only re-run the steps needed to rebuild
the plot and the final report.

The strength of these tools is their language-agnosticism, but their weaknesses
are twofold:

1. **File-centric**: You must manually handle all I/O. Your first script saves
   `data.csv`, your second loads it. This adds boilerplate and creates surfaces
   for error.
2. **Environment-agnostic**: They track files but know nothing about the
   *software environment* needed to create those files.

This is where R's **`{targets}`** package shines. It tracks dependencies between
**R objects directly**, automatically handling serialisation. But `{targets}`
operates within a single R session; for polyglot pipelines, you must manually
coordinate via `{reticulate}`.

### The Separation Problem

All these tools (from `make` to `{targets}` to Airflow) separate workflow
management from environment management. You use one tool to run the pipeline and
another (Docker, `{renv}`) to set up the software.

This separation creates friction. Running `{targets}` inside Docker ensures
reproducibility, but forces the entire pipeline into one monolithic environment.
What if your Python step requires TensorFlow 2.15 but your R step needs
reticulate with Python 3.9? You're stuck.

### The Imperative Approach: Make + Docker

To illustrate this, consider the traditional setup for a polyglot pipeline.
You'd need:

1. A `Dockerfile` to set up the environment
2. A `Makefile` to orchestrate the workflow
3. Wrapper scripts for each step

Here's what a `Makefile` might look like:

```makefile
# Makefile for a Python → R pipeline

DATA_DIR = data
OUTPUT_DIR = output

$(OUTPUT_DIR)/predictions.csv: $(DATA_DIR)/raw.csv scripts/train_model.py
python scripts/train_model.py $(DATA_DIR)/raw.csv $@

$(OUTPUT_DIR)/plot.png: $(OUTPUT_DIR)/predictions.csv scripts/visualise.R
Rscript scripts/visualise.R $< $@

$(OUTPUT_DIR)/report.html: $(OUTPUT_DIR)/plot.png report.qmd
quarto render report.qmd -o $@

all: $(OUTPUT_DIR)/report.html

clean:
rm -rf $(OUTPUT_DIR)/*
```

This looks clean, but notice the procedural boilerplate required in each script. 
Your `train_model.py` must parse command-line arguments and handle file I/O:

```python
# scripts/train_model.py
import sys
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

def main():
    input_path = sys.argv[1]
    output_path = sys.argv[2]

    # Load data
    df = pd.read_csv(input_path)

    # ... actual ML logic ...

    # Save results
    predictions.to_csv(output_path, index=False)

if __name__ == "__main__":
    main()
```

And your `visualise.R` script needs the same boilerplate:

```r
# scripts/visualise.R
args <- commandArgs(trailingOnly = TRUE)
input_path <- args[1]
output_path <- args[2]

# Load data
predictions <- read.csv(input_path)

# ... actual visualisation logic ...

# Save plot
ggsave(output_path, plot)
```

The scientific logic is buried under file I/O scaffolding. And the environment?
That's a separate 100+ line `Dockerfile` you must maintain.

### rixpress: Unified Orchestration

This brings us to a key insight: a reproducible pipeline should be nothing more
than a **composition of pure functions**, each with explicit inputs and outputs,
no hidden state, and no reliance on execution order beyond what the data
dependencies require.

**`{rixpress}`** solves this by using Nix not just as a package manager, but as
the build automation engine itself. Each pipeline step is a **Nix derivation**:
a hermetically sealed build unit.

Compare the same pipeline in `{rixpress}`:

```{r}
#| eval: false
library(rixpress)

list(
  rxp_py_file(
    name = raw_data,
    path = "data/raw.csv",
    read_function = "lambda x: pandas.read_csv(x)"
  ),

  rxp_py(
    name = predictions,
    expr = "train_model(raw_data)",
    user_functions = "functions.py"
  ),

  rxp_py2r(
    # rxp_py2r uses reticulate for conversion
    name = predictions_r,
    expr = predictions
  ),

  rxp_r(
    name = plot,
    expr = visualise(predictions_r),
    user_functions = "functions.R"
  ),

  rxp_qmd(
    name = report,
    qmd_file = "report.qmd"
  )
) |>
  rxp_populate()
```

And your `functions.py` contains *only* the scientific logic:

```python
# functions.py
def train_model(df):
    # ... pure ML logic, no file I/O ...
    return predictions
```

The difference is stark:

| Aspect | Make + Docker | rixpress |
|--------|--------------|----------|
| **Files needed** | Dockerfile, Makefile, wrapper scripts | gen-env.R, gen-pipeline.R, function files |
| **I/O handling** | Manual in every script | Automatic via encoders/decoders |
| **Dependencies** | Explicit file rules | Inferred from object references |
| **Environment** | Separate Docker setup | Unified via `{rix}` |
| **Expertise needed** | Linux admin, Make syntax | R programming |

This provides two key benefits:

1. **True Polyglot Pipelines**: Each step can have its own Nix environment. A
   Python step runs in a pure Python environment, an R step in an R environment,
   a Quarto step in yet another, all within the same pipeline. Julia is also
   supported, and `{reticulate}` can be used for data transfer, but arbitrary
   encoder and decoder as well (more on this later).

2. **Deep Reproducibility**: Each step is cached based on the cryptographic hash
   of *all* its inputs: the code, the data, *and the environment*. Any change in
   dependencies triggers a rebuild. This is reproducibility at the build level,
   not just the environment level.

The interface is heavily inspired by `{targets}`, so you get the ergonomic,
object-passing feel you're used to, combined with the bit-for-bit
reproducibility of the Nix build system.

::: {.callout-tip}
## Getting LLM assistance with `{rixpress}` and `ryxpress`

If the `{rixpress}` syntax is new to you, remember that you can use `pkgctx` to
generate LLM-ready context (as mentioned in the introduction). Both the
`{rixpress}` (R) and `ryxpress` (Python) repositories include `.pkgctx.yaml`
files you can feed to your LLM to help it understand the package's API. You can
also generate your own context files:

```bash
# For the R package
nix run github:b-rodrigues/pkgctx -- r github:b-rodrigues/rixpress > rixpress.pkgctx.yaml

# For the Python package
nix run github:b-rodrigues/pkgctx -- python ryxpress > ryxpress.pkgctx.yaml
```

With this context, your LLM can help you write correct pipeline definitions,
even if the syntax is completely new to you. You can do so for any package
hosted on CRAN, GitHub, or local `.tar.gz` files.
:::

## What is rixpress?

`{rixpress}` streamlines creation of *micropipelines* (small-to-medium,
single-machine analytic pipelines) by expressing a pipeline in idiomatic R while
delegating build orchestration to the Nix build system.

Key features:

- Define pipeline *derivations* with concise `rxp_*()` helper functions
- Seamlessly mix R, Python, Julia, and Quarto steps
- Reuse hermetic environments defined via `{rix}` and a `default.nix`
- Visualise and inspect the DAG; selectively read, load, or copy outputs
- Automatic caching: only rebuild what changed

`{rixpress}` provides several functions to define pipeline steps:

| Function | Purpose |
|----------|---------|
| `rxp_r()` | Run R code |
| `rxp_r_file()` | Read a file using R |
| `rxp_py()` | Run Python code |
| `rxp_py_file()` | Read a file using Python |
| `rxp_qmd()` | Render a Quarto document |
| `rxp_py2r()` | Convert Python object to R using reticulate |
| `rxp_r2py()` | Convert R object to Python using reticulate |

Here is what a basic pipeline looks like:

```{r}
#| eval: false
library(rixpress)

list(
  rxp_r_file(
    mtcars,
    'mtcars.csv',
    \(x) read.csv(file = x, sep = "|")
  ),

  rxp_r(
    mtcars_am,
    filter(mtcars, am == 1)
  ),

  rxp_r(
    mtcars_head,
    head(mtcars_am)
  ),

  rxp_qmd(
    page,
    "page.qmd"
  )
) |>
  rxp_populate()
```

## Getting Started

### Initialising a project

If you're starting fresh, you can bootstrap a project using a temporary shell:

```bash
nix-shell -p R rPackages.rix rPackages.rixpress
```

Once inside, start R and run:

```{r}
#| eval: false
rixpress::rxp_init()
```

This creates two essential files:

- **`gen-env.R`**: Where you define your environment with `{rix}`
- **`gen-pipeline.R`**: Where you define your pipeline with `{rixpress}`

### Defining the environment

Open `gen-env.R` and define the tools your pipeline needs:

```{r}
#| eval: false
library(rix)

rix(
  date = "2025-10-14",
  r_pkgs = c("dplyr", "ggplot2", "quarto", "rixpress"),
  ide = "none",
  project_path = ".",
  overwrite = TRUE
)
```

Run this script to generate `default.nix`, then build and enter your environment:

```bash
nix-build
nix-shell
```

### Defining the pipeline

Open `gen-pipeline.R` and define your pipeline:

```{r}
#| eval: false
library(rixpress)

list(
  rxp_r_file(
    name = mtcars,
    path = "data/mtcars.csv",
    read_function = \(x) read.csv(x, sep = "|")
  ),

  rxp_r(
    name = mtcars_am,
    expr = dplyr::filter(mtcars, am == 1)
  ),

  rxp_r(
    name = mtcars_head,
    expr = head(mtcars_am)
  )
) |>
  rxp_populate()
```

Running `rxp_populate()` generates a `pipeline.nix` file and builds the entire
pipeline. It also creates a `_rixpress` folder with required files for the
project.

You don't need to write `gen-pipeline.R` completely in one go. Instead, start
with a single derivation, usually the one loading the data, and build the
pipeline using `source("gen-pipeline.R")` and then `rxp_make()`. Alternatively,
if you set `build = TRUE` in `rxp_populate()`, sourcing the script alone would
be enough, as the pipeline would be built automatically.

In an interactive session, you can use `rxp_read()`, `rxp_load()` to read or
load artifacts into the R session respectively and `rxp_trace()` to check the
lineage of an artifact. This way, you can also work interactively with
`{rixpress}`: add derivations one by one and incrementally build the pipeline.
If you change previous derivations or functions that affect them, you don't need
to worry about re-running past steps—these will be executed automatically since
they've changed!


## Real-World Examples

The [rixpress_demos](https://github.com/b-rodrigues/rixpress_demos) repository
contains many complete examples. Here are a few practical patterns to get you
started.

### Example 1: Reading Many Input Files

When you have multiple CSV files in a directory:

```{r}
#| eval: false
library(rixpress)

list(
  # R approach: read all files at once
  rxp_r_file(
    name = mtcars_r,
    path = "data",
    read_function = \(x) {
      readr::read_delim(list.files(x, full.names = TRUE), delim = "|")
    }
  ),

  # Python approach: custom function
  rxp_py_file(
    name = mtcars_py,
    path = "data",
    read_function = "read_many_csvs",
    user_functions = "functions.py"
  ),

  rxp_py(
    name = head_mtcars,
    expr = "mtcars_py.head()"
  )
) |>
  rxp_populate()
```

The key insight: `rxp_r_file()` and `rxp_py_file()` can point to a directory,
and your `read_function` handles the logic.

### Example 2: Machine Learning with XGBoost

This pipeline trains an XGBoost classifier in Python, then passes predictions to
R for evaluation with `{yardstick}`:

```{r}
#| eval: false
library(rixpress)

list(
  # Load data as NumPy array
  rxp_py_file(
    name = dataset_np,
    path = "data/pima-indians-diabetes.csv",
    read_function = "lambda x: loadtxt(x, delimiter=',')"
  ),

  # Split features and target
  rxp_py(name = X, expr = "dataset_np[:,0:8]"),
  rxp_py(name = Y, expr = "dataset_np[:,8]"),

  # Train/test split
  rxp_py(
    name = splits,
    expr = "train_test_split(X, Y, test_size=0.33, random_state=7)"
  ),

  # Extract splits

  rxp_py(name = X_train, expr = "splits[0]"),
  rxp_py(name = X_test, expr = "splits[1]"),
  rxp_py(name = y_train, expr = "splits[2]"),
  rxp_py(name = y_test, expr = "splits[3]"),

  # Train XGBoost model
  rxp_py(
    name = model,
    expr = "XGBClassifier(use_label_encoder=False, eval_metric='logloss').fit(X_train, y_train)"
  ),

  # Make predictions
  rxp_py(name = y_pred, expr = "model.predict(X_test)"),

  # Export predictions to CSV for R
  rxp_py(
    name = combined_df,
    expr = "DataFrame({'truth': y_test, 'estimate': y_pred})"
  ),

  rxp_py(
    name = combined_csv,
    expr = "combined_df",
    user_functions = "functions.py",
    encoder = "write_to_csv"
  ),

  # Compute confusion matrix in R
  rxp_r(
    combined_factor,
    expr = mutate(combined_csv, across(everything(), factor)),
    decoder = "read.csv"
  ),

  rxp_r(
    name = confusion_matrix,
    expr = yardstick::conf_mat(combined_factor, truth, estimate)
  )
) |>
  rxp_populate(build = FALSE)

# Adjust Python imports
adjust_import("import numpy", "from numpy import array, loadtxt")
adjust_import("import xgboost", "from xgboost import XGBClassifier")
adjust_import("import sklearn", "from sklearn.model_selection import train_test_split")
add_import("from pandas import DataFrame", "default.nix")

rxp_make()
```

This demonstrates:

- Python-heavy computation with XGBoost
- Custom serialization via `encoder`/`decoder`
- Adjusting Python imports with `adjust_import()` and `add_import()`
- Passing results to R for evaluation

### Example 3: Simple Python→R→Quarto Workflow

A complete pipeline that bounces data between languages and renders a report:

```{r}
#| eval: false
library(rixpress)

list(
  # Read with Python polars
  rxp_py_file(
    name = mtcars_pl,
    path = "data/mtcars.csv",
    read_function = "lambda x: polars.read_csv(x, separator='|')"
  ),

  # Filter in Python, convert to pandas for reticulate
  rxp_py(
    name = mtcars_pl_am,
    expr = "mtcars_pl.filter(polars.col('am') == 1).to_pandas()"
  ),

  # Convert to R
  rxp_py2r(name = mtcars_am, expr = mtcars_pl_am),

  # Process in R
  rxp_r(
    name = mtcars_head,
    expr = my_head(mtcars_am),
    user_functions = "functions.R"
  ),

  # Back to Python
  rxp_r2py(name = mtcars_head_py, expr = mtcars_head),

  # More Python processing
  rxp_py(name = mtcars_tail_py, expr = "mtcars_head_py.tail()"),

  # Back to R
  rxp_py2r(name = mtcars_tail, expr = mtcars_tail_py),

  # Final R step
  rxp_r(name = mtcars_mpg, expr = dplyr::select(mtcars_tail, mpg)),

  # Render Quarto document
  rxp_qmd(
    name = page,
    qmd_file = "my_doc/page.qmd",
    additional_files = c("my_doc/content.qmd", "my_doc/images")
  )
) |>
  rxp_populate()
```

Note the `additional_files` argument for `rxp_qmd()`: this includes child
documents and images that the main Quarto file needs.

## Working with Pipelines

### Building the pipeline

You can build the pipeline in two steps:

```{r}
#| eval: false
# Generate pipeline.nix only (don't build)
rxp_populate(build = FALSE)

# Build the pipeline
rxp_make()
```

Or in a single step with `rxp_populate(build = TRUE)`. Once you've built the
pipeline, you can inspect the results.

### Inspecting outputs

Because outputs live in `/nix/store/`, `{rixpress}` provides helpers:

```{r}
#| eval: false
# List all built artifacts
rxp_inspect()

# Read an artifact into R
rxp_read("mtcars_head")

# Load an artifact into the global environment
rxp_load("mtcars_head")

```

It is important to understand that all artifacts are stored in the
`/nix/store/`. So if you want to save the outputs somewhere else, you need to
use `rxp_copy()`.

```r
rxp_copy("clean_data")
```

This will copy the `clean_data` artifacts into the current working directory. If
you call `rxp_copy()` without arguments, every artifact will be copied over.

### Visualising the pipeline

There are three ways to visualise pipelines. The first is to look at the *trace*
of a derivation or of the full pipeline. This is similar to a genealogy tree.
You can start by running `rxp_inspect()` to double check how the derivation
names are written:

```r
rxp_inspect()
```

```r
derivation build_success
1              X_test          TRUE
2             X_train          TRUE
3               alpha          TRUE
4                beta          TRUE
5     all-derivations          TRUE
6               delta          TRUE
7        final_report          TRUE
8   model_predictions          TRUE
9         output_plot          TRUE
10        predictions          TRUE
11     processed_data          TRUE
12                rho          TRUE
13              sigma          TRUE
14            sigma_z          TRUE
15 simulated_rbc_data          TRUE
16      trained_model          TRUE
17             y_test          TRUE
18            y_train          TRUE
```

Let's check the trace of `output_plot`:

```r
rxp_trace("output_plot")
```

```r
==== Lineage for: output_plot ====
Dependencies (ancestors):
  - predictions
    - y_test*
      - processed_data*
        - simulated_rbc_data*
          - alpha*
          - beta*
          - delta*
          - rho*
          - sigma*
          - sigma_z*
    - model_predictions*
      - X_test*
        - processed_data*
      - trained_model*
        - X_train*
          - processed_data*
        - y_train*
          - processed_data*

Reverse dependencies (children):
  - final_report

Note: '*' marks transitive dependencies (depth >= 2).
```


We see that `output_plot` depends directly on `predictions`, and `predictions`
on `y_test`, which itself depends on all the other listed dependencies. The
children of the derivation are also listed, in this case `final_report`.

Calling `rxp_trace()` without arguments shows the entire pipeline:

```r
rxp_trace()
```

```r
- final_report
  - simulated_rbc_data
    - alpha*
    - beta*
    - delta*
    - rho*
    - sigma*
    - sigma_z*
  - output_plot
    - predictions*
      - y_test*
        - processed_data*
          - simulated_rbc_data*
      - model_predictions*
        - X_test*
          - processed_data*
        - trained_model*
          - X_train*
            - processed_data*
          - y_train*
            - processed_data*

Note: '*' marks transitive dependencies (depth >= 2).
```

Another way to visualise the pipeline is by using `rxp_ggdag()`. This
requires the `{ggdag}` package to be installed in the environment. Calling
`rxp_ggdag()` shows the following plot:

::: {.content-hidden when-format="pdf"}
<figure>
    <img src="images/rixpress_dag.png"
         alt="Derivations are coloured by programming language."></img>
    <figcaption>Derivations are coloured by programming language.</figcaption>
</figure>
:::

::: {.content-visible when-format="pdf"}
```{r, echo = F, out.width="400px"}
#| fig-cap: "Derivations are coloured by programming language."
knitr::include_graphics("images/rixpress_dag.png")
```
:::

Finally, you can also use `rxp_visnetwork()` (which requires the `{visNetwork}`
package). This opens an interactive JavaScript visualization in your browser,
allowing you to zoom into and explore different parts of your pipeline.

### Caching and Incremental Builds

One of the most powerful features of using Nix for pipelines is automatic
caching. Because Nix tracks all inputs to each derivation, it knows exactly what
needs to be rebuilt when something changes.

Try this:

1. Build your pipeline with `rxp_make()`
2. Change one step in your pipeline
3. Run `rxp_make()` again

Nix will detect that unchanged steps are already cached and instantly reuse
them. It only rebuilds the steps affected by your change.

### Build Logs and Debugging

Every time you run `rxp_populate()`, a timestamped log is saved in the
`_rixpress/` directory. This is like having a Git history for your pipeline's
*outputs*.

```{r}
#| eval: false
# List all past builds
rxp_list_logs()

# Load artifact from current build
new_result <- rxp_read("mtcars_head")

# Load artifact from previous build
old_result <- rxp_read("mtcars_head", which_log = "z9y8x")

# Compare them
identical(new_result, old_result)
```

This is incredibly powerful for debugging and validation. You can go back in
time to inspect any output from any previous pipeline run.

### Running Someone Else's Pipeline

The ultimate test of reproducibility: can someone else run your pipeline?

With a Nix-based workflow, they need only:

1. `git clone` your repository
2. Run `nix-build && nix-shell`
3. Run `source("gen-pipeline.R")` or `rxp_make()`

That's it. Nix reads your `default.nix` and `pipeline.nix` files and builds the 
*exact* same environment and data product, bit-for-bit.

### Exporting and Importing Artifacts

For CI/CD or sharing between machines:

```{r}
#| eval: false
# Export build products to a tarball
rxp_export_artifacts()

# Import on another machine before building
rxp_import_artifacts()
```

This speeds up continuous integration by avoiding unnecessary rebuilds.

## Organizing Large Projects with Sub-Pipelines

As pipelines grow, a single `gen-pipeline.R` file can become difficult to
manage. Consider the typical data science project:

- Data extraction and cleaning (ETL)
- Feature engineering
- Model training
- Model evaluation
- Report generation

Putting all derivations in one file makes it hard to:

- Navigate the code
- Understand which derivations belong to which phase
- Collaborate across team members
- Reuse pipeline components in other projects

To solve this issue, you can define your project using sub-pipelines and join
them into a master pipeline using `rxp_pipeline()`.

This allows you to organise derivations into named groups and even colour-code
them for making it easy to visualise.

A project with sub-pipelines would look something like this:

```
my-project/
├── default.nix           # Nix environment (generated by rix)
├── gen-env.R             # Script to generate default.nix
├── gen-pipeline.R        # MASTER SCRIPT: combines all sub-pipelines
└── pipelines/
    ├── 01_data_prep.R    # Data preparation sub-pipeline
    ├── 02_analysis.R     # Analysis sub-pipeline
    └── 03_reporting.R    # Reporting sub-pipeline
```

Each sub-pipeline file returns a list of derivations:

```{r sub-pipeline-1, eval = FALSE}
# Data Preparation Sub-Pipeline
# pipelines/01_data_prep.R
library(rixpress)

list(
  rxp_r(
    name = raw_mtcars,
    expr = mtcars
  ),
  rxp_r(
    name = clean_mtcars,
    expr = dplyr::filter(raw_mtcars, am == 1)
  ),
  rxp_r(
    name = selected_mtcars,
    expr = dplyr::select(clean_mtcars, mpg, cyl, hp, wt)
  )
)
```

The `rxp_pipeline()` function takes:

- **name**: A descriptive name for this group of derivations
- **path**: Either a **file path** to an R script returning a list of derivations (recommended), or a list of derivation objects.
- **color**: Optional CSS color name or hex code for DAG visualisation

The second sub-pipeline:

```{r sub-pipeline-2, eval = FALSE}
# Analysis Sub-Pipeline
# pipelines/02_analysis.R
library(rixpress)

list(
  rxp_r(
    name = summary_stats,
    expr = summary(selected_mtcars)
  ),
  rxp_r(
    name = mpg_model,
    expr = lm(mpg ~ hp + wt, data = selected_mtcars)
  ),
  rxp_r(
    name = model_coefs,
    expr = coef(mpg_model),
    )
)
```

The master script becomes very clean, as `rxp_pipeline()` handles sourcing the
files:

```{r master-script, eval = FALSE}
# gen-pipeline.R
library(rixpress)

# Create named pipelines with colours by pointing to the files
pipe_data_prep <- rxp_pipeline(
  name = "Data Preparation",
  path = "pipelines/01_data_prep.R",
  color = "#E69F00"
)

pipe_analysis <- rxp_pipeline(
  name = "Statistical Analysis",
  path = "pipelines/02_analysis.R",
  color = "#56B4E9"
)

# Build combined pipeline
rxp_populate(
  list(pipe_data_prep, pipe_analysis),
  project_path = ".",
  build = TRUE)
```

### Visualising Sub-Pipelines

When sub-pipelines are defined, visualisation tools use pipeline colours:

1. **Interactive Network** (`rxp_visnetwork()`) and **Static DAG**
   (`rxp_ggdag()`) both use a dual-encoding approach:
   - **Node fill (interior)**: Derivation type colour (R = blue, Python =
     yellow, etc.)
   - **Node border (thick stroke)**: Pipeline group colour This allows you to
   see both what *type* of computation each node is and which *pipeline* it
   belongs to.

<figure>
<img src="https://raw.githubusercontent.com/ropensci/rixpress/refs/heads/main/vignettes/subpipelines.png" alt="Subpipelines are coloured." />
<figcaption aria-hidden="true">
Subpipelines are coloured.
</figcaption>
</figure>

2. **Trace**: `rxp_trace()` output in the console is coloured by pipeline (using
   the `cli` package).

<figure>
<img src="https://raw.githubusercontent.com/ropensci/rixpress/refs/heads/main/vignettes/subpipelines_trace.png" alt="If your terminal supports it, derivation names are coloured according to the chosen sub-pipeline colour." />
<figcaption aria-hidden="true">
If your terminal supports it, derivation names are coloured according to the chosen sub-pipeline colour.
</figcaption>
</figure>

It is also possible to not highlight sub-pipelines by using the `colour_by`
argument (allows you to switch between colouring the pipeline, or by derivation
type):

```{r color-modes, eval = FALSE}
rxp_ggdag(colour_by = "pipeline")

rxp_ggdag(colour_by = "type")
```
## Polyglot Pipelines

One of `{rixpress}`'s strengths is seamlessly mixing languages. Here's a
pipeline that reads data with Python's `polars`, processes it with R's `dplyr`,
and renders a Quarto report.

::: {.callout-tip}
## Polyglot Development Is Now Cheap

Historically, using multiple languages in one project meant significant setup
overhead: installing interpreters, managing conflicting dependencies, writing
glue code. With Nix, that cost drops to near zero. You declare your R and Python
dependencies in one file, and Nix handles the rest.

LLMs lower the barrier further. Even if you are primarily an R programmer, you
can ask an LLM to generate the Python code for a specific step, or vice versa.
You don't need to master both languages; you just need to know enough to
recognise when each shines. Use R for statistics, Bayesian modelling, and
visualisation with `{ggplot2}`. Use Python for deep learning, web scraping, or
leveraging a library that only exists in the Python ecosystem. With Nix handling
environments and LLMs helping with syntax, the "cost" of crossing language
boundaries becomes negligible.
:::

Open `gen-env.R` and define the tools your pipeline needs:

```{r}
#| eval: false
library(rix)

rix(
  date = "2026-01-26",
  r_pkgs = c(
    "chronicler",
    "dplyr",
    "igraph",
    "quarto",
    "reticulate",
    "rix",
    "rixpress"
  ),
  py_conf = list(
    py_version = "3.14",
    py_pkgs = c(
      "biocframe",
      "numpy",
      "phart",
      "polars",
      "pyarrow",
      "rds2py",
      "ryxpress"
    )
  ),
  ide = "none",
  project_path = ".",
  overwrite = TRUE
)
```

The Python packages `biocframe`, `phart`, and `rds2py` are optional but highly
recommended when working with `ryxpress`. `biocframe` enables direct transfer to
`polars` or Bioconductor DataFrames, bypassing the need for `pandas`. `rds2py`
(also from the BiocPy project) allows Python to read `.rds` files, R's native
binary format. Together, these packages ensure seamless interoperability between
your R and Python steps. `phart` is a nice little package that allows you to
visualise pipelines from a Python interpreter prompt: not as sexy as R's
`{ggdag}` but it gets the job done!

### Transferring data between Python and R

The `rxp_py2r()` and `rxp_r2py()` functions use `{reticulate}` to convert
objects between languages:

```{r}
#| eval: false
rxp_py2r(
  name = mtcars_r,
  expr = mtcars_py
)
```

However, `{reticulate}` conversions can sometimes be fragile or behave
unexpectedly with complex objects. For robust production pipelines, I recommend
using the `encoder` and `decoder` arguments to explicitly serialize data to
intermediary formats like CSV, JSON, or Apache Arrow (Parquet/Feather).

You can define a custom encoder in Python and a decoder in R:

```{r}
#| eval: false
# Python step: serialize to JSON
rxp_py(
  name = mtcars_json,
  expr = "mtcars_pl.filter(polars.col('am') == 1)",
  user_functions = "functions.py",
  encoder = "serialize_to_json"
),

# R step: deserialize from JSON
rxp_r(
  name = mtcars_head,
  expr = my_head(mtcars_json),
  user_functions = "functions.R",
  decoder = "jsonlite::fromJSON"
)
```

The Python `serialize_to_json` function would be defined in `functions.py`:

```python
#| eval: false
def serialize_to_json(pl_df, path):
    with open(path, 'w') as f:
        f.write(pl_df.write_json())
```

For larger datasets, Apache Arrow is ideal because it allows zero-copy reads and
is supported natively by `polars`, `pandas`, and R's `{arrow}` package.

Here is how you would transfer data from Python to R:

```{r}
#| eval: false
rxp_py(
  name = processed_data,
  expr = "prepare_features(raw_data)",
  user_functions = "functions.py",
  encoder = "save_arrow"
),

rxp_r(
  name = analysis_results,
  expr = "run_analysis(processed_data)",
  user_functions = "functions.R",
  decoder = "arrow::read_feather"
)
```

The Python `save_arrow` encoder function:

```python
#| eval: false
def save_arrow(df: pd.DataFrame, path: str):
    """Encoder function to save a pandas DataFrame to an Arrow file."""
    feather.write_feather(df, path)
```

Both `encoder` and `decoder` functions must accept the object as the first
argument and the file path as the second argument. This pattern works for any
serialization format supported by your languages of choice.

## Advanced Topics

### Complete Polyglot Pipeline Walkthrough

You can find the complete code source to this example over
[here](https://github.com/b-rodrigues/rixpress_demos/tree/master/rbc)^[https://github.com/b-rodrigues/rixpress_demos/tree/master/rbc].

For this polyglot pipeline, I use three programming languages: Julia to simulate
synthetic data from a macroeconomic model, Python to train a machine learning
model and R to visualize the results. Let me make it clear, though, that
scientifically speaking, this is nonsensical: this is merely an example to
showcase how to setup a complete end-to-end project with `{rixpress}`.

The model I use is the foundational Real Business Cycle (RBC) model. The theory
and log-linearized equations used in the Julia script are taken directly from
the excellent lecture slides by Bianca De Paoli.

::: {.callout-note}
### Source Attribution

The economic theory and equations for the RBC model are based on the following
source. This document serves as a guide for our implementation.

**Source:** De Paoli, B. (2009). *Slides 1: The RBC Model, Analytical and
Numerical solutions*.
[https://personal.lse.ac.uk/depaoli/RBC_slides1.pdf](https://personal.lse.ac.uk/depaoli/RBC_slides1.pdf)
:::

First, we need the actual execution environment. Thanks to the `{rix}` package
we can declaratively define a `default.nix` file. This file locks down the exact
versions of R, Julia, Python, and all their respective packages, ensuring the
pipeline runs identically today, tomorrow, or on any machine with Nix installed.

```r
#| eval: false
#| code-summary: "Environment Definition with {rix}"
# This script defines the polyglot environment our pipeline will run in.
library(rix)

# Define the complete execution environment
rix(
  # Pin the environment to a specific date to ensure that all package
  # versions are resolved as they were on this day.
  date = "2025-10-14",

  # 1. R Packages
  # We need packages for plotting, data manipulation, and reading arrow files.
  r_pkgs = c(
    "ggplot2",
    "dplyr",
    "arrow",
    "rix",
    "rixpress"
  ),

  # 2. Julia Configuration
  # We specify the Julia version and the list of packages needed
  # for our manual RBC model simulation.
  jl_conf = list(
    jl_version = "lts",
    jl_pkgs = c(
      "Distributions", # For creating random shocks
      "DataFrames", # For structuring the output
      "Arrow", # For saving the data in a cross-language format
      "Random"
    )
  ),

  # 3. Python Configuration
  # We specify the Python version and the packages needed for the
  # machine learning step.
  py_conf = list(
    py_version = "3.13",
    py_pkgs = c(
      "pandas",
      "scikit-learn",
      "xgboost",
      "pyarrow"
    )
  ),

  # We set the IDE to 'none' for a minimal environment. You could change
  # this to "rstudio" if you prefer to work interactively in RStudio.
  ide = "none",

  # Define the project path and allow overwriting the default.nix file.
  project_path = ".",
  overwrite = TRUE
)

```

Assuming we are working on a system that has Nix installed, we can "drop" into
a temporary Nix shell with R and `{rix}` available:

```bash
nix-shell --expr "$(curl -sl https://raw.githubusercontent.com/ropensci/rix/main/inst/extdata/default.nix)"
```

Once the shell is ready, start R (by simply typing `R`) and run
`source("gen-env.R")` to generate the adequate `default.nix` file (or even
easier, just `Rscript gen-env.R`). This is the environment that we can use to
work interactively with the code while developing, and in which the pipeline
will be executed.

We then need some dedicated scripts with our code. This Julia script contains a
pure function that simulates the RBC model and returns a DataFrame. The code is
a direct implementation of the state-space solution derived from the equations
in the aforementioned lecture and is saved under `functions/functions.jl`. I
don't show the scripts contents here, as it is quite long, so look for it in the
link from before.

At the end of the script, I create a wrapper around `Arrow.write()` called
`arrow_write()` to serialize the generated data frame into an Arrow file.

While developing, it is possible to start the Julia interpreter and test things
and see if they work. Or you could even start by writing the first lines
of `gen-pipeline.R` like so:

```r
library(rixpress)

list(
  # STEP 0: Define RBC Model Parameters as Derivations
  # This makes the parameters an explicit part of the pipeline.
  # Changing a parameter will cause downstream steps to rebuild.
  rxp_jl(alpha, 0.3), # Capital's share of income
  rxp_jl(beta, 1 / 1.01), # Discount factor
  rxp_jl(delta, 0.025), # Depreciation rate
  rxp_jl(rho, 0.95), # Technology shock persistence
  rxp_jl(sigma, 1.0), # Risk aversion (log-utility)
  rxp_jl(sigma_z, 0.01), # Technology shock standard deviation

  # STEP 1: Julia - Simulate a Real Business Cycle (RBC) model.
  # This derivation runs our Julia script to generate the source data.
  rxp_jl(
    name = simulated_rbc_data,
    expr = "simulate_rbc_model(alpha, beta, delta, rho, sigma, sigma_z)",
    user_functions = "functions/functions.jl", # The file containing the function
    encoder = "arrow_write" # The function to use for saving the output
  )
) |>
  rxp_populate(
    project_path = ".",
    build = TRUE,
    verbose = 1
  )
```

Start an R session, and run `source("gen-pipeline.R")`. This will cause the
model to be simulated. Now, to inspect the data, you can use `rxp_read()`: by
default, `rxp_read()` will try to find the adequate function to read the data
and present it to you. This works for common R and Python objects. But in this
case, the generated data is an Arrow data file, so `rxp_read()`, not knowing how
to read it, will simply return the path to the file in the Nix store. We can
pass this file to the adequate reader.

```r
rxp_read("simulated_rbc_data") |>
  arrow::read_feather() |>
  head()
```

Now that we are happy that our Julia code works, we can move on to the Python
step, by writing a dedicated Python script, `functions/functions.py`.

This Python script defines a function to perform our machine learning task. It
takes the DataFrame from the Julia step, trains an XGBoost model to predict
next-quarter's output, and returns a new DataFrame containing the predictions.
Of course, we could have simply used the RBC model itself for the predictions,
but again, this is a toy example.

The Python script contains several little functions to make the code as modular
as possible. I don't show the script here, as it is quite long, but just as for
the Julia script, I write a wrapper around `feather.write_feather()` to
serialize the data into an Arrow file and pass it finally to R. Just as before,
it is possible to start a Python interpreter as defined in the `default.nix`
file and use to test and debug, or to add derivations to `gen-pipeline.R`,
rebuild the pipeline and check the outputs:


```r
library(rixpress)

list(
  # ... the julia steps from before ...,

  # STEP 2.1: Python - Prepare features (lagging data)
  rxp_py(
    name = processed_data,
    expr = "prepare_features(simulated_rbc_data)",
    user_functions = "functions/functions.py",
    # Decode the Arrow file from Julia into a pandas DataFrame
    decoder = "feather.read_feather"
    # Note: No encoder needed here. {rixpress} will use pickle by default
    # to pass the DataFrame between Python steps.
  ),

  # STEP 2.2: Python - Split data into training and testing sets
  rxp_py(
    name = X_train,
    expr = "get_X_train(processed_data)",
    user_functions = "functions/functions.py"
  ),

  # ... more Python steps as needed ...
) |>
  rxp_populate(
    project_path = ".",
    build = TRUE,
    verbose = 1
  )
```

Arrow is used to pass data from Julia to Python, but then from Python to Python,
pickle is used. Finally, we can continue by passing data further down to R. Once
again, I recommend writing a dedicated script with your functions that you need
and save it in `functions/functions.R`. The `gen-pipeline.R` script would look
something like this:

```r
library(rixpress)

list(
  # ... the julia steps from before ...,

  # ... Python steps ...,

  # STEP 2.5: Python - Format final results for R
  rxp_py(
    name = predictions,
    expr = "format_results(y_test, model_predictions)",
    user_functions = "functions/functions.py",
    # We need an encoder here to save the final DataFrame as an Arrow file
    # so the R step can read it.
    encoder = "save_arrow"
  ),

  # STEP 3: R - Visualize the predictions from the Python model.
  # This final derivation depends on the output of the Python step.
  rxp_r(
    name = output_plot,
    expr = plot_predictions(predictions), # The function to call from functions.R
    user_functions = "functions/functions.R",
    # Specify how to load the upstream data (from Python) into R.
    decoder = arrow::read_feather
  ),

  # STEP 4: Quarto - Compile the final report.
  rxp_qmd(
    name = final_report,
    additional_files = "_rixpress",
    qmd_file = "readme.qmd"
  )
) |>
  rxp_populate(
    project_path = ".",
    build = TRUE,
    verbose = 1
  )
```

To view the plot in an interactive session, you can use `rxp_read()` again:

```r
rxp_read("output_plot")
```

To run the pipeline, start the environment by simply typing `nix-shell` in the
terminal, which will use the environment as defined in the `default.nix` and run
`source("gen-pipeline.R")`. If there are issues, setting the `verbose` argument
of `rxp_populate()` to `1` is helpful to see the full error logs.

### ryxpress: The Python Interface

If you prefer working in Python, `ryxpress` provides the same functionality. You
still define your pipeline in R (since that's where `{rixpress}` runs), but you
can build and inspect artifacts from Python.

To set up an environment with `ryxpress`:

```{r}
#| eval: false
rix(
  date = "2025-10-14",
  r_pkgs = c("rixpress"),
  py_conf = list(
    py_version = "3.13",
    py_pkgs = c("ryxpress", "rds2py", "biocframe", "pandas")
  ),
  ide = "none",
  project_path = ".",
  overwrite = TRUE
)
```

Then from Python:

```python
from ryxpress import rxp_make, rxp_inspect, rxp_load

# Build the pipeline
rxp_make()

# Inspect artifacts
rxp_inspect()

# Load an artifact
rxp_load("mtcars_head")
```

`ryxpress` handles the conversion automatically:

- Tries `pickle.load` first
- Falls back to `rds2py` for R objects
- Returns file paths for complex outputs

### More Examples

The [rixpress_demos](https://github.com/b-rodrigues/rixpress_demos) repository
includes:

- **jl_example**: Using Julia in pipelines
- **r_qs**: Using `{qs}` for faster serialization
- **python_r_typst**: Compiling to Typst documents
- **r_multi_envs**: Different Nix environments for different derivations
- **yanai_lercher_2020**: Reproducing a published paper's analysis

## Summary

`{rixpress}` unifies environment management and workflow orchestration:

- **Define** pipelines with `rxp_*()` functions in familiar R syntax
- **Mix** languages freely: R, Python, Julia, Quarto
- **Build** with Nix for deterministic, cached execution
- **Inspect** outputs with `rxp_read()`, `rxp_load()`, `rxp_copy()`
- **Debug** with timestamped build logs
- **Share** reproducible pipelines via Git

The Python port `ryxpress` provides the same experience for Python-first
workflows.

By embracing structured, plain-text pipelines over notebooks for production
work, your analysis becomes more reliable, more scalable, and fundamentally more
reproducible.

