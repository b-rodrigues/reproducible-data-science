# Functional Programming

In this chapter, we will see why functional programming is crucial for
reproducible, testable, and collaborative data science. We will compare how to
write self-contained, "pure" functions in both R and Python, and how to use
functional concepts like `map`, `filter`, and `reduce` to replace error-prone
loops. Finally, we will discuss how writing functions makes your code easier to
review, debug, and even generate with LLMs.

## Introduction: From Scripts to Functions

In the previous chapter, we learned how to create reproducible development
environments with `{rix}`. We can now ensure everyone has the *exact same
tools* (R, Python, system libraries) *to run our code.

We could have stopped there: after all, we have now reproducible environments,
as many as we need for each of our projects. But having the right tools is only
half the battle. Now we turn to **writing reproducible code itself**. A common
way to start a data analysis is by writing a script: a sequence of commands
executed from top to bottom.

```{r}
#| eval: false
# R script example
library(dplyr)
data(mtcars)
heavy_cars <- filter(mtcars, wt > 4)
mean_mpg_heavy <- mean(heavy_cars$mpg)
print(mean_mpg_heavy)
```

```{python}
#| eval: false
# Python script example
import pandas as pd
mtcars = pd.read_csv("mtcars.csv")
heavy_cars = mtcars[mtcars['wt'] > 4]
mean_mpg_heavy = heavy_cars['mpg'].mean()
print(mean_mpg_heavy)
```

This works, but it has a hidden, dangerous property: **state**. The script
relies on variables like `heavy_cars` existing in the environment, making the
code hard to reason about, debug, and test. Of course, this is a simple example
that is only 4 lines long. But what if you have several scripts, each hundreds
of lines long? Making sense of that is not easy, if you use the coding style
above especially.

### The Notebook Problem

If scripting with state is a crack in the foundation of reproducibility, then
using computational notebooks is a gaping hole. I will not make many friends in
the Python community with the following paragraphs, but that's because only
truth hurts.

Notebooks like Jupyter introduce an even more insidious form of state: the cell
execution order. You can execute cells out of order, meaning the visual layout
of your code has no relation to how it actually ran. This is a recipe for
non-reproducible results and a primary cause of the "it worked yesterday, why is
it broken today?" problem.

In a famous talk from JupyterCon 2018, Joel Grus (a research engineer at the
Allen Institute for AI) played the role of the "heel" at a conference dedicated
to the very tool he was
criticising.^[[I don't like notebooks](https://www.youtube.com/watch?v=7jiPeIFXb6U)]
His central thesis is that data science code should follow software engineering
best practices, and Jupyter notebooks actively discourage those practices.

His biggest complaint is **hidden state and out-of-order execution**. The state
of the variables depends on the *execution history*, not the order of the code
on the screen. You can delete a cell that defined a variable, but that variable
still exists in memory. This leads to unreproducible results where the code
works right now (because of hidden state in memory) but will fail if you restart
the kernel and run top-to-bottom. Worse, it confuses beginners who do not
understand why their code works one minute and breaks the next.

Notebooks also **discourage modular code**. Because it is difficult to import
code from one notebook into another, users tend to write massive, monolithic
scripts, copy and paste the same code blocks into multiple notebooks, and avoid
creating functions or modules that can be tested and reused.

Then there is the **"works on my machine" problem**. Notebooks often lack clear
dependency specifications, users hardcode file paths that only exist on their
specific computer, and reusing someone else's work usually involves manually
copying and pasting cells, which is error-prone.

Grus also argues that notebooks have **poor tooling compared to IDEs**. Actual
text editors provide linting (identifying stylistic errors or unused variables),
type checking, superior autocomplete, and the ability to run unit tests. All of
these are difficult or impossible in notebooks.

Finally, **version control is a nightmare**. Notebooks are JSON files. If two
people edit a notebook and try to merge their changes in Git, the diffs are
unreadable blocks of JSON metadata, and merge conflicts are incredibly difficult
to resolve. This encourages workflows where people email files back and forth
rather than using proper version control.

What does Grus suggest instead? Write code in modules (`.py` or `.R` files)
using a proper editor, write unit tests to ensure the code works, and use
notebooks *only* for the final step: importing those modules to visualise the
data or present the results. Ideally, the notebook should contain very little
logic and mostly just function calls.

This is exactly the approach we will take in this book.

### The Functional Solution

The solution is to embrace a paradigm that minimises state: **Functional 
Programming (FP)**. Instead of a linear script, we structure our code as a 
collection of self-contained, predictable functions.

The power of FP comes from the concept of **purity**, borrowed from mathematics. 
A mathematical function has a beautiful property: for a given input, it always 
returns the same output. `sqrt(4)` is always `2`. Its result doesn't depend on 
what you calculated before or on a random internet connection.

Our Nix environments handle the "right library" problem; purity handles the 
"right logic" problem. Our goal is to write our analysis code with this same 
level of rock-solid predictability.

### FP vs OOP: Transformations vs Actors

To appreciate what FP brings, it helps to contrast it with **Object-Oriented 
Programming (OOP)**, arguably the dominant paradigm in many software systems.

OOP organises computation around *who does what*: a network of objects 
communicating with each other and managing their own internal state. You send 
a message to an object, asking it to perform an action, without needing to know 
how it works internally.

Functional programming, by contrast, organises computation around *how data 
changes*. It replaces a network of interacting objects with a flow of 
transformations: data goes in, data comes out, and nothing else changes in the 
process.

This shift is especially powerful in data science:

- Analyses are naturally expressed as **pipelines of transformations** 
  (cleaning, filtering, aggregating, modelling)
- Pure functions make results **reproducible**: same inputs always yield same 
  outputs
- **Immutability** prevents accidental side effects on shared data
- Because transformations can be composed, tested, and reused independently, 
  FP encourages **modular, maintainable** analysis code

There is also a structural reason why FP fits data science better than OOP. OOP
excels when you have *many different types of objects*, each with a *small set
of methods*. A graphical user interface, for example, has buttons, menus,
windows, and dialogs, each responding to a few actions like `click()` or
`resize()`. But data science is the opposite: we typically work with *a small
number of data structures* (data frames, arrays, models) and apply *a large
number of operations* to them (filtering, grouping, joining, summarising,
plotting). FP handles this case naturally: adding a new function is trivial,
while OOP would require modifying every class.

### Why Does This Matter for Data Science?

Adopting a functional style brings massive benefits:

1. **Unit Testing is Now Possible:** You can't easily test a 200-line script. 
   But you *can* easily test a small function that does one thing.

2. **Code Review is Easier:** A Pull Request that just adds or modifies a 
   single function is simple for your collaborators to understand and approve.

3. **Working with LLMs is More Effective:** It's incredibly effective to ask, 
   "Write a Python function that takes a pandas DataFrame and a column name, 
   and returns the mean of that column, handling missing values. Also, write 
   three `pytest` unit tests for it."

4. **Readability:** Well-named functions are self-documenting:

   ```{r}
   #| eval: false
   starwars %>% 
     group_by(species) %>% 
     summarize(mean_height = mean(height))
   ``` 

   is instantly understandable. The equivalent `for` loop is a puzzle.

## Purity and Side Effects

A **pure function** has two rules:

1. It only depends on its inputs. It doesn't use any "global" variables defined 
   outside the function.
2. It doesn't change anything outside of its own scope. It doesn't modify a 
   global variable or write a file to disk. This is called having "no side 
   effects."

Consider this "impure" function in Python:

```{python}
# IMPURE: Relies on a global variable
discount_rate = 0.10

def calculate_discounted_price(price):
    return price * (1 - discount_rate)  # What if discount_rate changes?

print(calculate_discounted_price(100))
# > 90.0
discount_rate = 0.20  # Someone changes the state
print(calculate_discounted_price(100))
# > 80.0  -- Same input, different output!
```

The pure version passes *all* its dependencies as arguments:

```{python}
# PURE: All inputs are explicit arguments
def calculate_discounted_price_pure(price, rate):
    return price * (1 - rate)

print(calculate_discounted_price_pure(100, 0.10))
# > 90.0
print(calculate_discounted_price_pure(100, 0.20))
# > 80.0
```

Now the function is predictable and self-contained.

### Handling "Impure" Operations like Randomness

Some operations, like generating random numbers, are inherently impure. Each 
time you run `rnorm(10)` or `numpy.random.rand(10)`, you get a different result.

The functional approach is not to avoid this, but to *control* it by making the 
source of impurity (the random seed) an explicit input.

In R, the `{withr}` package helps create a temporary, controlled context:

```{r}
library(withr)

# This function is now pure! For a given seed, the output is always the same.
pure_rnorm <- function(n, seed) {
  with_seed(seed, {
    rnorm(n)
  })
}

pure_rnorm(n = 5, seed = 123)
pure_rnorm(n = 5, seed = 123)  # Same result!
```

In Python, `numpy` provides a more modern, object-oriented way:

```{python}
import numpy as np

# Create a random number generator instance with a seed
rng = np.random.default_rng(seed=123)
print(rng.standard_normal(5))

# If we re-create the same generator, we get the same numbers
rng2 = np.random.default_rng(seed=123)
print(rng2.standard_normal(5))
```

The key is the same: the "state" (the seed) is explicitly managed, not hidden 
globally.

### The OOP Caveat in Python

This introduces a concept from OOP: the `rng` variable is an *object* that 
bundles together data (its internal seed state) and methods (`.standard_normal()`). 
This is **encapsulation**.

This is a double-edged sword for reproducibility. The `rng` object is now a 
stateful entity. If we called `rng.standard_normal(5)` a second time, it would 
produce different numbers because its internal state was mutated.

Core Python libraries like `pandas`, `scikit-learn`, and `matplotlib` are 
fundamentally object-oriented. Our guiding principle must be:

> **Use functions for the flow and logic of your analysis, and treat objects 
> from libraries as values that are passed between these functions.**

Avoid building your own complex classes with hidden state for your data 
pipeline. A pipeline composed of functions (`df2 = clean_data(df1); df3 = 
analyze_data(df2)`) is almost always more transparent than an OOP one 
(`pipeline.load(); pipeline.clean(); pipeline.analyze()`).

## Functions: A Refresher and Beyond

You likely already know how to write functions in R and Python. This section
serves as a quick refresher, but also introduces some concepts you may not have
encountered: **higher-order functions**, **closures**, and **decorators**.

### The Basics

In R, functions are first-class citizens. You assign them to variables and pass
them around like any other value:

```{r}
calculate_ci <- function(x, level = 0.95) {
  se <- sd(x, na.rm = TRUE) / sqrt(length(x))
  mean_val <- mean(x, na.rm = TRUE)
  alpha <- 1 - level
  lower <- mean_val - qnorm(1 - alpha/2) * se
  upper <- mean_val + qnorm(1 - alpha/2) * se
  c(mean = mean_val, lower = lower, upper = upper)
}
```

In Python, the `def` keyword defines functions. Type hints are recommended:

```{python}
def calculate_ci(x: list[float], level: float = 0.95) -> dict:
    """Calculate confidence interval for a list of numbers."""
    import statistics
    import scipy.stats as stats
    n = len(x)
    mean_val = statistics.mean(x)
    se = statistics.stdev(x) / (n ** 0.5)
    alpha = 1 - level
    z = stats.norm.ppf(1 - alpha / 2)
    return {"mean": mean_val, "lower": mean_val - z * se, "upper": mean_val + z * se}
```

### Higher-Order Functions

A **higher-order function** is a function that takes another function as an
argument, returns a function, or both. This is the foundation of functional
programming.

You have already seen examples: `map()`, `filter()`, and `reduce()` are all
higher-order functions because they take a function as their first argument.

Here is a simple example in R:

```{r}
apply_twice <- function(f, x) {
 f(f(x))
}

apply_twice(sqrt, 16)  # sqrt(sqrt(16)) = sqrt(4) = 2
```

And in Python:

```{python}
def apply_twice(f, x):
    return f(f(x))

apply_twice(lambda x: x ** 2, 2)  # (2^2)^2 = 16
```

### Closures: Functions That Remember

A **closure** is a function that "remembers" variables from its enclosing scope,
even after that scope has finished executing. This is useful for creating
specialised functions. These are sometimes called *function factories*.

In R:

```{r}
make_power <- function(n) {
  function(x) x^n
}

square <- make_power(2)
cube <- make_power(3)

square(4)  # 16
cube(4)    # 64
```

In Python:

```{python}
def make_power(n):
    def power(x):
        return x ** n
    return power

square = make_power(2)
cube = make_power(3)

square(4)  # 16
cube(4)    # 64
```

The inner function "closes over" the variable `n`, preserving its value.

### Decorators (Python)

Python has a special syntax for a common use of higher-order functions:
**decorators**. A decorator wraps a function to extend its behaviour without
modifying its code.

```{python}
import time

def timer(func):
    """A decorator that prints how long a function takes to run."""
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        end = time.time()
        print(f"{func.__name__} took {end - start:.4f} seconds")
        return result
    return wrapper

@timer
def slow_sum(n):
    return sum(range(n))

slow_sum(1_000_000)
# > slow_sum took 0.0312 seconds
```

The `@timer` syntax is equivalent to `slow_sum = timer(slow_sum)`. Decorators
are widely used in Python frameworks (Flask, FastAPI, pytest) for logging,
authentication, and caching.

If you find decorators elegant, you will appreciate the chapter on **monads**
later in this book. Monads take the idea of wrapping and chaining functions
further, providing a principled way to handle errors, missing values, and side
effects in a purely functional style.

R does not have built-in decorator syntax, but you can achieve the same effect
with higher-order functions:

```{r}
timer <- function(f) {
  function(...) {
    start <- Sys.time()
    result <- f(...)
    end <- Sys.time()
    message(sprintf("Elapsed: %.4f seconds", end - start))
    result
  }
}

slow_sum <- timer(function(n) sum(seq_len(n)))
slow_sum(1e6)
```

### Tidy Evaluation in R

For data analysis, you will often want functions that work with column names.
The `{dplyr}` package uses "tidy evaluation" with the `{{ }}` (curly-curly)
syntax:

```{r}
library(dplyr)

summarise_variable <- function(data, var) {
  data %>%
    summarise(
      n = n(),
      mean = mean({{ var }}, na.rm = TRUE),
      sd = sd({{ var }}, na.rm = TRUE)
    )
}

starwars %>%
  group_by(species) %>%
  summarise_variable(height)
```

The `{{ var }}` tells `dplyr` to treat `var` as a column name rather than a
literal variable.

### Anonymous Functions and Lambdas

Sometimes you need a quick, throwaway function that does not deserve a name.
Both R and Python support **anonymous functions**.

In Python, the `lambda` keyword creates a small function in a single expression:

```{python}
squares = list(map(lambda x: x ** 2, [1, 2, 3, 4]))
squares
```

In R, the base syntax is verbose, but `{purrr}` introduced the **formula
shorthand** using `~` and `.x`:

```{r}
library(purrr)

# Full anonymous function
map_dbl(1:4, function(x) x^2)

# Formula shorthand (purrr style)
map_dbl(1:4, ~ .x^2)
```

R 4.1 also introduced a shorter base syntax with `\(x)`:

```{r}
# Base R shorthand (R >= 4.1)
sapply(1:4, \(x) x^2)
```

Use anonymous functions when the logic is simple and a full function definition
would be overkill.

### Partial Application

**Partial application** means fixing some arguments of a function to create a
more specialised version. This is closely related to function factories but
works with existing functions rather than defining new ones.

In R, use `purrr::partial()`:

```{r}
library(purrr)

# Create a function that always rounds to 2 decimal places
round2 <- partial(round, digits = 2)

round2(3.14159)  # 3.14
round2(2.71828)  # 2.72
```

In Python, use `functools.partial()`:

```{python}
from functools import partial

# Create a function that always rounds to 2 decimal places
round2 = partial(round, ndigits=2)

round2(3.14159)  # 3.14
round2(2.71828)  # 2.72
```

Partial application is useful for creating callbacks, simplifying repetitive
code, and making functions fit the signature expected by `map()` or similar.

### Immutability: Data That Does Not Change

A core principle of functional programming is **immutability**: once data is
created, it is never modified in place. Instead, transformations produce *new*
copies of the data.

R has **copy-on-modify** semantics. When you "modify" a data frame, R actually
creates a new copy:
 
```{r}
df <- data.frame(x = 1:3)
df2 <- df
df2$x[1] <- 99
df$x[1]  # Still 1, df was not mutated
```

Python, by contrast, has **mutable defaults**, which can surprise newcomers:

```{python}
import pandas as pd

df = pd.DataFrame({"x": [1, 2, 3]})
df2 = df  # This is a reference, not a copy!
df2.loc[0, "x"] = 99
df.loc[0, "x"]  # Now 99, df was mutated!

# To avoid this, explicitly copy:
df2 = df.copy()
```

Immutability prevents entire classes of bugs where one part of your code
unexpectedly modifies data used elsewhere. When using Python, be explicit about
copying when you need independent data.

## The Functional Toolkit: Map, Filter, and Reduce

Most `for` loops can be replaced by one of three core functional concepts: 
**mapping**, **filtering**, or **reducing**. These are "higher-order 
functions": functions that take other functions as arguments.

### 1. Mapping: Applying a Function to Each Element

**The pattern:** You have a list of things, and you want to perform the same 
action on each element, producing a new list of the same length.

#### In R with `purrr::map()`

The `{purrr}` package is the gold standard for functional programming in R:

- `map()`: Always returns a list
- `map_dbl()`: Returns a vector of doubles (numeric)
- `map_chr()`: Returns a vector of characters (strings)
- `map_lgl()`: Returns a vector of logicals (booleans)

```{r}
library(purrr)

# The classic for-loop way (verbose)
means_loop <- vector("double", ncol(mtcars))
for (i in seq_along(mtcars)) {
  means_loop[[i]] <- mean(mtcars[[i]], na.rm = TRUE)
}

# The functional way with map_dbl()
means_functional <- map_dbl(mtcars, mean, na.rm = TRUE)
```

The `map()` version is not just shorter; it's safer. You can't make an 
off-by-one error.

#### In Python with List Comprehensions

Python's most idiomatic tool for mapping is the **list comprehension**:

```{python}
numbers = [1, 2, 3, 4, 5]
squares = [n**2 for n in numbers]
# > [1, 4, 9, 16, 25]
```

Python also has a built-in `map()` function:

```{python}
def to_upper_case(s: str) -> str:
    return s.upper()

words = ["hello", "world"]
upper_words = list(map(to_upper_case, words))
# > ['HELLO', 'WORLD']
```

### 2. Filtering: Keeping Elements That Match a Condition

**The pattern:** You have a list of things, and you want to keep only the 
elements that satisfy a certain condition.

#### In R with `purrr::keep()`

```{r}
df1 <- data.frame(x = 1:50)
df2 <- data.frame(x = 1:200)
df3 <- data.frame(x = 1:75)
list_of_dfs <- list(a = df1, b = df2, c = df3)

# Keep only data frames with more than 100 rows
large_dfs <- keep(list_of_dfs, ~ nrow(.x) > 100)
```

#### In Python with List Comprehensions

List comprehensions have a built-in `if` clause:

```{python}
numbers = [1, 10, 5, 20, 15, 30]
large_numbers = [n for n in numbers if n > 10]
# > [20, 15, 30]
```

### 3. Reducing: Combining All Elements into a Single Value

**The pattern:** You have a list of things, and you want to iteratively combine 
them into a single summary value.

#### In R with `purrr::reduce()`

```{r}
# Sum all elements
total_sum <- reduce(c(1, 2, 3, 4, 5), `+`)

# Find common columns across multiple data frames
list_of_colnames <- map(list_of_dfs, names)
common_cols <- reduce(list_of_colnames, intersect)
```

#### In Python with `functools.reduce`

```{python}
from functools import reduce
import operator

numbers = [1, 2, 3, 4, 5]
total_sum = reduce(operator.add, numbers)
# > 15
```

## The Power of Composition

The final, beautiful consequence of a functional style is **composition**. You 
can chain functions together to build complex workflows from simple, reusable 
parts.

This R code is a sequence of function compositions:

```{r}
#| eval: false
starwars %>%
  filter(!is.na(mass)) %>%
  select(species, sex, mass) %>%
  group_by(sex, species) %>%
  summarise(mean_mass = mean(mass), .groups = "drop")
```

The same idea in Python with pandas:

```{python}
#| eval: false
(starwars_py
 .dropna(subset=['mass'])
 .filter(items=['species', 'sex', 'mass'])
 .groupby(['sex', 'species'])
 ['mass'].mean()
 .reset_index()
)
```

Each step is a function that takes a data frame and returns a new, transformed 
data frame. By combining `map`, `filter`, and `reduce` with this compositional 
style, you can express complex data manipulation pipelines without writing a 
single `for` loop.

### The Composition Challenge in Python

Method chaining in pandas is elegant but limited to the methods defined for 
`DataFrame` objects. R's pipe operators (`|>` and `%>%`) are more flexible 
because functions are not strictly owned by objects, and they can be more easily 
combined.

This reflects the languages' different philosophies:

- **R**'s lineage traces back to Scheme (a Lisp dialect), making functional 
  composition natural
- **Python** was designed around "one obvious way to do it," favouring explicit
  loops and method chaining over function composition

R is fundamentally a functional language that acquired OOP features, while
Python is an OOP language with powerful functional capabilities. Recognising
this helps you leverage the native strengths of each.

## Handling Errors Functionally

What happens when a function in your pipeline fails? In imperative code, you
might wrap everything in try/catch blocks. Functional programming offers a
cleaner approach: functions that *capture* errors rather than throw them.

### In R with `purrr::safely()` and `purrr::possibly()`

The `{purrr}` package provides wrappers that turn error-prone functions into
safe ones:

```{r}
library(purrr)

# A function that might fail
risky_log <- function(x) {
  if (x <= 0) stop("x must be positive")
  log(x)
}

# safely() returns a list with $result and $error
safe_log <- safely(risky_log)
safe_log(10)   # list(result = 2.302585, error = NULL)
safe_log(-1)   # list(result = NULL, error = <error>)

# possibly() returns a default value on error
maybe_log <- possibly(risky_log, otherwise = NA)
map_dbl(c(10, -1, 5), maybe_log)  # c(2.30, NA, 1.61)
```

This lets your pipeline continue even when some elements fail, and you can
inspect failures afterwards.

### In Python with try/except and Result patterns

Python's traditional approach uses `try/except`:

```{python}
def safe_log(x):
    try:
        if x <= 0:
            raise ValueError("x must be positive")
        import math
        return math.log(x)
    except ValueError:
        return None

results = [safe_log(x) for x in [10, -1, 5]]
# [2.302585, None, 1.609437]
```

For a more functional approach, libraries like `returns` provide a `Result`
type that explicitly represents success or failure. We will explore this
pattern further in the chapter on monads.

## When NOT to Use Functional Programming

Functional programming is powerful, but it is not always the best choice. Here
are situations where imperative or object-oriented code may be clearer:

1. **Complex stateful algorithms**: Some algorithms (like graph traversals or
   simulations) naturally require mutable state. Forcing them into a functional
   style can make the code harder to read.

2. **Performance-critical inner loops**: Functional abstractions like `map()`
   introduce overhead. In tight loops where microseconds matter, a simple `for`
   loop may be faster, especially in Python.

3. **Interactive or exploratory work**: When exploring data interactively, it
   is fine to use scripts and notebooks to iterate quickly. The functional
   discipline matters most when code needs to be shared, tested, or maintained.

4. **When your team is not familiar**: Code is read more often than written. If
   your colleagues are not comfortable with functional patterns, overly
   abstract code becomes a liability.

The goal is not functional purity for its own sake, but **clarity and
correctness**. Use functional techniques where they help, and step back to
simpler approaches when they do not.

## Summary

This chapter has laid the groundwork for writing reproducible code by embracing 
**Functional Programming**.

Key takeaways:

- **Pure functions** guarantee the same output for the same input, with no hidden 
  dependencies on global state
- Make impure operations (like randomness) explicit by controlling the seed
- Replace error-prone `for` loops with **map, filter, and reduce**
- Use **composition** to build complex pipelines from simple, reusable functions
- In Python, treat stateful library objects as values passed between pure 
  functions

Understanding the distinction between R's functional heritage and Python's OOP 
nature is key to becoming an effective data scientist in either language. By 
mastering the functional paradigm, you're building a foundation for code that 
is robust, easy to review, simple to debug, and truly reproducible.

In the next chapter, we'll put these principles into practice with 
**`{rixpress}`**, a package that leverages functional composition and Nix to 
build fully reproducible, polyglot analytical pipelines.
